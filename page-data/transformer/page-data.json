{"componentChunkName":"component---src-templates-post-tsx","path":"/transformer/","result":{"data":{"markdownRemark":{"html":"<p>원 논문:</p>\n<blockquote>\n<p>Vaswani et al., 2017, \"Attention is all you need\" (<a href=\"https://arxiv.org/abs/1706.03762\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Link to arxiv</a>)</p>\n</blockquote>\n<h2 id=\"0-transformer\" style=\"position:relative;\"><a href=\"#0-transformer\" aria-label=\"0 transformer permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>0. Transformer</h2>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 366px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/9ad1305d7484bcca224f42cf70766034/6fc67/transformers.jpg\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 74.32432432432432%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAPABQDASIAAhEBAxEB/8QAFwABAQEBAAAAAAAAAAAAAAAAAAIDBf/EABUBAQEAAAAAAAAAAAAAAAAAAAAB/9oADAMBAAIQAxAAAAHjVhUQK//EABkQAAMAAwAAAAAAAAAAAAAAAAABAgMREv/aAAgBAQABBQKMfZWCkNaE2juhvZ//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/AT//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/AT//xAAYEAEBAQEBAAAAAAAAAAAAAAAAIQEyUf/aAAgBAQAGPwKa9VNdar//xAAaEAEAAgMBAAAAAAAAAAAAAAABABEhUZHx/9oACAEBAAE/IXOC9QO+AYioI6ZlEJ6kRWrZ/9oADAMBAAIAAwAAABBPD//EABURAQEAAAAAAAAAAAAAAAAAABAh/9oACAEDAQE/EIf/xAAVEQEBAAAAAAAAAAAAAAAAAAAQIf/aAAgBAgEBPxCn/8QAGhABAAMBAQEAAAAAAAAAAAAAAQARITFRcf/aAAgBAQABPxBKnDqG/uEq4tlWzteRORdCkidg9GVCGndROxOq2z//2Q=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"transformers\"\n        title=\"transformers\"\n        src=\"/static/9ad1305d7484bcca224f42cf70766034/6fc67/transformers.jpg\"\n        srcset=\"/static/9ad1305d7484bcca224f42cf70766034/a80bd/transformers.jpg 148w,\n/static/9ad1305d7484bcca224f42cf70766034/1c91a/transformers.jpg 295w,\n/static/9ad1305d7484bcca224f42cf70766034/6fc67/transformers.jpg 366w\"\n        sizes=\"(max-width: 366px) 100vw, 366px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<center>\nThe Transformer (TV Series), from Wikipedia\n</center>\n</br>\n<p> 트랜스포머는 Seq2Seq 모델과 비슷한 인코더-디코더 구조를 갖고 있지만, 보다 긴 시퀀스를 효율적으로 다룰수 있는 모델로 환영받았다. 트랜스포머는 새로운 어텐션을 도입했다. 2014년에 등장한 어텐션(Bahdanau et al., 2014)이 RNN 네트워크의 성능을 향상시키는 활용된 것과 달리, 2017년의 어텐션은 신경망을 이용하지 않고 행렬 곱으로 이루어진 방식을 제안하면서 자연어 처리에 있어 획기적인 성능 향상을 불러왔다. </p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/d251194d238dd0e07ab438d0dc0f629c/54c3a/transformer.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 71.62162162162163%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAOCAYAAAAvxDzwAAAACXBIWXMAABYlAAAWJQFJUiTwAAABjUlEQVQ4y52TW0/bQBBG8/+f+oiQ+ggtIFWAICgpakxM4oDjC2kCVSmOkTARuRg7duI4t4PX4qFVS4j7Saud1c6enV3Nl1sul7w3hIJhgDtw03hVbm4d2Hw+x2iZVLQq4ThcCV0LGMcxleoFxdMSvX7//4CLxSIdQp73zJejHfZPdtH0OlEUvQnNvVXV7+r2B+yV63ySDS7NJoHvrw8Umkxirq+bnMl5SlI+jaXiFqdHm1zWZEbjKBtw6HtUVI3P5Rp7UhVd11AK23wv7/KjaST/6GYD+p6P2WigGCp6s4Ft2xS/HnCc30FVLwjDUTbgcwI01HPkgw9IhxtYtzfoxY/cfNugZSh4fpgNGAQhhlanVjpGkQo4Dw8oZwWUZN0wdYJ1KxSazWY4jsPtrzt+3rUZeEPElpjF2mrbdDqdNO9f0L+Aovfi6RS326Uly9jJX0ajEZZpciVJDJ6eiCaT1D3vAv9wR9K8jmXxeH/PLLlAzI9WmziBvZKyOUVonByevlbieh7dXi99wSqrvgDhtjA6NjhXjgAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"transformer\"\n        title=\"transformer\"\n        src=\"/static/d251194d238dd0e07ab438d0dc0f629c/fcda8/transformer.png\"\n        srcset=\"/static/d251194d238dd0e07ab438d0dc0f629c/12f09/transformer.png 148w,\n/static/d251194d238dd0e07ab438d0dc0f629c/e4a3f/transformer.png 295w,\n/static/d251194d238dd0e07ab438d0dc0f629c/fcda8/transformer.png 590w,\n/static/d251194d238dd0e07ab438d0dc0f629c/efc66/transformer.png 885w,\n/static/d251194d238dd0e07ab438d0dc0f629c/c83ae/transformer.png 1180w,\n/static/d251194d238dd0e07ab438d0dc0f629c/54c3a/transformer.png 1257w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<center>\n트랜스포머 모델의 구조, Vaswani et al., 2017\n</center>\n</br>\n<p>트랜스포머 모델은 크게 봐서 왼쪽의 인코더와 오른쪽의 디코더를 가지는 구조이다. RNN 모델에서와 마찬가지로 인코더의 결과값을 디코더의 입력값과 결합해서 연산을 거쳐 결과값을 도출한다. 다만 RNN과의 차이점은 시퀀스를 <strong>동시에</strong> 처리한다는 점이다. 즉 시퀀스 모델처럼 시퀀스 데이터에 순서대로 접근해서 하나씩 처리하는 것이 아니라 전체 시퀀스를 한 번에 연산한다. 따라서 분산 연산이 용이하며 RNN 모델에서 발생하는 vanishing gradient 현상에 효과적으로 대처해서 보다 긴 시퀀스를 다룰 수 있게 되었다.</p>\n<p>트랜스포머 네트워크에 사용되는 어텐션은 행렬곱을 이용한 순차적이지 않은 (따라서 병렬 연산이 가능한) 어텐션이며, 이 어텐션을 Scaled-Dot product 어텐션이라고 한다.  </p>\n<h3 id=\"-scaled-dot-prooduct-attention\" style=\"position:relative;\"><a href=\"#-scaled-dot-prooduct-attention\" aria-label=\" scaled dot prooduct attention permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>🔔 Scaled-Dot prooduct Attention</h3>\n<p>Scaled-Dot prooduct 어텐션에는 세개의 행렬 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>Q</mi></mrow><annotation encoding=\"application/x-tex\">Q</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8777699999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\">Q</span></span></span></span>, <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>K</mi></mrow><annotation encoding=\"application/x-tex\">K</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.07153em;\">K</span></span></span></span>, <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>V</mi></mrow><annotation encoding=\"application/x-tex\">V</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">V</span></span></span></span> 입력이 필요하다:</p>\n<ul>\n<li><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>Q</mi></mrow><annotation encoding=\"application/x-tex\">Q</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8777699999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\">Q</span></span></span></span>, Queries : 비교하고자 하는 시퀀스로, 키 K와 유사도를 측정한다.</li>\n<li><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>K</mi></mrow><annotation encoding=\"application/x-tex\">K</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.07153em;\">K</span></span></span></span>, Keys : 비교 대상이 되는 시퀀스로, 쿼리 Q와 유사도를 측정한다.</li>\n<li><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>V</mi></mrow><annotation encoding=\"application/x-tex\">V</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">V</span></span></span></span>, Values : <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>Q</mi><mi>K</mi></mrow><annotation encoding=\"application/x-tex\">QK</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8777699999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\">Q</span><span class=\"mord mathdefault\" style=\"margin-right:0.07153em;\">K</span></span></span></span>의 가중치가 곱해지는 행렬이다.</li>\n</ul>\n<p>어텐션이란 행렬 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>Q</mi></mrow><annotation encoding=\"application/x-tex\">Q</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8777699999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\">Q</span></span></span></span>와 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>K</mi></mrow><annotation encoding=\"application/x-tex\">K</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.07153em;\">K</span></span></span></span>-<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>V</mi></mrow><annotation encoding=\"application/x-tex\">V</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">V</span></span></span></span> 쌍을 결과값에 매핑하는 함수로, 결과값은 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>V</mi></mrow><annotation encoding=\"application/x-tex\">V</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">V</span></span></span></span>의 가중치 합으로 계산되고, 각각의 값 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>v</mi></mrow><annotation encoding=\"application/x-tex\">v</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">v</span></span></span></span>에 할당된 가중치는 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>q</mi></mrow><annotation encoding=\"application/x-tex\">q</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">q</span></span></span></span>와 그에 대응하는 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>k</mi></mrow><annotation encoding=\"application/x-tex\">k</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03148em;\">k</span></span></span></span>로부터 계산된다. 이 어텐션이 이름 붙은 이유는 어텐션을 연산하는 방법에 있다.</p>\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>A</mi><mi>t</mi><mi>t</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo stretchy=\"false\">(</mo><mi>Q</mi><mo separator=\"true\">,</mo><mi>K</mi><mo separator=\"true\">,</mo><mi>V</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mi>s</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>m</mi><mi>a</mi><mi>x</mi><mo stretchy=\"false\">(</mo><mfrac><mrow><mi>Q</mi><mo>⋅</mo><msup><mi>K</mi><mi>T</mi></msup></mrow><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mfrac><mo stretchy=\"false\">)</mo><mo>⋅</mo><mi>V</mi></mrow><annotation encoding=\"application/x-tex\">Attention(Q, K, V) = softmax(\\frac{Q \\cdot K^T}{\\sqrt{d_k}}) \\cdot V</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\">A</span><span class=\"mord mathdefault\">t</span><span class=\"mord mathdefault\">t</span><span class=\"mord mathdefault\">e</span><span class=\"mord mathdefault\">n</span><span class=\"mord mathdefault\">t</span><span class=\"mord mathdefault\">i</span><span class=\"mord mathdefault\">o</span><span class=\"mord mathdefault\">n</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\">Q</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.07153em;\">K</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">V</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:2.448331em;vertical-align:-0.93em;\"></span><span class=\"mord mathdefault\">s</span><span class=\"mord mathdefault\">o</span><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span><span class=\"mord mathdefault\">t</span><span class=\"mord mathdefault\">m</span><span class=\"mord mathdefault\">a</span><span class=\"mord mathdefault\">x</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.5183309999999999em;\"><span style=\"top:-2.25278em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord sqrt\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.85722em;\"><span class=\"svg-align\" style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\" style=\"padding-left:0.833em;\"><span class=\"mord\"><span class=\"mord mathdefault\">d</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.33610799999999996em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03148em;\">k</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span><span style=\"top:-2.81722em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"hide-tail\" style=\"min-width:0.853em;height:1.08em;\"><svg width='400em' height='1.08em' viewBox='0 0 400000 1080' preserveAspectRatio='xMinYMin slice'><path d='M95,702c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,\n-10,-9.5,-14c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54c44.2,-33.3,65.8,\n-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10s173,378,173,378c0.7,0,\n35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429c69,-144,104.5,-217.7,106.5,\n-221c5.3,-9.3,12,-14,20,-14H400000v40H845.2724s-225.272,467,-225.272,467\ns-235,486,-235,486c-2.7,4.7,-9,7,-19,7c-6,0,-10,-1,-12,-3s-194,-422,-194,-422\ns-65,47,-65,47z M834 80H400000v40H845z'/></svg></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.18278000000000005em;\"><span></span></span></span></span></span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.677em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">Q</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">⋅</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.07153em;\">K</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413309999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.13889em;\">T</span></span></span></span></span></span></span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.93em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">⋅</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">V</span></span></span></span></span>\n<p>첫 번째 행렬 곱(dot product)은 쿼리 Q와 키 K의 유사도를 분석하고, 두번째 곱은 softmax 함수값으로 얻어진 가중치를 발류 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>V</mi></mrow><annotation encoding=\"application/x-tex\">V</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">V</span></span></span></span>에 할당한다. <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>Q</mi></mrow><annotation encoding=\"application/x-tex\">Q</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8777699999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\">Q</span></span></span></span>와 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>K</mi></mrow><annotation encoding=\"application/x-tex\">K</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.07153em;\">K</span></span></span></span>의 곱은 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>K</mi></mrow><annotation encoding=\"application/x-tex\">K</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.07153em;\">K</span></span></span></span>의 차원 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>d</mi><mi>k</mi></msub></mrow><annotation encoding=\"application/x-tex\">d_k</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.84444em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">d</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.33610799999999996em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03148em;\">k</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>의 루트로 조정(scaled) 하는데, 이 스케일링은 reguralization효과를 가진다. 그 다음 softmax 함수를 거쳐 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>Q</mi></mrow><annotation encoding=\"application/x-tex\">Q</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8777699999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\">Q</span></span></span></span>와 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>K</mi></mrow><annotation encoding=\"application/x-tex\">K</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.07153em;\">K</span></span></span></span>의 유사도를 가중치로 변환해 마지막으로 그 값을 행렬 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>V</mi></mrow><annotation encoding=\"application/x-tex\">V</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">V</span></span></span></span>에 반영해 모든 쿼리의 어텐션을 얻을 수 있다. </p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/8c829fba5d92c05239fe7a3d488cda81/7388e/attention.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 61.48648648648649%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAYAAABiDJ37AAAACXBIWXMAABYlAAAWJQFJUiTwAAABtklEQVQoz42SzUoCURTHR0FRKHUVBS1EF00EDpaLdmK4EhEUQfwIFCUZIghsEKFa+Cw+gc8gvUCbWugIIRQ0OoKi49e/5sTYVFod+HPuPfee3z3ncpjFYoHvUm00GmEwGEBRFEhdCXgPT6dT0mw2I6mmz1GNgc70h/1+H+PxmNaPzQdIwxcoYwVyX6aY+pCWozdGD9G8+nq9Xke5XIZwJeD8ggd/eYab6xsIgoBSqYRarUZ35/P51wq/t6teEFtNnGYy8Pv9YBhmpSKRCAHUL/gB/KzuY/30/Io9dh9ulwtHPh/YXRcC3DGBLBYL+UQisR6oBaaTCV67Pdzdt3DAHVLi1s42bvkrxH1B2ptMJvLRaPRv4HA4hCiKaIstcBxHicFgEIGTADY2N2Gz2WA0Gikei8X+bllbd3u9JVADhMNhpNPp5R+uBephmjqdDrxe7xeg3W4Hz/Nwu920j8fjv7esr7DdbsPj8VCi2WymfzMYDLBarahUKnA6nQiFQv8DqpJlGSzLrhwXh8NBZ8lkcjmza4Hai5IkoVqtIpVKIZfLIZvNIp/Po1gsolDIv6uARqOxcrDfALls3HQJ0U3wAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"attention\"\n        title=\"attention\"\n        src=\"/static/8c829fba5d92c05239fe7a3d488cda81/fcda8/attention.png\"\n        srcset=\"/static/8c829fba5d92c05239fe7a3d488cda81/12f09/attention.png 148w,\n/static/8c829fba5d92c05239fe7a3d488cda81/e4a3f/attention.png 295w,\n/static/8c829fba5d92c05239fe7a3d488cda81/fcda8/attention.png 590w,\n/static/8c829fba5d92c05239fe7a3d488cda81/efc66/attention.png 885w,\n/static/8c829fba5d92c05239fe7a3d488cda81/7388e/attention.png 1104w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<center>\n어텐션, Badnau et al., 2015\n</center>\n</br>\n<p>어텐션은 쿼리와 키의 heatmap을 생성하는 것과 같다. 전체 시퀀스를 한 번에 단어 쌍 단위 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mo stretchy=\"false\">(</mo><mi>q</mi><mo separator=\"true\">,</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(q, k)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">q</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03148em;\">k</span><span class=\"mclose\">)</span></span></span></span>로 살펴보기 때문에, 단어가 문장에서 등장하는 순서에 관계없이 유사도를 계산할 수 있다. 위의 예처럼 어순이 다른 영어와 프랑스어 문장에서도 단어의 대응 관계를 제대로 파악할 수 있다.</p>\n<h3 id=\"-multi-head-attention\" style=\"position:relative;\"><a href=\"#-multi-head-attention\" aria-label=\" multi head attention permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>🔔 Multi-Head Attention</h3>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/30264da045e993647985458dc2263503/5d72a/multi-head-attention.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 36.486486486486484%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAHCAYAAAAIy204AAAACXBIWXMAAAsTAAALEwEAmpwYAAABaUlEQVQoz0WRS0+DUBCF+f8LFy5M3LlxoXGjrjSNpmkwxjTQAq0J5Q0tfdBAeRUI5chM00gyuY+5882cg5DnOcIwRHks+jhiPp9D0zRMJhPMZjPeS5KEKIqw2Wyg6zpMw0SSxngfv+BZfEC490Ff13UQsiyD4zgMdV33XGCaDKMQRRGyLGM0GsEwDKRp2jcuUTU1LNeBaVuID8k/sCgKLJdLBnqex/sgCLgJnS3L4slowrquubDMcmjDH8iDL0gDEcrHN+JNxDme8ALcbrcMIwBNuV6v+RzHMY69HfSWvr0b4vPmEa9Xd3i7vsfw9gn27wJZkUMgD1erFXa7Ha8kmyC2bTOUPCSpJJvu27ZF4Pf5RZ9TVCjyFJ59VpMkCYSyLPkhwWgiShDM930GqqrK8uknkWwqGo/H0Bc62zFVFPh9PTWkOoE6EpS8rKqKpV2CPKNc0zScp5WMj+MEp/aENE85ulN/d0g59wcmngH/Ox417gAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"multi head attention\"\n        title=\"multi head attention\"\n        src=\"/static/30264da045e993647985458dc2263503/fcda8/multi-head-attention.png\"\n        srcset=\"/static/30264da045e993647985458dc2263503/12f09/multi-head-attention.png 148w,\n/static/30264da045e993647985458dc2263503/e4a3f/multi-head-attention.png 295w,\n/static/30264da045e993647985458dc2263503/fcda8/multi-head-attention.png 590w,\n/static/30264da045e993647985458dc2263503/5d72a/multi-head-attention.png 834w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<center>\nMulti-Head Attention, from deeplearning.ai\n</center>\n</br>\n<p>트랜스포머의 어텐션의 또다른 특징은 어텐션이 머리(heads)를 여러개 가진다는 것이다. 입력값에 대해 어텐션을 여러번 수행한 후 결과를 결합(concatenate)해서 최종적으로 하나의 행렬을 만드는 것을 뜻한다. <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>d</mi><mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub></mrow><annotation encoding=\"application/x-tex\">d_{model}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.84444em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">d</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.33610799999999996em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">m</span><span class=\"mord mathdefault mtight\">o</span><span class=\"mord mathdefault mtight\">d</span><span class=\"mord mathdefault mtight\">e</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.01968em;\">l</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> 차원의 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>Q</mi></mrow><annotation encoding=\"application/x-tex\">Q</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8777699999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\">Q</span></span></span></span>, <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>K</mi></mrow><annotation encoding=\"application/x-tex\">K</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.07153em;\">K</span></span></span></span>, <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>V</mi></mrow><annotation encoding=\"application/x-tex\">V</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">V</span></span></span></span>에 대해 일회 어텐션을 적용하는 대신 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>Q</mi></mrow><annotation encoding=\"application/x-tex\">Q</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8777699999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\">Q</span></span></span></span>, <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>K</mi></mrow><annotation encoding=\"application/x-tex\">K</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.07153em;\">K</span></span></span></span>, <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>V</mi></mrow><annotation encoding=\"application/x-tex\">V</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">V</span></span></span></span>를 <code class=\"language-text\">n_heads</code>번 선형 투사(linear projection)해서 서로다른 학습된 선형 투사들에 대해 어텐션을 적용한다. 각 선형 투사를 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>Q</mi></mrow><annotation encoding=\"application/x-tex\">Q</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8777699999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\">Q</span></span></span></span>에 대해 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msup><mi>W</mi><mi>Q</mi></msup></mrow><annotation encoding=\"application/x-tex\">W^Q</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8413309999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413309999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">Q</span></span></span></span></span></span></span></span></span></span></span>, <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>K</mi></mrow><annotation encoding=\"application/x-tex\">K</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.07153em;\">K</span></span></span></span>에 대해 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msup><mi>W</mi><mi>K</mi></msup></mrow><annotation encoding=\"application/x-tex\">W^K</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8413309999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413309999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.07153em;\">K</span></span></span></span></span></span></span></span></span></span></span>, <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>V</mi></mrow><annotation encoding=\"application/x-tex\">V</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">V</span></span></span></span>에 대해 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msup><mi>W</mi><mi>V</mi></msup></mrow><annotation encoding=\"application/x-tex\">W^V</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8413309999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413309999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.22222em;\">V</span></span></span></span></span></span></span></span></span></span></span>라고 하며 각각 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>n</mi><mrow><mi>s</mi><mi>e</mi><mi>q</mi></mrow></msub><mo separator=\"true\">,</mo><msub><mi>d</mi><mi>k</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(n_{seq}, d_k)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.036108em;vertical-align:-0.286108em;\"></span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathdefault\">n</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15139200000000003em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">s</span><span class=\"mord mathdefault mtight\">e</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03588em;\">q</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.286108em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">d</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.33610799999999996em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03148em;\">k</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span>, <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>n</mi><mrow><mi>s</mi><mi>e</mi><mi>q</mi></mrow></msub><mo separator=\"true\">,</mo><msub><mi>d</mi><mi>k</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(n_{seq}, d_k)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.036108em;vertical-align:-0.286108em;\"></span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathdefault\">n</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15139200000000003em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">s</span><span class=\"mord mathdefault mtight\">e</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03588em;\">q</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.286108em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">d</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.33610799999999996em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03148em;\">k</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span>, <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>n</mi><mrow><mi>s</mi><mi>e</mi><mi>q</mi></mrow></msub><mo separator=\"true\">,</mo><msub><mi>d</mi><mi>v</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(n_{seq}, d_v)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.036108em;vertical-align:-0.286108em;\"></span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathdefault\">n</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15139200000000003em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">s</span><span class=\"mord mathdefault mtight\">e</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03588em;\">q</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.286108em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">d</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.151392em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03588em;\">v</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span> 차원을 가진다. 어텐션을 적용하면 한개의 결과값에 대해 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>n</mi><mrow><mi>s</mi><mi>e</mi><mi>q</mi></mrow></msub><mo separator=\"true\">,</mo><msub><mi>d</mi><mi>v</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(n_{seq}, d_v)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.036108em;vertical-align:-0.286108em;\"></span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathdefault\">n</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15139200000000003em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">s</span><span class=\"mord mathdefault mtight\">e</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03588em;\">q</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.286108em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">d</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.151392em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03588em;\">v</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span> 차원을 가지므로 <code class=\"language-text\">n_heads</code>개의 결과값을 결합한 행렬 다시 선형 투사해서 어텐션 결과값을 얻는다. 매번 학습할 때마다 다른 가중치를 가지므로 여러번 어텐션을 적용함으로서 시퀀스 데이터 사이의 다양한 관계를 학습할 수 있다. 이때 여러번의 어텐션을 순서대로 실행해야 할 필요가 없으므로 병렬 연산을 하기 용이한 구조다. </p>\n<p>📂 다음 코드는 Trax <code class=\"language-text\">SplitIntoHeads</code>와 <code class=\"language-text\">MergeHeads</code> 메서드로, Multi-Head 어텐션의 일부분을 구현한다. </p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">def</span> <span class=\"token function\">SplitIntoHeads</span><span class=\"token punctuation\">(</span>n_heads<span class=\"token punctuation\">,</span> merged_batch_and_head<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n  <span class=\"token triple-quoted-string string\">\"\"\"Returns a layer that reshapes an array for multi-head computation.\"\"\"</span>\n  <span class=\"token keyword\">def</span> <span class=\"token function\">f</span><span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    batch_size<span class=\"token punctuation\">,</span> seq_len<span class=\"token punctuation\">,</span> d_feature <span class=\"token operator\">=</span> x<span class=\"token punctuation\">.</span>shape\n    <span class=\"token keyword\">if</span> d_feature <span class=\"token operator\">%</span> n_heads <span class=\"token operator\">!=</span> <span class=\"token number\">0</span><span class=\"token punctuation\">:</span>\n      <span class=\"token keyword\">raise</span> ValueError<span class=\"token punctuation\">(</span>\n          <span class=\"token string-interpolation\"><span class=\"token string\">f'Feature embedding dimensionality (</span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>d_feature<span class=\"token punctuation\">}</span></span><span class=\"token string\">) is not a multiple'</span></span>\n          <span class=\"token string-interpolation\"><span class=\"token string\">f' of the requested number of attention heads (</span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>n_heads<span class=\"token punctuation\">}</span></span><span class=\"token string\">).'</span></span><span class=\"token punctuation\">)</span>\n\n    d_head <span class=\"token operator\">=</span> d_feature <span class=\"token operator\">//</span> n_heads\n\n    <span class=\"token comment\"># (b_size, seq_len, d_feature) --> (b_size*n_heads, seq_len, d_head)</span>\n    x <span class=\"token operator\">=</span> x<span class=\"token punctuation\">.</span>reshape<span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span>batch_size<span class=\"token punctuation\">,</span> seq_len<span class=\"token punctuation\">,</span> n_heads<span class=\"token punctuation\">,</span> d_head<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    x <span class=\"token operator\">=</span> x<span class=\"token punctuation\">.</span>transpose<span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">3</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">if</span> merged_batch_and_head<span class=\"token punctuation\">:</span>\n      x <span class=\"token operator\">=</span> x<span class=\"token punctuation\">.</span>reshape<span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span>batch_size <span class=\"token operator\">*</span> n_heads<span class=\"token punctuation\">,</span> seq_len<span class=\"token punctuation\">,</span> d_head<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">return</span> x\n  <span class=\"token keyword\">return</span> Fn<span class=\"token punctuation\">(</span><span class=\"token string\">'SplitIntoHeads'</span><span class=\"token punctuation\">,</span> f<span class=\"token punctuation\">)</span></code></pre></div>\n<p><code class=\"language-text\">SplitIntoHeads</code>는 <code class=\"language-text\">d_feature</code>(그림에서는 <code class=\"language-text\">d_model</code>)가 Head의 개수 <code class=\"language-text\">n_heads</code>의 정수배일 것을 강제한다. 위의 그림처럼 <code class=\"language-text\">(batch, seq_len, d_feature)</code>을 입력받아 <code class=\"language-text\">(b_size*n_heads, seq_len, d_head)</code>차원을 출력하고 있다. 그 다음 <code class=\"language-text\">PureAttention</code>처럼 1개 Head에 대한 어텐션을 <code class=\"language-text\">n_heads</code>만큼 수행한 후, 여러 Head를 <code class=\"language-text\">MergeHeads</code>로 결합한다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">def</span> <span class=\"token function\">MergeHeads</span><span class=\"token punctuation\">(</span>n_heads<span class=\"token punctuation\">,</span> merged_batch_and_head<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n  <span class=\"token triple-quoted-string string\">\"\"\"Returns a layer that rejoins heads, after multi-head computation.\"\"\"</span>\n  <span class=\"token keyword\">def</span> <span class=\"token function\">f</span><span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">if</span> merged_batch_and_head<span class=\"token punctuation\">:</span>\n      dim_0<span class=\"token punctuation\">,</span> seq_len<span class=\"token punctuation\">,</span> d_head <span class=\"token operator\">=</span> x<span class=\"token punctuation\">.</span>shape\n      <span class=\"token keyword\">if</span> dim_0 <span class=\"token operator\">%</span> n_heads <span class=\"token operator\">!=</span> <span class=\"token number\">0</span><span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">raise</span> ValueError<span class=\"token punctuation\">(</span>\n            <span class=\"token string-interpolation\"><span class=\"token string\">f\"Array's leading dimension (</span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>dim_0<span class=\"token punctuation\">}</span></span><span class=\"token string\">) is not a multiple of the\"</span></span>\n            <span class=\"token string-interpolation\"><span class=\"token string\">f\" number of attention heads (</span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>n_heads<span class=\"token punctuation\">}</span></span><span class=\"token string\">).\"</span></span><span class=\"token punctuation\">)</span>\n\n      batch_size <span class=\"token operator\">=</span> dim_0 <span class=\"token operator\">//</span> n_heads\n      x <span class=\"token operator\">=</span> x<span class=\"token punctuation\">.</span>reshape<span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span>batch_size<span class=\"token punctuation\">,</span> n_heads<span class=\"token punctuation\">,</span> seq_len<span class=\"token punctuation\">,</span> d_head<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">else</span><span class=\"token punctuation\">:</span>\n      batch_size<span class=\"token punctuation\">,</span> _<span class=\"token punctuation\">,</span> seq_len<span class=\"token punctuation\">,</span> d_head <span class=\"token operator\">=</span> x<span class=\"token punctuation\">.</span>shape\n\n    <span class=\"token comment\"># (b_size, n_heads, seq_len, d_head) --> (b_size, seq_len, d_feature)</span>\n    x <span class=\"token operator\">=</span> x<span class=\"token punctuation\">.</span>transpose<span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">3</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    x <span class=\"token operator\">=</span> x<span class=\"token punctuation\">.</span>reshape<span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span>batch_size<span class=\"token punctuation\">,</span> seq_len<span class=\"token punctuation\">,</span> n_heads <span class=\"token operator\">*</span> d_head<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">return</span> x\n  <span class=\"token keyword\">return</span> Fn<span class=\"token punctuation\">(</span><span class=\"token string\">'MergeHeads'</span><span class=\"token punctuation\">,</span> f<span class=\"token punctuation\">)</span></code></pre></div>\n<p>함수 <code class=\"language-text\">f</code>는 <code class=\"language-text\">(batch_size, n_heads, seq_len, d_head)</code> 차원의 입력값을 어텐션 블럭의 입력값 차원 <code class=\"language-text\">(b_size, seq_len, d_feature)</code>로 변형해서 반환한다. </p>\n<h2 id=\"1-encoder\" style=\"position:relative;\"><a href=\"#1-encoder\" aria-label=\"1 encoder permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>1. Encoder</h2>\n<p>널리 쓰이는 트랜스포머 사전훈련 모델인 BERT의 경우 인코더만 (<code class=\"language-text\">bert-large</code>의  경우) 24개 쌓은 네트워크다. LSTM을 생각해 보면 인코더의 구조는 상대적으로 단순해 보이지만, 인코더만으로도 많은 작업을 수행할 수 있음을 알 수 있다.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/98e8833c17d9e0c060a447cf25081023/ce0a7/transformer_encoder.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 50%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAYAAAC0VX7mAAAACXBIWXMAABYlAAAWJQFJUiTwAAABtUlEQVQoz2WSSW/bMBCF/f9/RE+9FL7kEBRogByyXFIHCYrEsWVJsaUo2qx9qSzJ8ivfGDICdwBiqCH56c0jJ1VVwXVdeJ6HMNwiimKkaXqqMfq+x263k/w1uq5D27aSx5g0TQM/CGAaOnTtDS9/nrFZr2HbHwJsuxZJnGCz2chPRgBzGIRqny11clifkFrXFd6dAD9vn3D98Ar93cKw3+NwOGC7DeE4DizLQhzHSJIEZVmqHP9XL4riCCzyFGZQYXpv4PLRgWF7qMtCqetk42q1wlqp5jzLMtAm5jAMZU3TNASqS9Ynfd/B9Xx46wVuLr5hdvUDnm1iOEDaIsQwDCwWC5imqTyOVEe1+JznOZbLJebzucCpXBRmeYn8U8Pb9XeY91MEtiEGE5gmqRyiQrbIgyPQ933xmer4LUBO6MOnUumGEeK8QP23OQGpkLARSL8IZMu8EF3X5VL4I/EwS5UX0Rap5WM2vcLr5R368gjkU4niCNxD8DjoFYUQTihfAEWdLoXRpCV+/7rBy90M+93xXXVtJwep5us4tpycbpaDfsql8GkMw4DzYH1cOx9j/Xyd3/8ASrjx3DYfs6gAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"transformer encoder\"\n        title=\"transformer encoder\"\n        src=\"/static/98e8833c17d9e0c060a447cf25081023/fcda8/transformer_encoder.png\"\n        srcset=\"/static/98e8833c17d9e0c060a447cf25081023/12f09/transformer_encoder.png 148w,\n/static/98e8833c17d9e0c060a447cf25081023/e4a3f/transformer_encoder.png 295w,\n/static/98e8833c17d9e0c060a447cf25081023/fcda8/transformer_encoder.png 590w,\n/static/98e8833c17d9e0c060a447cf25081023/efc66/transformer_encoder.png 885w,\n/static/98e8833c17d9e0c060a447cf25081023/c83ae/transformer_encoder.png 1180w,\n/static/98e8833c17d9e0c060a447cf25081023/ce0a7/transformer_encoder.png 1590w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<center>\n인코더 구조, from deeplearning.ai\n</center>\n</br>\n<p>인코더는 크게 <strong>두개의 레이어</strong>로 구성된다. Multi-Head Attention과 FeedForward이다. FeedForward는 일련의 훈련 가능한 신경망으로 구성된 블럭이다. 0️⃣ 입력값을 Embedding하고 Positional Encoding을 적용한 후에, 한 개의 <strong>인코더 블럭</strong>은 1️⃣ Residual을 적용한 Multi-Head Attention을 실행하고 2️⃣ 다시 Residual을 적용한 FeedForward 레이어를 실행한다. 모델을 깊게 만들기 위해 이 인코더 블럭을 여러번 실행한다.</p>\n<p>Residual 레이어는 함수 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>F</mi><mi>n</mi><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mn>1</mn></msub><mo separator=\"true\">,</mo><msub><mi>x</mi><mn>2</mn></msub><mo separator=\"true\">,</mo><mi mathvariant=\"normal\">.</mi><mi mathvariant=\"normal\">.</mi><mi mathvariant=\"normal\">.</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">Fn(x_1, x_2, ...)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">F</span><span class=\"mord mathdefault\">n</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathdefault\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\">.</span><span class=\"mord\">.</span><span class=\"mord\">.</span><span class=\"mclose\">)</span></span></span></span>에 대해 다음을 뜻한다.</p>\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>R</mi><mi>e</mi><mi>s</mi><mi>i</mi><mi>d</mi><mi>u</mi><mi>a</mi><mi>l</mi><mo stretchy=\"false\">(</mo><mi>F</mi><mi>n</mi><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mn>1</mn></msub><mo separator=\"true\">,</mo><msub><mi>x</mi><mn>2</mn></msub><mo separator=\"true\">,</mo><mi mathvariant=\"normal\">.</mi><mi mathvariant=\"normal\">.</mi><mi mathvariant=\"normal\">.</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mi>F</mi><mi>n</mi><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mn>1</mn></msub><mo separator=\"true\">,</mo><msub><mi>x</mi><mn>2</mn></msub><mo separator=\"true\">,</mo><mi mathvariant=\"normal\">.</mi><mi mathvariant=\"normal\">.</mi><mi mathvariant=\"normal\">.</mi><mo stretchy=\"false\">)</mo><mo>+</mo><msub><mi>x</mi><mn>1</mn></msub></mrow><annotation encoding=\"application/x-tex\">Residual(Fn)(x_1, x_2, ...) = Fn(x_1, x_2, ...) + x_1</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.00773em;\">R</span><span class=\"mord mathdefault\">e</span><span class=\"mord mathdefault\">s</span><span class=\"mord mathdefault\">i</span><span class=\"mord mathdefault\">d</span><span class=\"mord mathdefault\">u</span><span class=\"mord mathdefault\">a</span><span class=\"mord mathdefault\" style=\"margin-right:0.01968em;\">l</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">F</span><span class=\"mord mathdefault\">n</span><span class=\"mclose\">)</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathdefault\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\">.</span><span class=\"mord\">.</span><span class=\"mord\">.</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">F</span><span class=\"mord mathdefault\">n</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathdefault\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\">.</span><span class=\"mord\">.</span><span class=\"mord\">.</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.58056em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span></span>\n<p>즉 입력값의 첫번째 값을 함수의 결과값에 더하는 기능을 한다. Residual 레이어를 적용하는 이유는 shortcut 연결 기능을 하기 때문이다. 특히 깊은 네트워크를 학습할 때 Residual이 효과적임이 검증되었다.</p>\n<blockquote>\n<p>Further study - Residual 네트워크를 사용하는 이유 : <a href=\"https://arxiv.org/pdf/1512.03385.pdf\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">He et al., 2015</a></p>\n</blockquote>\n<p>📂 다음 코드는 Trax 라이브러리의 <code class=\"language-text\">TransformerEncoder</code> 모델이다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">def</span> <span class=\"token function\">TransformerEncoder</span><span class=\"token punctuation\">(</span>vocab_size<span class=\"token punctuation\">,</span>\n                       n_classes<span class=\"token operator\">=</span><span class=\"token number\">10</span><span class=\"token punctuation\">,</span>\n                       d_model<span class=\"token operator\">=</span>D_MODEL<span class=\"token punctuation\">,</span>\n                       d_ff<span class=\"token operator\">=</span>D_FF<span class=\"token punctuation\">,</span>\n                       n_layers<span class=\"token operator\">=</span>N_LAYERS<span class=\"token punctuation\">,</span>\n                       n_heads<span class=\"token operator\">=</span>N_HEADS<span class=\"token punctuation\">,</span>\n                       max_len<span class=\"token operator\">=</span>MAX_SEQUENCE_LENGTH<span class=\"token punctuation\">,</span>\n                       dropout<span class=\"token operator\">=</span>DROPOUT_RATE<span class=\"token punctuation\">,</span>\n                       dropout_shared_axes<span class=\"token operator\">=</span>DROPOUT_SHARED_AXES<span class=\"token punctuation\">,</span>\n                       mode<span class=\"token operator\">=</span>MODE<span class=\"token punctuation\">,</span>\n                       ff_activation<span class=\"token operator\">=</span>FF_ACTIVATION_TYPE<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    \n  <span class=\"token keyword\">def</span> <span class=\"token function\">_Dropout</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">return</span> tl<span class=\"token punctuation\">.</span>Dropout<span class=\"token punctuation\">(</span>rate<span class=\"token operator\">=</span>dropout<span class=\"token punctuation\">,</span> shared_axes<span class=\"token operator\">=</span>dropout_shared_axes<span class=\"token punctuation\">,</span> mode<span class=\"token operator\">=</span>mode<span class=\"token punctuation\">)</span>\n\n  <span class=\"token keyword\">def</span> <span class=\"token function\">_EncBlock</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">return</span> _EncoderBlock<span class=\"token punctuation\">(</span>d_model<span class=\"token punctuation\">,</span> d_ff<span class=\"token punctuation\">,</span> n_heads<span class=\"token punctuation\">,</span> dropout<span class=\"token punctuation\">,</span> dropout_shared_axes<span class=\"token punctuation\">,</span>\n                         mode<span class=\"token punctuation\">,</span> ff_activation<span class=\"token punctuation\">)</span>\n\n  <span class=\"token keyword\">return</span> tl<span class=\"token punctuation\">.</span>Serial<span class=\"token punctuation\">(</span>\n      tl<span class=\"token punctuation\">.</span>Branch<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> tl<span class=\"token punctuation\">.</span>PaddingMask<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>  <span class=\"token comment\"># Creates masks from copy of the tokens.</span>\n      tl<span class=\"token punctuation\">.</span>Embedding<span class=\"token punctuation\">(</span>vocab_size<span class=\"token punctuation\">,</span> d_model<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n      _Dropout<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n      tl<span class=\"token punctuation\">.</span>PositionalEncoding<span class=\"token punctuation\">(</span>max_len<span class=\"token operator\">=</span>max_len<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n      <span class=\"token punctuation\">[</span>_EncBlock<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">for</span> _ <span class=\"token keyword\">in</span> <span class=\"token builtin\">range</span><span class=\"token punctuation\">(</span>n_layers<span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>\n      tl<span class=\"token punctuation\">.</span>Select<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> n_in<span class=\"token operator\">=</span><span class=\"token number\">2</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>  <span class=\"token comment\"># Drops the masks.</span>\n      tl<span class=\"token punctuation\">.</span>LayerNorm<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n      tl<span class=\"token punctuation\">.</span>Mean<span class=\"token punctuation\">(</span>axis<span class=\"token operator\">=</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n      tl<span class=\"token punctuation\">.</span>Dense<span class=\"token punctuation\">(</span>n_classes<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n  <span class=\"token punctuation\">)</span></code></pre></div>\n<p><code class=\"language-text\">TransformerEncoder</code>는 토큰화된 텍스트를 <code class=\"language-text\">n_classes</code>개로 분류한다. 함수 반환값의 첫줄에 등장하는 <code class=\"language-text\">tl.Branch</code>는 입력값을 받아서 각각의 함수를 병렬적으로 실행한다. 즉 입력값을 리스트<code class=\"language-text\">[]</code>로 만든 값과 패딩마스크 <code class=\"language-text\">tl.PaddingMask()</code> 값 두개를 반환할 것이다. 두 값 모두 임베딩과 positional encoding을 거쳐 인코더 블럭에 입력된다. 인코더 블럭의 코드에서 데이터와 마스크 쌍 <code class=\"language-text\">(activations, mask)</code>을 입력받는 것을 확인할 수 있다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">def</span> <span class=\"token function\">_EncoderBlock</span><span class=\"token punctuation\">(</span>d_model<span class=\"token punctuation\">,</span>\n                  d_ff<span class=\"token punctuation\">,</span>\n                  n_heads<span class=\"token punctuation\">,</span>\n                  dropout<span class=\"token punctuation\">,</span>\n                  dropout_shared_axes<span class=\"token punctuation\">,</span>\n                  mode<span class=\"token punctuation\">,</span>\n                  ff_activation<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n  <span class=\"token triple-quoted-string string\">\"\"\"Returns a list of layers that implements a Transformer encoder block.\n  The input to the block is a pair (activations, mask) where the mask was\n  created from the original source tokens to prevent attending to the padding\n  part of the input. The block's outputs are the same type/shape as its inputs,\n  so that multiple blocks can be chained together.\n  \"\"\"</span>\n  <span class=\"token keyword\">def</span> <span class=\"token function\">_Attention</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">return</span> tl<span class=\"token punctuation\">.</span>Attention<span class=\"token punctuation\">(</span>d_model<span class=\"token punctuation\">,</span> n_heads<span class=\"token operator\">=</span>n_heads<span class=\"token punctuation\">,</span> dropout<span class=\"token operator\">=</span>dropout<span class=\"token punctuation\">,</span> mode<span class=\"token operator\">=</span>mode<span class=\"token punctuation\">)</span>\n\n  <span class=\"token comment\"># ...</span>\n\n  <span class=\"token keyword\">return</span> <span class=\"token punctuation\">[</span>\n      tl<span class=\"token punctuation\">.</span>Residual<span class=\"token punctuation\">(</span>\n          tl<span class=\"token punctuation\">.</span>LayerNorm<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n          _Attention<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n          _Dropout<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n      <span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n      tl<span class=\"token punctuation\">.</span>Residual<span class=\"token punctuation\">(</span>\n          tl<span class=\"token punctuation\">.</span>LayerNorm<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n          _FFBlock<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n          _Dropout<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n      <span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n  <span class=\"token punctuation\">]</span></code></pre></div>\n<p>여기서 <code class=\"language-text\">tl.Attention</code>은 <code class=\"language-text\">n_heads</code>개의 머리를 가지는 multi-head 셀프-어텐션이며, Attention 레이어와 FeedForward 블럭이 Residual을 거치는 것을 확인할 수 있다. 어텐션 블럭을 지난 후에는 마스크가 필요 없으므로 <code class=\"language-text\">tl.Select()</code>로 <code class=\"language-text\">(activations, mask)</code>쌍에서 앞의 값만 취하고 <code class=\"language-text\">tl.LayerNorm()</code>과 같은 쿼리에 해당하는 열에 대한 덧셈 <code class=\"language-text\">tl.Mean(axis=1)</code>, 그리고 <code class=\"language-text\">n_classes</code>개의 <code class=\"language-text\">tl.Dense()</code> 층을 거쳐 마무리한다.</p>\n<h3 id=\"-dimensionality-setting\" style=\"position:relative;\"><a href=\"#-dimensionality-setting\" aria-label=\" dimensionality setting permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>🔆 Dimensionality Setting</h3>\n<p>Residual 레이어를 적용하기 위해서는 어텐션의 입력값과 결과값의 차원이 같아야한다. 앞의 Multi-Head Attention 그림을 참고하여 배치 크기를 <code class=\"language-text\">batch</code>, 입력 시퀀스 길이를 <code class=\"language-text\">length</code>, 그리고 어텐션의 차원을 <code class=\"language-text\">d_model</code>로 설정하자. Q, K, V가 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mo stretchy=\"false\">(</mo><mi>b</mi><mi>a</mi><mi>t</mi><mi>c</mi><mi>h</mi><mo separator=\"true\">,</mo><mi>l</mi><mi>e</mi><mi>n</mi><mi>g</mi><mi>t</mi><mi>h</mi><mo separator=\"true\">,</mo><msub><mi>d</mi><mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(batch, length, d_{model})</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">(</span><span class=\"mord mathdefault\">b</span><span class=\"mord mathdefault\">a</span><span class=\"mord mathdefault\">t</span><span class=\"mord mathdefault\">c</span><span class=\"mord mathdefault\">h</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.01968em;\">l</span><span class=\"mord mathdefault\">e</span><span class=\"mord mathdefault\">n</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">g</span><span class=\"mord mathdefault\">t</span><span class=\"mord mathdefault\">h</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">d</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.33610799999999996em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">m</span><span class=\"mord mathdefault mtight\">o</span><span class=\"mord mathdefault mtight\">d</span><span class=\"mord mathdefault mtight\">e</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.01968em;\">l</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span> 차원을 가지며, <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msup><mi>W</mi><mi>Q</mi></msup></mrow><annotation encoding=\"application/x-tex\">W^Q</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8413309999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413309999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">Q</span></span></span></span></span></span></span></span></span></span></span>, <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msup><mi>W</mi><mi>K</mi></msup></mrow><annotation encoding=\"application/x-tex\">W^K</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8413309999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413309999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.07153em;\">K</span></span></span></span></span></span></span></span></span></span></span>, <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msup><mi>W</mi><mi>V</mi></msup></mrow><annotation encoding=\"application/x-tex\">W^V</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8413309999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413309999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.22222em;\">V</span></span></span></span></span></span></span></span></span></span></span>는 각각 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mo stretchy=\"false\">(</mo><mi>b</mi><mi>a</mi><mi>t</mi><mi>c</mi><mi>h</mi><mo separator=\"true\">,</mo><mi>l</mi><mi>e</mi><mi>n</mi><mi>g</mi><mi>t</mi><mi>h</mi><mo separator=\"true\">,</mo><msub><mi>d</mi><mi>k</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(batch, length, d_k)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">(</span><span class=\"mord mathdefault\">b</span><span class=\"mord mathdefault\">a</span><span class=\"mord mathdefault\">t</span><span class=\"mord mathdefault\">c</span><span class=\"mord mathdefault\">h</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.01968em;\">l</span><span class=\"mord mathdefault\">e</span><span class=\"mord mathdefault\">n</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">g</span><span class=\"mord mathdefault\">t</span><span class=\"mord mathdefault\">h</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">d</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.33610799999999996em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03148em;\">k</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span>, <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mo stretchy=\"false\">(</mo><mi>b</mi><mi>a</mi><mi>t</mi><mi>c</mi><mi>h</mi><mo separator=\"true\">,</mo><mi>l</mi><mi>e</mi><mi>n</mi><mi>g</mi><mi>t</mi><mi>h</mi><mo separator=\"true\">,</mo><msub><mi>d</mi><mi>k</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(batch, length, d_k)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">(</span><span class=\"mord mathdefault\">b</span><span class=\"mord mathdefault\">a</span><span class=\"mord mathdefault\">t</span><span class=\"mord mathdefault\">c</span><span class=\"mord mathdefault\">h</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.01968em;\">l</span><span class=\"mord mathdefault\">e</span><span class=\"mord mathdefault\">n</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">g</span><span class=\"mord mathdefault\">t</span><span class=\"mord mathdefault\">h</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">d</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.33610799999999996em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03148em;\">k</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span>, <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mo stretchy=\"false\">(</mo><mi>b</mi><mi>a</mi><mi>t</mi><mi>c</mi><mi>h</mi><mo separator=\"true\">,</mo><mi>l</mi><mi>e</mi><mi>n</mi><mi>g</mi><mi>t</mi><mi>h</mi><mo separator=\"true\">,</mo><msub><mi>d</mi><mi>v</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(batch, length, d_v)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">(</span><span class=\"mord mathdefault\">b</span><span class=\"mord mathdefault\">a</span><span class=\"mord mathdefault\">t</span><span class=\"mord mathdefault\">c</span><span class=\"mord mathdefault\">h</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.01968em;\">l</span><span class=\"mord mathdefault\">e</span><span class=\"mord mathdefault\">n</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">g</span><span class=\"mord mathdefault\">t</span><span class=\"mord mathdefault\">h</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">d</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.151392em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03588em;\">v</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span> 차원이라고 하자. 위의 설정에서 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>d</mi><mi>k</mi></msub><mo>=</mo><msub><mi>d</mi><mi>v</mi></msub></mrow><annotation encoding=\"application/x-tex\">d_k = d_v</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.84444em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">d</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.33610799999999996em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03148em;\">k</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.84444em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">d</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.151392em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03588em;\">v</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>로 두며, 그림에서는 이 값이 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>d</mi><mrow><mi>h</mi><mi>e</mi><mi>a</mi><mi>d</mi></mrow></msub></mrow><annotation encoding=\"application/x-tex\">d_{head}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.84444em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">d</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.33610799999999996em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">h</span><span class=\"mord mathdefault mtight\">e</span><span class=\"mord mathdefault mtight\">a</span><span class=\"mord mathdefault mtight\">d</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>로 나타나 있다. 어텐션을 수행하고 난 후 i번 째 어텐션은 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>Z</mi><mi>i</mi></msub><mo>∈</mo><mo stretchy=\"false\">(</mo><mi>b</mi><mi>a</mi><mi>t</mi><mi>c</mi><mi>h</mi><mo separator=\"true\">,</mo><mi>l</mi><mi>e</mi><mi>n</mi><mi>g</mi><mi>t</mi><mi>h</mi><mo separator=\"true\">,</mo><msub><mi>d</mi><mi>v</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">Z_i \\in (batch, length, d_v)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.83333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.07153em;\">Z</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.07153em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">∈</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">(</span><span class=\"mord mathdefault\">b</span><span class=\"mord mathdefault\">a</span><span class=\"mord mathdefault\">t</span><span class=\"mord mathdefault\">c</span><span class=\"mord mathdefault\">h</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.01968em;\">l</span><span class=\"mord mathdefault\">e</span><span class=\"mord mathdefault\">n</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">g</span><span class=\"mord mathdefault\">t</span><span class=\"mord mathdefault\">h</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">d</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.151392em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03588em;\">v</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span> 차원이 되는데, <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>n</mi><mrow><mi>h</mi><mi>e</mi><mi>a</mi><mi>d</mi><mi>s</mi></mrow></msub></mrow><annotation encoding=\"application/x-tex\">n_{heads}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.58056em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">n</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.33610799999999996em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">h</span><span class=\"mord mathdefault mtight\">e</span><span class=\"mord mathdefault mtight\">a</span><span class=\"mord mathdefault mtight\">d</span><span class=\"mord mathdefault mtight\">s</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>개의 어텐션을 결합하고 난 후 처음 입력값과 같은 차원을 얻기 위해 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>n</mi><mrow><mi>h</mi><mi>e</mi><mi>a</mi><mi>d</mi><mi>s</mi></mrow></msub><mo>=</mo><msub><mi>d</mi><mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub><mi mathvariant=\"normal\">/</mi><msub><mi>d</mi><mi>v</mi></msub></mrow><annotation encoding=\"application/x-tex\">n_{heads} = d_{model} / d_v</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.58056em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">n</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.33610799999999996em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">h</span><span class=\"mord mathdefault mtight\">e</span><span class=\"mord mathdefault mtight\">a</span><span class=\"mord mathdefault mtight\">d</span><span class=\"mord mathdefault mtight\">s</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">d</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.33610799999999996em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">m</span><span class=\"mord mathdefault mtight\">o</span><span class=\"mord mathdefault mtight\">d</span><span class=\"mord mathdefault mtight\">e</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.01968em;\">l</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mord\">/</span><span class=\"mord\"><span class=\"mord mathdefault\">d</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.151392em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03588em;\">v</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>로 설정한다. 요약하면:</p>\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>d</mi><mi>k</mi></msub><mo>=</mo><msub><mi>d</mi><mi>v</mi></msub><mo>=</mo><msub><mi>d</mi><mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub><mi mathvariant=\"normal\">/</mi><msub><mi>n</mi><mrow><mi>h</mi><mi>e</mi><mi>a</mi><mi>d</mi><mi>s</mi></mrow></msub></mrow><annotation encoding=\"application/x-tex\">d_k = d_v = d_{model} / n_{heads}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.84444em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">d</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.33610799999999996em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03148em;\">k</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.84444em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">d</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.151392em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03588em;\">v</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">d</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.33610799999999996em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">m</span><span class=\"mord mathdefault mtight\">o</span><span class=\"mord mathdefault mtight\">d</span><span class=\"mord mathdefault mtight\">e</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.01968em;\">l</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mord\">/</span><span class=\"mord\"><span class=\"mord mathdefault\">n</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.33610799999999996em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">h</span><span class=\"mord mathdefault mtight\">e</span><span class=\"mord mathdefault mtight\">a</span><span class=\"mord mathdefault mtight\">d</span><span class=\"mord mathdefault mtight\">s</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span></span>\n<h3 id=\"-positional-encoding\" style=\"position:relative;\"><a href=\"#-positional-encoding\" aria-label=\" positional encoding permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>🔆 Positional Encoding</h3>\n<p>인코딩에 앞서, 단순히 단어 임베딩을 통해 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow><annotation encoding=\"application/x-tex\">QK^T</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.035771em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\">Q</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.07153em;\">K</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413309999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.13889em;\">T</span></span></span></span></span></span></span></span></span></span></span> 2차원 행렬을 계산하면 시퀀스 모델과 달리 단어의 문장 내 위치 정보를 반영할 수 없다. 그러나 어순은 맥락을 파악하는데에 중요한 단서가 된다. 예를 들어 문장 내에서 같은 단어가 등장해도 각 단어는 다른 의미를 가리킬 수 있고, 언어마다 문법 구조에 따라 어순이 다르며, 같은 단어를 사용해도 어순에 따라 다른 의미를 내포할 수 있다.</p>\n<p>따라서 위치 정보를 반영하기 위해 위치에 따른 임의의 값을 설정해 Q, K와 V의 임베딩에 <strong>더하는</strong>데, 이 것을 positional encoding이라고 한다. Trax에서는 여러가지 positional encoding 방법을 지원하고 있는데 (<a href=\"https://trax-ml.readthedocs.io/en/latest/trax.layers.html?highlight=positional%20encoding#module-trax.layers.research.position_encodings\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">link</a>), 원 논문에서는 차원 <code class=\"language-text\">i</code>와 위치 <code class=\"language-text\">pos</code>에 대한 sine 곡선으로 표현했다.</p>\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mtable rowspacing=\"0.24999999999999992em\" columnalign=\"right left\" columnspacing=\"0em\"><mtr><mtd><mstyle scriptlevel=\"0\" displaystyle=\"true\"><mrow><mi>P</mi><msub><mi>E</mi><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mi>o</mi><mi>s</mi><mo separator=\"true\">,</mo><mn>2</mn><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></msub></mrow></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"true\"><mrow><mrow></mrow><mo>=</mo><mi>s</mi><mi>i</mi><mi>n</mi><mo stretchy=\"false\">(</mo><mi>p</mi><mi>o</mi><mi>s</mi><mi mathvariant=\"normal\">/</mi><mn>1000</mn><msup><mn>0</mn><mrow><mn>2</mn><mi>i</mi><mi mathvariant=\"normal\">/</mi><msub><mi>d</mi><mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub></mrow></msup><mo stretchy=\"false\">)</mo></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel=\"0\" displaystyle=\"true\"><mrow><mi>P</mi><msub><mi>E</mi><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mi>o</mi><mi>s</mi><mo separator=\"true\">,</mo><mn>2</mn><mi>i</mi><mo>+</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msub></mrow></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"true\"><mrow><mrow></mrow><mo>=</mo><mi>c</mi><mi>o</mi><mi>s</mi><mo stretchy=\"false\">(</mo><mi>p</mi><mi>o</mi><mi>s</mi><mi mathvariant=\"normal\">/</mi><mn>1000</mn><msup><mn>0</mn><mrow><mn>2</mn><mi>i</mi><mi mathvariant=\"normal\">/</mi><msub><mi>d</mi><mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub></mrow></msup><mo stretchy=\"false\">)</mo></mrow></mstyle></mtd></mtr></mtable><annotation encoding=\"application/x-tex\">\\begin{aligned}\nPE_{(pos,2i)} &amp;= sin(pos/10000^{2i/d_{model}}) \\\\\nPE_{(pos,2i+1)} &amp;= cos(pos/10000^{2i/d_{model}})\n\\end{aligned}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:3.196em;vertical-align:-1.348em;\"></span><span class=\"mord\"><span class=\"mtable\"><span class=\"col-align-r\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.848em;\"><span style=\"top:-3.91em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">P</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.05764em;\">E</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.34480000000000005em;\"><span style=\"top:-2.5198em;margin-left:-0.05764em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mopen mtight\">(</span><span class=\"mord mathdefault mtight\">p</span><span class=\"mord mathdefault mtight\">o</span><span class=\"mord mathdefault mtight\">s</span><span class=\"mpunct mtight\">,</span><span class=\"mord mtight\">2</span><span class=\"mord mathdefault mtight\">i</span><span class=\"mclose mtight\">)</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3551999999999999em;\"><span></span></span></span></span></span></span></span></span><span style=\"top:-2.3120000000000003em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">P</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.05764em;\">E</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.34480000000000005em;\"><span style=\"top:-2.5198em;margin-left:-0.05764em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mopen mtight\">(</span><span class=\"mord mathdefault mtight\">p</span><span class=\"mord mathdefault mtight\">o</span><span class=\"mord mathdefault mtight\">s</span><span class=\"mpunct mtight\">,</span><span class=\"mord mtight\">2</span><span class=\"mord mathdefault mtight\">i</span><span class=\"mbin mtight\">+</span><span class=\"mord mtight\">1</span><span class=\"mclose mtight\">)</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3551999999999999em;\"><span></span></span></span></span></span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.348em;\"><span></span></span></span></span></span><span class=\"col-align-l\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.848em;\"><span style=\"top:-3.91em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\"></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mord mathdefault\">s</span><span class=\"mord mathdefault\">i</span><span class=\"mord mathdefault\">n</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\">p</span><span class=\"mord mathdefault\">o</span><span class=\"mord mathdefault\">s</span><span class=\"mord\">/</span><span class=\"mord\">1</span><span class=\"mord\">0</span><span class=\"mord\">0</span><span class=\"mord\">0</span><span class=\"mord\"><span class=\"mord\">0</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.938em;\"><span style=\"top:-3.113em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">2</span><span class=\"mord mathdefault mtight\">i</span><span class=\"mord mtight\">/</span><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">d</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3448em;\"><span style=\"top:-2.3487714285714287em;margin-left:0em;margin-right:0.07142857142857144em;\"><span class=\"pstrut\" style=\"height:2.5em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">m</span><span class=\"mord mathdefault mtight\">o</span><span class=\"mord mathdefault mtight\">d</span><span class=\"mord mathdefault mtight\">e</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.01968em;\">l</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15122857142857138em;\"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span><span style=\"top:-2.3120000000000003em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\"></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mord mathdefault\">c</span><span class=\"mord mathdefault\">o</span><span class=\"mord mathdefault\">s</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\">p</span><span class=\"mord mathdefault\">o</span><span class=\"mord mathdefault\">s</span><span class=\"mord\">/</span><span class=\"mord\">1</span><span class=\"mord\">0</span><span class=\"mord\">0</span><span class=\"mord\">0</span><span class=\"mord\"><span class=\"mord\">0</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.938em;\"><span style=\"top:-3.113em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">2</span><span class=\"mord mathdefault mtight\">i</span><span class=\"mord mtight\">/</span><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">d</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3448em;\"><span style=\"top:-2.3487714285714287em;margin-left:0em;margin-right:0.07142857142857144em;\"><span class=\"pstrut\" style=\"height:2.5em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">m</span><span class=\"mord mathdefault mtight\">o</span><span class=\"mord mathdefault mtight\">d</span><span class=\"mord mathdefault mtight\">e</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.01968em;\">l</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15122857142857138em;\"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.348em;\"><span></span></span></span></span></span></span></span></span></span></span></span>\n<p>즉 positional encoding의 각 차원은 사인 곡선에 대응한다. PE는 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mn>2</mn><mi>π</mi></mrow><annotation encoding=\"application/x-tex\">2\\pi</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">2</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">π</span></span></span></span>에서 부터 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mn>10000</mn><mo>⋅</mo><mn>2</mn><mi>π</mi></mrow><annotation encoding=\"application/x-tex\">10000 \\cdot 2\\pi</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">1</span><span class=\"mord\">0</span><span class=\"mord\">0</span><span class=\"mord\">0</span><span class=\"mord\">0</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">⋅</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">2</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">π</span></span></span></span>까지의 기하학적 형태를 나타낸다. 이렇게 함으로서 상수 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>k</mi></mrow><annotation encoding=\"application/x-tex\">k</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03148em;\">k</span></span></span></span>에 대해 상대적인 위치인 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>P</mi><mi>E</mi><mo stretchy=\"false\">(</mo><mi>p</mi><mi>o</mi><mi>s</mi><mo>+</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">PE(pos+k)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">P</span><span class=\"mord mathdefault\" style=\"margin-right:0.05764em;\">E</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\">p</span><span class=\"mord mathdefault\">o</span><span class=\"mord mathdefault\">s</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03148em;\">k</span><span class=\"mclose\">)</span></span></span></span>를 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>P</mi><mi>E</mi><mo stretchy=\"false\">(</mo><mi>p</mi><mi>o</mi><mi>s</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">PE(pos)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">P</span><span class=\"mord mathdefault\" style=\"margin-right:0.05764em;\">E</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\">p</span><span class=\"mord mathdefault\">o</span><span class=\"mord mathdefault\">s</span><span class=\"mclose\">)</span></span></span></span>의 선형 함수로 나타낼 수 있다. trax.layers.Attention에서 정의하고 있는 <code class=\"language-text\">PositionalEncoding</code> 도 같은 방법을 적용했다.</p>\n<p>📂 다음 코드는 Trax 라이브러리의 <code class=\"language-text\">PositionalEncoding</code> 레이어다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">class</span> <span class=\"token class-name\">PositionalEncoding</span><span class=\"token punctuation\">(</span>base<span class=\"token punctuation\">.</span>Layer<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n  \n  <span class=\"token comment\"># ...</span>\n\n  <span class=\"token keyword\">def</span> <span class=\"token function\">forward</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> inputs<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token triple-quoted-string string\">\"\"\"Returns the input activations, with added positional information.\"\"\"</span>\n    weights <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>weights\n\n    <span class=\"token comment\"># ...</span>\n\n    emb <span class=\"token operator\">=</span> fastmath<span class=\"token punctuation\">.</span>dynamic_slice_in_dim<span class=\"token punctuation\">(</span>\n        weights<span class=\"token punctuation\">,</span> self<span class=\"token punctuation\">.</span>state<span class=\"token punctuation\">,</span> inputs<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> axis<span class=\"token operator\">=</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span>\n    self<span class=\"token punctuation\">.</span>state <span class=\"token operator\">+=</span> inputs<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span>\n    <span class=\"token keyword\">return</span> inputs <span class=\"token operator\">+</span> emb\n\n  <span class=\"token keyword\">def</span> <span class=\"token function\">init_weights_and_state</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> input_signature<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token triple-quoted-string string\">\"\"\"Randomly initializes the positional encoding vectors.\n    \"\"\"</span>\n    d_feature <span class=\"token operator\">=</span> input_signature<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">[</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span>\n    <span class=\"token keyword\">if</span> self<span class=\"token punctuation\">.</span>_d_feature <span class=\"token keyword\">is</span> <span class=\"token keyword\">not</span> <span class=\"token boolean\">None</span><span class=\"token punctuation\">:</span>\n      d_feature <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>_d_feature\n    pe <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>zeros<span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">.</span>_max_len<span class=\"token punctuation\">,</span> d_feature<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> dtype<span class=\"token operator\">=</span>np<span class=\"token punctuation\">.</span>float32<span class=\"token punctuation\">)</span>\n    position <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>arange<span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span> self<span class=\"token punctuation\">.</span>_max_len<span class=\"token punctuation\">)</span><span class=\"token punctuation\">[</span><span class=\"token punctuation\">:</span><span class=\"token punctuation\">,</span> np<span class=\"token punctuation\">.</span>newaxis<span class=\"token punctuation\">]</span>\n    div_term <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>exp<span class=\"token punctuation\">(</span>\n        np<span class=\"token punctuation\">.</span>arange<span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span> d_feature<span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">*</span> <span class=\"token operator\">-</span><span class=\"token punctuation\">(</span>np<span class=\"token punctuation\">.</span>log<span class=\"token punctuation\">(</span><span class=\"token number\">10000.0</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">/</span> d_feature<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    pe<span class=\"token punctuation\">[</span><span class=\"token punctuation\">:</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">:</span><span class=\"token punctuation\">:</span><span class=\"token number\">2</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>sin<span class=\"token punctuation\">(</span>position <span class=\"token operator\">*</span> div_term<span class=\"token punctuation\">)</span>\n    pe<span class=\"token punctuation\">[</span><span class=\"token punctuation\">:</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">:</span><span class=\"token punctuation\">:</span><span class=\"token number\">2</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>cos<span class=\"token punctuation\">(</span>position <span class=\"token operator\">*</span> div_term<span class=\"token punctuation\">)</span>  <span class=\"token comment\"># [self._max_len, d_feature]</span>\n    <span class=\"token keyword\">if</span> self<span class=\"token punctuation\">.</span>_use_bfloat16<span class=\"token punctuation\">:</span>\n      pe <span class=\"token operator\">=</span> pe<span class=\"token punctuation\">.</span>astype<span class=\"token punctuation\">(</span>jnp<span class=\"token punctuation\">.</span>bfloat16<span class=\"token punctuation\">)</span>\n    w <span class=\"token operator\">=</span> jnp<span class=\"token punctuation\">.</span>array<span class=\"token punctuation\">(</span>pe<span class=\"token punctuation\">)</span>  <span class=\"token comment\"># Trainable parameters, initialized above.</span>\n    <span class=\"token comment\"># ...</span></code></pre></div>\n<p><code class=\"language-text\">init_weights_and_state</code> 함수는 <code class=\"language-text\">ShapeDtype</code> 객체를 입력받아서 임베딩 크기 <code class=\"language-text\">(max_len, d_feature)</code> 크기의 벡터 <code class=\"language-text\">pe</code>를 초기화 한다. <code class=\"language-text\">pe</code> 벡터의 짝수 행에는 <code class=\"language-text\">positon * div_term</code>의 sine 값을 할당하고 홀수 행에는 cosine 값을 할당한다. 이 값을 <code class=\"language-text\">weights</code>로 전달해 함수<code class=\"language-text\">forward</code>에서 <code class=\"language-text\">emb</code> 값으로 입력값에 더해 전달하고 있다. 즉 사인과 코사인 값으로 위치정보를 인코딩해 입력값에 더하는 방식으로 positional encoding을 수행한다.</p>\n<h3 id=\"-encoder-self-attention\" style=\"position:relative;\"><a href=\"#-encoder-self-attention\" aria-label=\" encoder self attention permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>📣 Encoder Self-Attention</h3>\n<p>인코더는 셀프-어텐션 레이어를 활용한다. 셀프-어텐션은 주어진 데이터의 부분값과 다른 부분들의 관계를 파악하는 방법이다. 즉 문장 데이터에서 셀프-어텐션은 문장 내의 단어 문맥을 파악한다. 앞서 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>Q</mi></mrow><annotation encoding=\"application/x-tex\">Q</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8777699999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\">Q</span></span></span></span>, <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>K</mi></mrow><annotation encoding=\"application/x-tex\">K</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.07153em;\">K</span></span></span></span>, <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>V</mi></mrow><annotation encoding=\"application/x-tex\">V</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">V</span></span></span></span>의 값은 어텐션에 따라 다르다고 했는데, 셀프-어텐션 레이어에서는 모든 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>Q</mi></mrow><annotation encoding=\"application/x-tex\">Q</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8777699999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\">Q</span></span></span></span>, <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>K</mi></mrow><annotation encoding=\"application/x-tex\">K</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.07153em;\">K</span></span></span></span>, <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>V</mi></mrow><annotation encoding=\"application/x-tex\">V</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">V</span></span></span></span>가 같은 시퀀스, 즉 인코더의 이전 레이어의 결과값에서 온다. 주어진 문장이 있을 때, 임의의 단어 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>w</mi></mrow><annotation encoding=\"application/x-tex\">w</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02691em;\">w</span></span></span></span>에 대응하는 임베딩에 대한 가중치를 학습할 수 있다. 임베딩에 가중치를 곱해 얻은 쿼리 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>q</mi></mrow><annotation encoding=\"application/x-tex\">q</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">q</span></span></span></span>에 대해 모든 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>k</mi><mo>∈</mo><mi>K</mi></mrow><annotation encoding=\"application/x-tex\">k \\in K</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.73354em;vertical-align:-0.0391em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03148em;\">k</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">∈</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.07153em;\">K</span></span></span></span>와 dot product로 비교해서 유사도 점수를 얻을 수 있다. 그 다음 softmax 함수를 통해 모든 가중치를 더해서 1이 되는 양수값으로 변환한다. 그 후 두번째 dot product로 단어 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>w</mi></mrow><annotation encoding=\"application/x-tex\">w</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02691em;\">w</span></span></span></span>에 대응하는 다른 단어들의 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>v</mi></mrow><annotation encoding=\"application/x-tex\">v</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">v</span></span></span></span>값들을 구해 다시 학습한 가중치 행렬을 곱함으로서 모든 가중치 합을 구한다. 이것이 단어 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>w</mi></mrow><annotation encoding=\"application/x-tex\">w</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02691em;\">w</span></span></span></span>에 대한 어텐션이다.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/eb35de3808469f23000ff41fd4fd1080/5819f/attention2.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 106.08108108108108%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAVCAYAAABG1c6oAAAACXBIWXMAABYlAAAWJQFJUiTwAAABzElEQVQ4y5VUTXOCMBT0/x/7N3rusb302ENbrZ1WERQURFCUD1G+tnkZQhOlSN8Mk5BHNvv2bRhUVQUKGi/nfR7xvRgHfwFS5HmOKIqUPL2XZXkF1BvweDwq+SRJWoFaAS+BV6sVPM/jc8FqPp8jDMP+DGXA7XaL3W6nbFqv14jj+GpPJ0Mx+r7PS5TX9/s9gn1wVU0nQ4ogCDCbzXA4HJSSHceBYRhI07RV+9am0GbbtjEcDnnZMiCBjcdjfmCbVH+WLJoi60VBTZJ17QRs5jWb5XLZMBQ50zS5jv8CLMIzqrzCZDrBxnFZkq2dc57TNI0fJMtwGzA6I/ZPMBcLBH6ALM6QxzkSP8T4fYSv7y84tqP4sR1QKo10MnS9KS+ODlizAyzLwoKNbV5UAC8bkmUZTMtmGxOkSYTVTOOl28w2DnOAvKezy0VRcDMXTJ/Pt2foDMhbWiiygudddlPsGvBmUwQgMdCeHvFwf4eX1w+m/m+eGuK67m1AxYt5icjbQdcNxjZVOrrZbPh97sewqj8qKQku/rG+ywKQ2PVieNltiul02nSzlMzeW8PGizXiaDRqbCMAdWYl+ieKtX4M64dKO51OSp782XX1fgCQRmxdLfh82AAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"attention2\"\n        title=\"attention2\"\n        src=\"/static/eb35de3808469f23000ff41fd4fd1080/fcda8/attention2.png\"\n        srcset=\"/static/eb35de3808469f23000ff41fd4fd1080/12f09/attention2.png 148w,\n/static/eb35de3808469f23000ff41fd4fd1080/e4a3f/attention2.png 295w,\n/static/eb35de3808469f23000ff41fd4fd1080/fcda8/attention2.png 590w,\n/static/eb35de3808469f23000ff41fd4fd1080/efc66/attention2.png 885w,\n/static/eb35de3808469f23000ff41fd4fd1080/5819f/attention2.png 1042w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<center>\n셀프-어텐션, Vaswani et al., 2017\n</center>\n</br>\n<p>위 그림은 <em>\"making\"</em> 단어에 대한 어텐션을 표현하고 있다. 위 어텐션의 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>Q</mi></mrow><annotation encoding=\"application/x-tex\">Q</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8777699999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\">Q</span></span></span></span>, <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>K</mi></mrow><annotation encoding=\"application/x-tex\">K</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.07153em;\">K</span></span></span></span>, <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>V</mi></mrow><annotation encoding=\"application/x-tex\">V</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">V</span></span></span></span>는 모두 한 문장 <em>\"It is in this spirit that a majority of American governments have passed new laws since 2009 making the registration or voting process more difficult. <EOS> <pad> ...\"</em>에서 얻어진다. 다른 색깔은 다른 head를 나타내며 색이 선명할수록 관계도가 높다. <em>\"making\"</em>과 연관된 head는 <em>\"making ... more difficult\"</em> 구문을 완성한다.</p>\n<p>물론 효율을 위해 우리는 행렬 단위로 어텐션을 연산한다. 주어진 문장의 단어 임베딩이 임베딩 차원 emb에 대해 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>n</mi><mrow><mi>s</mi><mi>e</mi><mi>q</mi></mrow></msub><mo separator=\"true\">,</mo><mi>e</mi><mi>m</mi><mi>b</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(n_{seq}, emb)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.036108em;vertical-align:-0.286108em;\"></span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathdefault\">n</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15139200000000003em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">s</span><span class=\"mord mathdefault mtight\">e</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03588em;\">q</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.286108em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathdefault\">e</span><span class=\"mord mathdefault\">m</span><span class=\"mord mathdefault\">b</span><span class=\"mclose\">)</span></span></span></span> 차원이라고 하자. 우리는 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mo stretchy=\"false\">(</mo><mi>e</mi><mi>m</mi><mi>b</mi><mo separator=\"true\">,</mo><msub><mi>d</mi><mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(emb, d_{model})</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">(</span><span class=\"mord mathdefault\">e</span><span class=\"mord mathdefault\">m</span><span class=\"mord mathdefault\">b</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">d</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.33610799999999996em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">m</span><span class=\"mord mathdefault mtight\">o</span><span class=\"mord mathdefault mtight\">d</span><span class=\"mord mathdefault mtight\">e</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.01968em;\">l</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span>차원의 가중치 행렬 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msup><mi>W</mi><mi>Q</mi></msup></mrow><annotation encoding=\"application/x-tex\">W^Q</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8413309999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413309999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">Q</span></span></span></span></span></span></span></span></span></span></span>, <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msup><mi>W</mi><mi>K</mi></msup></mrow><annotation encoding=\"application/x-tex\">W^K</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8413309999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413309999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.07153em;\">K</span></span></span></span></span></span></span></span></span></span></span>, 그리고 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msup><mi>W</mi><mi>V</mi></msup></mrow><annotation encoding=\"application/x-tex\">W^V</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8413309999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413309999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.22222em;\">V</span></span></span></span></span></span></span></span></span></span></span>을 학습한다. 단어에 대해 했던 것과 마찬가지로 임베딩 행렬에 가중치 행렬을 곱해서 각각 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>n</mi><mrow><mi>s</mi><mi>e</mi><mi>q</mi></mrow></msub><mo separator=\"true\">,</mo><msub><mi>d</mi><mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(n_{seq}, d_{model})</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.036108em;vertical-align:-0.286108em;\"></span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathdefault\">n</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15139200000000003em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">s</span><span class=\"mord mathdefault mtight\">e</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03588em;\">q</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.286108em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">d</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.33610799999999996em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">m</span><span class=\"mord mathdefault mtight\">o</span><span class=\"mord mathdefault mtight\">d</span><span class=\"mord mathdefault mtight\">e</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.01968em;\">l</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span> 차원의 행렬 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>Q</mi></mrow><annotation encoding=\"application/x-tex\">Q</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8777699999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\">Q</span></span></span></span>, <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>K</mi></mrow><annotation encoding=\"application/x-tex\">K</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.07153em;\">K</span></span></span></span>, 그리고 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>V</mi></mrow><annotation encoding=\"application/x-tex\">V</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">V</span></span></span></span>를 도출할 수 있다. 이후의 어텐션 연산은 scaled-dot product 어텐션에서 살펴본 것과 같다. Multi-head 셀프-어텐션을 실행한다면, <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>i</mi><mo>∈</mo><msub><mi>n</mi><mrow><mi>h</mi><mi>e</mi><mi>a</mi><mi>d</mi><mi>s</mi></mrow></msub></mrow><annotation encoding=\"application/x-tex\">i \\in n_{heads}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69862em;vertical-align:-0.0391em;\"></span><span class=\"mord mathdefault\">i</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">∈</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.58056em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">n</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.33610799999999996em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">h</span><span class=\"mord mathdefault mtight\">e</span><span class=\"mord mathdefault mtight\">a</span><span class=\"mord mathdefault mtight\">d</span><span class=\"mord mathdefault mtight\">s</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>에 대해 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msubsup><mi>W</mi><mi>i</mi><mi>Q</mi></msubsup></mrow><annotation encoding=\"application/x-tex\">W^Q_i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.236103em;vertical-align:-0.276864em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.959239em;\"><span style=\"top:-2.4231360000000004em;margin-left:-0.13889em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span><span style=\"top:-3.1809080000000005em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">Q</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.276864em;\"><span></span></span></span></span></span></span></span></span></span>, <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msubsup><mi>W</mi><mi>i</mi><mi>K</mi></msubsup></mrow><annotation encoding=\"application/x-tex\">W^K_i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.0999949999999998em;vertical-align:-0.258664em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413309999999999em;\"><span style=\"top:-2.441336em;margin-left:-0.13889em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.07153em;\">K</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.258664em;\"><span></span></span></span></span></span></span></span></span></span>, <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msubsup><mi>W</mi><mi>i</mi><mi>V</mi></msubsup></mrow><annotation encoding=\"application/x-tex\">W^V_i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.0999949999999998em;vertical-align:-0.258664em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413309999999999em;\"><span style=\"top:-2.441336em;margin-left:-0.13889em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.22222em;\">V</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.258664em;\"><span></span></span></span></span></span></span></span></span></span>를 훈련하고 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>Q</mi><mi>i</mi></msub></mrow><annotation encoding=\"application/x-tex\">Q_i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8777699999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">Q</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>, <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>K</mi><mi>i</mi></msub></mrow><annotation encoding=\"application/x-tex\">K_i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.83333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.07153em;\">K</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.07153em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>, <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>V</mi><mi>i</mi></msub></mrow><annotation encoding=\"application/x-tex\">V_i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.83333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">V</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.22222em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>로 어텐션을 연산한 후, 어텐션 결과값 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>Z</mi><mi>i</mi></msub></mrow><annotation encoding=\"application/x-tex\">Z_i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.83333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.07153em;\">Z</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.07153em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>를 결합한 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>Z</mi></mrow><annotation encoding=\"application/x-tex\">Z</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.07153em;\">Z</span></span></span></span>에 학습한 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>n</mi><mrow><mi>s</mi><mi>e</mi><mi>q</mi></mrow></msub><mo separator=\"true\">,</mo><msub><mi>d</mi><mi>v</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(n_{seq}, d_v)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.036108em;vertical-align:-0.286108em;\"></span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathdefault\">n</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15139200000000003em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">s</span><span class=\"mord mathdefault mtight\">e</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03588em;\">q</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.286108em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">d</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.151392em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03588em;\">v</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span> 차원의 가중치 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msup><mi>W</mi><mi>O</mi></msup></mrow><annotation encoding=\"application/x-tex\">W^O</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8413309999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413309999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.02778em;\">O</span></span></span></span></span></span></span></span></span></span></span> 행렬을 곱해 최종적으로 multi-head 어텐션을 만들 수 있다.</p>\n<p>📂 다음 코드는 Trax 라이브러리의 <code class=\"language-text\">Attention</code> 모델로, Multi-Head 셀프-어텐션을 수행한다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">def</span> <span class=\"token function\">Attention</span><span class=\"token punctuation\">(</span>d_feature<span class=\"token punctuation\">,</span> n_heads<span class=\"token operator\">=</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> dropout<span class=\"token operator\">=</span><span class=\"token number\">0.0</span><span class=\"token punctuation\">,</span> mode<span class=\"token operator\">=</span><span class=\"token string\">'train'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n  <span class=\"token triple-quoted-string string\">\"\"\"Returns a layer that maps `(vectors, mask)` to `(new_vectors, mask)`.\n  This layer type represents one pass of multi-head self-attention, from vector\n  set to vector set, using masks to represent out-of-bound (e.g., padding)\n  positions. ...\n  \"\"\"</span>\n  <span class=\"token keyword\">return</span> cb<span class=\"token punctuation\">.</span>Serial<span class=\"token punctuation\">(</span>\n      cb<span class=\"token punctuation\">.</span>Select<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n      AttentionQKV<span class=\"token punctuation\">(</span>d_feature<span class=\"token punctuation\">,</span> n_heads<span class=\"token operator\">=</span>n_heads<span class=\"token punctuation\">,</span> dropout<span class=\"token operator\">=</span>dropout<span class=\"token punctuation\">,</span> mode<span class=\"token operator\">=</span>mode<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n  <span class=\"token punctuation\">)</span></code></pre></div>\n<p>디코더에서 살펴보겠지만, <code class=\"language-text\">AttentionQKV</code>는 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>Q</mi></mrow><annotation encoding=\"application/x-tex\">Q</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8777699999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\">Q</span></span></span></span>, <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>K</mi></mrow><annotation encoding=\"application/x-tex\">K</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.07153em;\">K</span></span></span></span>, <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>V</mi></mrow><annotation encoding=\"application/x-tex\">V</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">V</span></span></span></span>를 다른 입력에서 가져올 수 있다. 따라서 셀프-어텐션을 구현하기 위해 <code class=\"language-text\">Attention</code>은 <code class=\"language-text\">Select</code>로 첫번째 입력값을 3개로 복제한 값을 <code class=\"language-text\">AttentionQKV</code>에 전달한다.</p>\n<p>또 인코더의 셀프-어텐션은 <strong>Padding Mask</strong>를 활용한다. </p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">def</span> <span class=\"token function\">PaddingMask</span><span class=\"token punctuation\">(</span>pad<span class=\"token operator\">=</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n  <span class=\"token triple-quoted-string string\">\"\"\"Returns a layer that maps integer sequences to padding masks.\n  The layer expects as input a batch of integer sequences. The layer output is\n  an N-D array that marks for each sequence position whether the integer (e.g.,\n  a token ID) in that position represents padding -- value ``pad`` -- versus\n  text/content -- all other values. The padding mask shape is\n  (batch_size, 1, 1, encoder_sequence_length), such that axis 1 will broadcast\n  to cover any number of attention heads and axis 2 will broadcast to cover\n  decoder sequence positions. ...\n  \"\"\"</span>\n  <span class=\"token keyword\">def</span> <span class=\"token function\">f</span><span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">if</span> <span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">)</span> <span class=\"token operator\">!=</span> <span class=\"token number\">2</span><span class=\"token punctuation\">:</span>\n      <span class=\"token keyword\">raise</span> ValueError<span class=\"token punctuation\">(</span>\n          <span class=\"token string-interpolation\"><span class=\"token string\">f'Input to PaddingMask must be a 2-D array with shape '</span></span>\n          <span class=\"token string-interpolation\"><span class=\"token string\">f'(batch_size, sequence_length); instead got shape </span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>x<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">}</span></span><span class=\"token string\">.'</span></span><span class=\"token punctuation\">)</span>\n    batch_size <span class=\"token operator\">=</span> x<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span>\n    sequence_length <span class=\"token operator\">=</span> x<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span>\n    content_positions <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span>x <span class=\"token operator\">!=</span> pad<span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">return</span> content_positions<span class=\"token punctuation\">.</span>reshape<span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span>batch_size<span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">,</span> sequence_length<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n  <span class=\"token keyword\">return</span> Fn<span class=\"token punctuation\">(</span><span class=\"token string-interpolation\"><span class=\"token string\">f'PaddingMask(</span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>pad<span class=\"token punctuation\">}</span></span><span class=\"token string\">)'</span></span><span class=\"token punctuation\">,</span> f<span class=\"token punctuation\">)</span></code></pre></div>\n<p>즉 패딩 토큰 <code class=\"language-text\">pad</code>로 설정된 값과 같은 부분을 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">0</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">0</span></span></span></span>으로 바꾼다. 출력 차원은 <code class=\"language-text\">(batch_size, 1, 1, sequence_length)</code>으로, 어텐션과 차원을 맞추기 위해 1번과 2번 축을 추가한다.</p>\n<h2 id=\"2-decoder\" style=\"position:relative;\"><a href=\"#2-decoder\" aria-label=\"2 decoder permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>2. Decoder</h2>\n<p>트랜스포머의 디코더는 두 가지 어텐션을 거친다. 첫 번째 어텐션은 인코더에서와 같은 셀프-어텐션이고, 두번째는 인코더-디코더 어텐션이다. 인코더만 사용해 모델을 만들 수 있었던 것처럼, 디코더만 사용해서 모델을 형성할 수도 있다. 디코더만 사용할 때에는 Multi-Head 셀프-어텐션만 사용할 수 있다.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/88080a9fdad0d92f75e9cce5c0b7a291/ce92a/transformer_decoder.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 49.32432432432432%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAYAAAC0VX7mAAAACXBIWXMAABYlAAAWJQFJUiTwAAACVklEQVQoz2WS6U8aYRDG+bubNmn80KQfqkkPrwhqqq2xIijWI+JREatYREG5BFxhuZdl2eVe4NfXNU2TdpIn8868k2dOm67rpDNpJElClmVKpTLZbNaynzAYDPhXemafXr/HkNF/f7Z+v4+iKMQTcQvRWJRUKmX5VFVlYA5o1w0MRaOl6mglhdRtgkQ4Si1foau1aKuGhaGItf1hbvYanF4fCRzSNg3L1+52rMCzdy68Y0u4X82y9mIKz2s7zpfTHLz5QnDCw+X4Jhdv1zHytWdC0zRJ3idxrq+x7nIi52WLUK2rIuuQeqFBLpkjehUlGooRDoQJX0a4E3YqkiJ9m+YhKtHv9p8JW4ZO6L7ApOeKqa0QsccKo0EPrWEw6up0I06qZ4vEtt5z654QGCfiGie18wnJOyMwSze4LIgUbMPh0JpVXMxue9PN/u4W2UeRTVRtGAa6bvDNucHCwhIOuwPHnIN5xwL2OTufl1dYWXOz6vKwKmJqNfW5Qr3ZJnv7k0P7GMfzY+QTQUSnGA0NvWOyFqyw6JOY2bkRHVyJToJ82AgwvX3NzG6YxZMHvv6qojRFy7VazdpqOpPhLhojm5OtykajEZqmiS2btGp1qrk8ckaiLLRWrqKWKtTLFZR8UaBAr/G0ZRObUlXIFWSq9zLej6scTTppFevWUrS6OJVuk7PUEXuhDb4HnOxfe7h4OCEg+QgIHcz5uS6ecy4d02jX/57NoNMjcn5F7PIGU7yfpNPu0jHbBGU//uQB/sQBpwkvP+728MX3LX0hiEJFP8G8T5yezm+NAsZ5jPGqnQAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"transformer decoder\"\n        title=\"transformer decoder\"\n        src=\"/static/88080a9fdad0d92f75e9cce5c0b7a291/fcda8/transformer_decoder.png\"\n        srcset=\"/static/88080a9fdad0d92f75e9cce5c0b7a291/12f09/transformer_decoder.png 148w,\n/static/88080a9fdad0d92f75e9cce5c0b7a291/e4a3f/transformer_decoder.png 295w,\n/static/88080a9fdad0d92f75e9cce5c0b7a291/fcda8/transformer_decoder.png 590w,\n/static/88080a9fdad0d92f75e9cce5c0b7a291/efc66/transformer_decoder.png 885w,\n/static/88080a9fdad0d92f75e9cce5c0b7a291/c83ae/transformer_decoder.png 1180w,\n/static/88080a9fdad0d92f75e9cce5c0b7a291/ce92a/transformer_decoder.png 1459w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<center>\n디코더 구조, from deeplearning.ai\n</center>\n</br>\n<p>인코더에서와 마찬가지로 디코더에서도 입력값 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>Q</mi></mrow><annotation encoding=\"application/x-tex\">Q</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8777699999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\">Q</span></span></span></span>, <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>K</mi></mrow><annotation encoding=\"application/x-tex\">K</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.07153em;\">K</span></span></span></span>, <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>V</mi></mrow><annotation encoding=\"application/x-tex\">V</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">V</span></span></span></span>를 임베드하고 Positional Encoding 처리를 한 후에 Residual을 포함한 1️⃣ Multi-Head Attention과 2️⃣ FeedForward 레이어를 거친다. 디코더 블럭을 여러번 거친 후에 훈련 가능한 Linear 레이어와 Softmax 함수를 거치는데, 이 부분은 수행하고자 하는 과제에 따라 변경할 수 있다.</p>\n<p>📂 다음 코드는 Trax 라이브러리의 <code class=\"language-text\">TransformerLM</code>로, 디코더만 구현된 함수이다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">def</span> <span class=\"token function\">TransformerLM</span><span class=\"token punctuation\">(</span>vocab_size<span class=\"token punctuation\">,</span>\n                  d_model<span class=\"token operator\">=</span>D_MODEL<span class=\"token punctuation\">,</span>\n                  d_ff<span class=\"token operator\">=</span>D_FF<span class=\"token punctuation\">,</span>\n                  n_layers<span class=\"token operator\">=</span>N_LAYERS<span class=\"token punctuation\">,</span>\n                  n_heads<span class=\"token operator\">=</span>N_HEADS<span class=\"token punctuation\">,</span>\n                  max_len<span class=\"token operator\">=</span>MAX_SEQUENCE_LENGTH<span class=\"token punctuation\">,</span>\n                  dropout<span class=\"token operator\">=</span>DROPOUT_RATE<span class=\"token punctuation\">,</span>\n                  dropout_shared_axes<span class=\"token operator\">=</span>DROPOUT_SHARED_AXES<span class=\"token punctuation\">,</span>\n                  mode<span class=\"token operator\">=</span>MODE<span class=\"token punctuation\">,</span>\n                  ff_activation<span class=\"token operator\">=</span>FF_ACTIVATION_TYPE<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n\n  <span class=\"token comment\"># ...</span>\n\n  <span class=\"token keyword\">def</span> <span class=\"token function\">_DecBlock</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">return</span> _DecoderBlock<span class=\"token punctuation\">(</span>d_model<span class=\"token punctuation\">,</span> d_ff<span class=\"token punctuation\">,</span> n_heads<span class=\"token punctuation\">,</span> dropout<span class=\"token punctuation\">,</span> dropout_shared_axes<span class=\"token punctuation\">,</span>\n                         mode<span class=\"token punctuation\">,</span> ff_activation<span class=\"token punctuation\">)</span>\n\n  <span class=\"token keyword\">return</span> tl<span class=\"token punctuation\">.</span>Serial<span class=\"token punctuation\">(</span>\n      tl<span class=\"token punctuation\">.</span>ShiftRight<span class=\"token punctuation\">(</span>mode<span class=\"token operator\">=</span>mode<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>  <span class=\"token comment\"># Teacher Forcing</span>\n      tl<span class=\"token punctuation\">.</span>Embedding<span class=\"token punctuation\">(</span>vocab_size<span class=\"token punctuation\">,</span> d_model<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n      _Dropout<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n      tl<span class=\"token punctuation\">.</span>PositionalEncoding<span class=\"token punctuation\">(</span>max_len<span class=\"token operator\">=</span>max_len<span class=\"token punctuation\">,</span> mode<span class=\"token operator\">=</span>mode<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n      <span class=\"token punctuation\">[</span>_DecBlock<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">for</span> _ <span class=\"token keyword\">in</span> <span class=\"token builtin\">range</span><span class=\"token punctuation\">(</span>n_layers<span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>\n      tl<span class=\"token punctuation\">.</span>LayerNorm<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n      tl<span class=\"token punctuation\">.</span>Dense<span class=\"token punctuation\">(</span>vocab_size<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n  <span class=\"token punctuation\">)</span></code></pre></div>\n<p><code class=\"language-text\">_DecoderBlock</code>은 다음과 같다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">def</span> <span class=\"token function\">_DecoderBlock</span><span class=\"token punctuation\">(</span>d_model<span class=\"token punctuation\">,</span>\n                  d_ff<span class=\"token punctuation\">,</span>\n                  n_heads<span class=\"token punctuation\">,</span>\n                  dropout<span class=\"token punctuation\">,</span>\n                  dropout_shared_axes<span class=\"token punctuation\">,</span>\n                  mode<span class=\"token punctuation\">,</span>\n                  ff_activation<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n  <span class=\"token comment\"># ...</span>\n\n  <span class=\"token keyword\">return</span> <span class=\"token punctuation\">[</span>\n      tl<span class=\"token punctuation\">.</span>Residual<span class=\"token punctuation\">(</span>\n          tl<span class=\"token punctuation\">.</span>LayerNorm<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n          _CausalAttention<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n          _Dropout<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n      <span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n      tl<span class=\"token punctuation\">.</span>Residual<span class=\"token punctuation\">(</span>\n          tl<span class=\"token punctuation\">.</span>LayerNorm<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n          _FFBlock<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n          _Dropout<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n      <span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n  <span class=\"token punctuation\">]</span></code></pre></div>\n<p><code class=\"language-text\">TransformerLM</code> 함수는 Teacher Forcing을 거쳐 임베딩, positional encoding 처리 후 <code class=\"language-text\">n_layers</code>만큼의 디코더 블럭을 거쳐 <code class=\"language-text\">tl.LayerNorm()</code>과 <code class=\"language-text\">vocab_size</code>만큼의 <code class=\"language-text\">tl.Dense()</code> 레이어을 통과하는 구조를 가지고 있다. 결과적으로 사전에 주어진 단어들을 통해 다음에 올 단어를 vocabulary 내의 토큰으로 예측하는 언어 모델(language model)을 수행한다.</p>\n<h3 id=\"-teacher-forcing\" style=\"position:relative;\"><a href=\"#-teacher-forcing\" aria-label=\" teacher forcing permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>🔆 Teacher Forcing</h3>\n<p>디코더 블럭에 들어가기에 앞서, Teacher Forcing 기법을 활용해 모델의 훈련 속도를 높일 수 있다. RNN 모델인 Seq2Seq 모델은 바로 전 레이어의 예측 값을 다음 레이어의 입력값으로 사용한다. 때문에 모델 훈련 초기의 (덜 훈련된) 나쁜 예측 값이 계속해서 모델 훈련에 영향을 줄 가능성이 크다. 이런 문제를 완화하기 위해 이전 레이어의 예측값이 아닌 실제 타켓 값을 다음 레이어의 입력값으로 사용하는 것을 Teacher Forcing이라고 한다. 마치 선생님이 직접 이렇게 하라고 지도해주는 것과 같다. 훈련 초기에 타겟 값에 수렴하는 것을 돕기 때문에, 이 방법으로 모델 훈련 속도를 획기적으로 높일 수 있다.</p>\n<p>위의 코드에서 나타난 <code class=\"language-text\">ShiftRight</code> 레이어가 teacher forcing 역할을 한다. 즉 (한 시점 미래 값인) 바로 오른 쪽 값을 가져오는 것으로 학습하는 모델을 교정할 수 있다. 위의 코드에서 <code class=\"language-text\">mode</code>가 인자로 들어간 이유도, 학습 외에 예측을 수행할 때는 teacher forcing을 적용하지 않기 때문이다.</p>\n<p>그렇지만 Teacher Forcing은 모델을 훈련하는 과정에서 실제 타겟값을 노출하기 때문에 모델의 안정성, 즉 보다 일반적인 예에 대한 예측 능력이 떨어질 수 있다. 이렇게 훈련 중에 Label에 노출되는 경우를 <strong>Exposure Bias</strong>가 있다고 한다. 이 때문에 curriculum learning 방법에서는 FeedForward의 학습 초기에만 이전 레이어의 예측값을 타겟 값으로 대체하고 학습 후기에는 대체 하지 않는다.</p>\n<h3 id=\"-causal-self-attention\" style=\"position:relative;\"><a href=\"#-causal-self-attention\" aria-label=\" causal self attention permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>📣 Causal Self-Attention</h3>\n<p>디코더의 입력값에 대한 어텐션을 실행할 때도 어텐션에서와 마찬가지로 문맥을 위해 셀프-어텐션을 실행할 수 있다. 단, 디코더의 셀프-어텐션은 <strong>Causal Mask</strong>가 필요하다. 앞에서와 같이 기계 번역 과제를 고려해보자. RNN은 디코더의 입력값에 순차적으로 접근해 매번 인코더의 결과값과 해당 시점의 디코더 입력값을 비교할 것이다. 그렇지만 어텐션은 모든 시점의 데이터를 한번에 볼 수 있으므로 특정 시점에서 모델의 타깃인 오른쪽 값에 대한 접근(attend)을 방지해야 한다.</p>\n<p>📂 다음 코드는 Trax의 <code class=\"language-text\">_causal_mask</code>로, 인자로 받은 <code class=\"language-text\">length</code> 길이 정방 행렬의 lower triangular 행렬을 반환한다.  </p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">def</span> <span class=\"token function\">_causal_mask</span><span class=\"token punctuation\">(</span>length<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n  <span class=\"token comment\"># Not all backends define jnp.tril. However, using np.tril is inefficient</span>\n  <span class=\"token comment\"># in that it creates a large global constant. TODO(kitaev): try to find an</span>\n  <span class=\"token comment\"># alternative that works across all backends.</span>\n  <span class=\"token keyword\">if</span> fastmath<span class=\"token punctuation\">.</span>is_backend<span class=\"token punctuation\">(</span>fastmath<span class=\"token punctuation\">.</span>Backend<span class=\"token punctuation\">.</span>JAX<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">return</span> jnp<span class=\"token punctuation\">.</span>tril<span class=\"token punctuation\">(</span>jnp<span class=\"token punctuation\">.</span>ones<span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> length<span class=\"token punctuation\">,</span> length<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> dtype<span class=\"token operator\">=</span>np<span class=\"token punctuation\">.</span>bool_<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> k<span class=\"token operator\">=</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span>\n  <span class=\"token keyword\">else</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">return</span> np<span class=\"token punctuation\">.</span>tril<span class=\"token punctuation\">(</span>np<span class=\"token punctuation\">.</span>ones<span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> length<span class=\"token punctuation\">,</span> length<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> dtype<span class=\"token operator\">=</span>np<span class=\"token punctuation\">.</span>bool_<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> k<span class=\"token operator\">=</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p><code class=\"language-text\">DotProductCausalAttention</code> 어텐션은 1개 Head 어텐션을 구현하는 함수로 <code class=\"language-text\">CausalAttention</code>으로 구현될 수 있다. 주목해서 볼 점은 모델이 예측을 수행하지 않을 때만 Causal Mask를 활용하는 점이다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">class</span> <span class=\"token class-name\">DotProductCausalAttention</span><span class=\"token punctuation\">(</span>base<span class=\"token punctuation\">.</span>Layer<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n  <span class=\"token triple-quoted-string string\">\"\"\"Layer that computes attention strengths by masking out the \"future\".\n  Causal attention uses masking to prevent a given sequence position from\n  attending to positions greater than / following it. This is used, for\n  example, when training autoregressive sequence models, or when decoding a\n  sequence symbol by symbol.\n  This layer performs the core per-head attention calculation. The layer\n  assumes that any splitting into attention heads precedes it, and that any\n  merging of attention heads will follow it.\n  \"\"\"</span>\n  <span class=\"token comment\"># ...</span>\n\n  <span class=\"token keyword\">def</span> <span class=\"token function\">forward</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> inputs<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token triple-quoted-string string\">\"\"\"Returns attention-computed activations.\n    Args:\n      inputs: A (queries, keys, values) tuple.\n    \"\"\"</span>\n    q<span class=\"token punctuation\">,</span> k<span class=\"token punctuation\">,</span> v <span class=\"token operator\">=</span> inputs\n\n    <span class=\"token comment\"># ...</span>\n\n    <span class=\"token keyword\">if</span> self<span class=\"token punctuation\">.</span>_mode <span class=\"token operator\">==</span> <span class=\"token string\">'predict'</span><span class=\"token punctuation\">:</span>\n      self<span class=\"token punctuation\">.</span>state<span class=\"token punctuation\">,</span> mask <span class=\"token operator\">=</span> _fast_inference_update_state<span class=\"token punctuation\">(</span>\n          inputs<span class=\"token punctuation\">,</span> self<span class=\"token punctuation\">.</span>state<span class=\"token punctuation\">,</span>\n          mask_for_predict<span class=\"token operator\">=</span>mask_for_predict<span class=\"token punctuation\">)</span>\n      <span class=\"token comment\"># ...</span>\n    <span class=\"token keyword\">else</span><span class=\"token punctuation\">:</span>\n      sequence_length <span class=\"token operator\">=</span> q<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">[</span><span class=\"token operator\">-</span><span class=\"token number\">2</span><span class=\"token punctuation\">]</span>\n      mask <span class=\"token operator\">=</span> _causal_mask<span class=\"token punctuation\">(</span>sequence_length<span class=\"token punctuation\">)</span>\n\n    activations<span class=\"token punctuation\">,</span> attn_strengths <span class=\"token operator\">=</span> _per_head_attention<span class=\"token punctuation\">(</span>\n        q<span class=\"token punctuation\">,</span> k<span class=\"token punctuation\">,</span> v<span class=\"token punctuation\">,</span> mask<span class=\"token punctuation\">,</span> dropout<span class=\"token operator\">=</span>self<span class=\"token punctuation\">.</span>_dropout<span class=\"token punctuation\">,</span> mode<span class=\"token operator\">=</span>self<span class=\"token punctuation\">.</span>_mode<span class=\"token punctuation\">,</span> rng<span class=\"token operator\">=</span>self<span class=\"token punctuation\">.</span>rng<span class=\"token punctuation\">)</span>\n    <span class=\"token comment\">#...</span>\n    <span class=\"token keyword\">return</span> activations\n\n    <span class=\"token comment\"># ...</span></code></pre></div>\n<h3 id=\"-encoder-decoder-attention\" style=\"position:relative;\"><a href=\"#-encoder-decoder-attention\" aria-label=\" encoder decoder attention permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>📣 Encoder-Decoder Attention</h3>\n<p>이제 디코더의 입력값과 인코더의 입력값을 분석하는 과제가 남았다. 인코더-디코더 블럭의 입력값은 <code class=\"language-text\">(vec_d, mask, vec_e)</code>로 패딩 마스크 <code class=\"language-text\">mask</code>를 이용해 패딩된 값에 대해서는 어텐션을 수행하지 않는다. </p>\n<p>📂 다음 코드는 Trax 라이브러리의 <code class=\"language-text\">AttentionQKV</code> 함수로, <code class=\"language-text\">Attention</code>이 셀프-어텐션만 수행하는 것과 달리 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>Q</mi></mrow><annotation encoding=\"application/x-tex\">Q</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8777699999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\">Q</span></span></span></span>와 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>K</mi><mo>−</mo><mi>V</mi></mrow><annotation encoding=\"application/x-tex\">K-V</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.76666em;vertical-align:-0.08333em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.07153em;\">K</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">V</span></span></span></span>를 다른 데이터에서 가져오는 것을 허용한다. </p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">def</span> <span class=\"token function\">AttentionQKV</span><span class=\"token punctuation\">(</span>d_feature<span class=\"token punctuation\">,</span> n_heads<span class=\"token operator\">=</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> dropout<span class=\"token operator\">=</span><span class=\"token number\">0.0</span><span class=\"token punctuation\">,</span> mode<span class=\"token operator\">=</span><span class=\"token string\">'train'</span><span class=\"token punctuation\">,</span>\n                 cache_KV_in_predict<span class=\"token operator\">=</span><span class=\"token boolean\">False</span><span class=\"token punctuation\">,</span> q_sparsity<span class=\"token operator\">=</span><span class=\"token boolean\">None</span><span class=\"token punctuation\">,</span>\n                 result_sparsity<span class=\"token operator\">=</span><span class=\"token boolean\">None</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n  <span class=\"token comment\"># ...</span>\n\n  <span class=\"token keyword\">return</span> cb<span class=\"token punctuation\">.</span>Serial<span class=\"token punctuation\">(</span>\n      cb<span class=\"token punctuation\">.</span>Parallel<span class=\"token punctuation\">(</span>_SparsifiableDense<span class=\"token punctuation\">(</span>q_sparsity<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n                  _CacheableDense<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n                  _CacheableDense<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n      _PureAttention<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n      _SparsifiableDense<span class=\"token punctuation\">(</span>result_sparsity<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n  <span class=\"token punctuation\">)</span></code></pre></div>\n<p><code class=\"language-text\">cb.Parallel</code>은 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>Q</mi></mrow><annotation encoding=\"application/x-tex\">Q</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8777699999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\">Q</span></span></span></span>에 해당하는 입력을 밀도 <code class=\"language-text\">q_sparsity</code>를 가지는 행렬로, 그리고 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>K</mi></mrow><annotation encoding=\"application/x-tex\">K</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.07153em;\">K</span></span></span></span>와 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>V</mi></mrow><annotation encoding=\"application/x-tex\">V</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">V</span></span></span></span>에 해당하는 입력을 <code class=\"language-text\">d_feature</code>만큼의 <code class=\"language-text\">Dense</code> 레이어로 두고, <code class=\"language-text\">_PureAttention</code>과 일종의 훈련가능한 <code class=\"language-text\">Dense</code> 레이어를 통과한다.</p>\n<p>최종적으로 Trax 라이브러리의 <code class=\"language-text\">_EncoderDecoderBlock</code> 함수를 보자.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">def</span> <span class=\"token function\">_EncoderDecoderBlock</span><span class=\"token punctuation\">(</span>d_model<span class=\"token punctuation\">,</span>\n                         d_ff<span class=\"token punctuation\">,</span>\n                         n_heads<span class=\"token punctuation\">,</span>\n                         dropout<span class=\"token punctuation\">,</span>\n                         dropout_shared_axes<span class=\"token punctuation\">,</span>\n                         mode<span class=\"token punctuation\">,</span>\n                         ff_activation<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n\n  <span class=\"token keyword\">def</span> <span class=\"token function\">_Dropout</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">return</span> tl<span class=\"token punctuation\">.</span>Dropout<span class=\"token punctuation\">(</span>rate<span class=\"token operator\">=</span>dropout<span class=\"token punctuation\">,</span> shared_axes<span class=\"token operator\">=</span>dropout_shared_axes<span class=\"token punctuation\">,</span> mode<span class=\"token operator\">=</span>mode<span class=\"token punctuation\">)</span>\n\n  <span class=\"token keyword\">def</span> <span class=\"token function\">_AttentionQKV</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">return</span> tl<span class=\"token punctuation\">.</span>AttentionQKV<span class=\"token punctuation\">(</span>d_model<span class=\"token punctuation\">,</span> n_heads<span class=\"token operator\">=</span>n_heads<span class=\"token punctuation\">,</span> dropout<span class=\"token operator\">=</span>dropout<span class=\"token punctuation\">,</span>\n                           mode<span class=\"token operator\">=</span>mode<span class=\"token punctuation\">,</span> cache_KV_in_predict<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span>\n\n  <span class=\"token keyword\">def</span> <span class=\"token function\">_CausalAttention</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">return</span> tl<span class=\"token punctuation\">.</span>CausalAttention<span class=\"token punctuation\">(</span>d_model<span class=\"token punctuation\">,</span> n_heads<span class=\"token operator\">=</span>n_heads<span class=\"token punctuation\">,</span> mode<span class=\"token operator\">=</span>mode<span class=\"token punctuation\">)</span>\n\n  <span class=\"token keyword\">def</span> <span class=\"token function\">_FFBlock</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">return</span> _FeedForwardBlock<span class=\"token punctuation\">(</span>d_model<span class=\"token punctuation\">,</span> d_ff<span class=\"token punctuation\">,</span> dropout<span class=\"token punctuation\">,</span> dropout_shared_axes<span class=\"token punctuation\">,</span> mode<span class=\"token punctuation\">,</span>\n                             ff_activation<span class=\"token punctuation\">)</span>\n\n  <span class=\"token keyword\">return</span> <span class=\"token punctuation\">[</span>                             <span class=\"token comment\"># vec_d masks vec_e</span>\n      tl<span class=\"token punctuation\">.</span>Residual<span class=\"token punctuation\">(</span>\n          tl<span class=\"token punctuation\">.</span>LayerNorm<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n          _CausalAttention<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n          _Dropout<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n      <span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n      tl<span class=\"token punctuation\">.</span>Residual<span class=\"token punctuation\">(</span>\n          tl<span class=\"token punctuation\">.</span>LayerNorm<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n          tl<span class=\"token punctuation\">.</span>Select<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>  <span class=\"token comment\"># vec_d vec_e vec_e masks vec_e</span>\n          _AttentionQKV<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>             <span class=\"token comment\"># vec_d masks vec_e</span>\n          _Dropout<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n      <span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n      tl<span class=\"token punctuation\">.</span>Residual<span class=\"token punctuation\">(</span>\n          tl<span class=\"token punctuation\">.</span>LayerNorm<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n          _FFBlock<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n          _Dropout<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n      <span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n  <span class=\"token punctuation\">]</span></code></pre></div>\n<p><code class=\"language-text\">_EncoderDecoderBlock</code>은 디코더 벡터, 마스크, 인코더 벡터 쌍인 <code class=\"language-text\">(vec_d, masks, vec_e)</code>를 입력받는다. 여기서 디코더의 두가지 어텐션을 모두 실행하고 있음에 유의한다. 첫째로 <code class=\"language-text\">_CausalAttention</code>을 적용한 후 <code class=\"language-text\">(vec_d, vec_e, vec_e, masks, vec_e)</code>로 데이터를 정렬한다. 앞에서 부터 세 개 입력이 <code class=\"language-text\">_AttentionQKV()</code>레이어를 거쳐서 입력값과 같은 차원인 <code class=\"language-text\">(vec_d, masks, vec_e)</code>를 얻어 FeedForward 블럭을 거친다. 즉 인코더-디코더 어텐션에서는 디코더 벡터를 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>Q</mi></mrow><annotation encoding=\"application/x-tex\">Q</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8777699999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\">Q</span></span></span></span>로, 인코더 벡터를 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>K</mi></mrow><annotation encoding=\"application/x-tex\">K</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.07153em;\">K</span></span></span></span>와 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>V</mi></mrow><annotation encoding=\"application/x-tex\">V</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">V</span></span></span></span>로 입력받는 것을 확인할 수 있다.</p>\n<h2 id=\"4-overall\" style=\"position:relative;\"><a href=\"#4-overall\" aria-label=\"4 overall permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>4. Overall</h2>\n<p>다음은 인코딩과 디코딩을 직관적으로 보여주는그림으로, 영어 문장을 프랑스어 문장으로 번역하는 예시를 보이고있다.</p>\n<p><img src=\"https://3.bp.blogspot.com/-aZ3zvPiCoXM/WaiKQO7KRnI/AAAAAAAAB_8/7a1CYjp40nUg4lKpW7covGZJQAySxlg8QCLcBGAs/s1600/transform20fps.gif\">\n<em>Cool gif from <a href=\"https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Google AI blog</a></em></p>\n<p>Transformer 모델은 인코더나 디코더만으로도 쓰임이 있지만, 기계 번역과 같은 시퀀스-투-시퀀스 과제에는 인코더와 디코더를 모두 활용해야 한다. 아래의 <code class=\"language-text\">Transformer</code> 모델은 앞서 살펴 본 인코더와 디코더 블럭을 활용해 전체 트랜스포머 모델을 반환한다.</p>\n<p>📂 다음 코드는 Trax 라이브러리의 <code class=\"language-text\">Transformer</code> 모델이다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">def</span> <span class=\"token function\">Transformer</span><span class=\"token punctuation\">(</span>input_vocab_size<span class=\"token punctuation\">,</span>\n                output_vocab_size<span class=\"token operator\">=</span><span class=\"token boolean\">None</span><span class=\"token punctuation\">,</span>\n                d_model<span class=\"token operator\">=</span>D_MODEL<span class=\"token punctuation\">,</span>\n                d_ff<span class=\"token operator\">=</span>D_FF<span class=\"token punctuation\">,</span>\n                n_encoder_layers<span class=\"token operator\">=</span>N_LAYERS<span class=\"token punctuation\">,</span>\n                n_decoder_layers<span class=\"token operator\">=</span>N_LAYERS<span class=\"token punctuation\">,</span>\n                n_heads<span class=\"token operator\">=</span>N_HEADS<span class=\"token punctuation\">,</span>\n                max_len<span class=\"token operator\">=</span>MAX_SEQUENCE_LENGTH<span class=\"token punctuation\">,</span>\n                dropout<span class=\"token operator\">=</span>DROPOUT_RATE<span class=\"token punctuation\">,</span>\n                dropout_shared_axes<span class=\"token operator\">=</span>DROPOUT_SHARED_AXES<span class=\"token punctuation\">,</span>\n                mode<span class=\"token operator\">=</span>MODE<span class=\"token punctuation\">,</span>\n                ff_activation<span class=\"token operator\">=</span>FF_ACTIVATION_TYPE<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n  <span class=\"token triple-quoted-string string\">\"\"\"Returns a full Transformer model.\n  This model is an encoder-decoder that performs tokenized string-to-string\n  (\"source\"-to-\"target\") transduction:\n  \"\"\"</span>\n\n  <span class=\"token comment\"># ...</span>\n\n  <span class=\"token keyword\">def</span> <span class=\"token function\">_Dropout</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">return</span> tl<span class=\"token punctuation\">.</span>Dropout<span class=\"token punctuation\">(</span>rate<span class=\"token operator\">=</span>dropout<span class=\"token punctuation\">,</span> shared_axes<span class=\"token operator\">=</span>dropout_shared_axes<span class=\"token punctuation\">,</span> mode<span class=\"token operator\">=</span>mode<span class=\"token punctuation\">)</span>\n\n  <span class=\"token keyword\">def</span> <span class=\"token function\">_EncBlock</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">return</span> _EncoderBlock<span class=\"token punctuation\">(</span>d_model<span class=\"token punctuation\">,</span> d_ff<span class=\"token punctuation\">,</span> n_heads<span class=\"token punctuation\">,</span> dropout<span class=\"token punctuation\">,</span> dropout_shared_axes<span class=\"token punctuation\">,</span>\n                         mode<span class=\"token punctuation\">,</span> ff_activation<span class=\"token punctuation\">)</span>\n\n  <span class=\"token keyword\">def</span> <span class=\"token function\">_Encoder</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    encoder <span class=\"token operator\">=</span> tl<span class=\"token punctuation\">.</span>Serial<span class=\"token punctuation\">(</span>\n        in_embedder<span class=\"token punctuation\">,</span>\n        _Dropout<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n        tl<span class=\"token punctuation\">.</span>PositionalEncoding<span class=\"token punctuation\">(</span>max_len<span class=\"token operator\">=</span>max_len<span class=\"token punctuation\">,</span> mode<span class=\"token operator\">=</span>encoder_mode<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n        <span class=\"token punctuation\">[</span>_EncBlock<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">for</span> _ <span class=\"token keyword\">in</span> <span class=\"token builtin\">range</span><span class=\"token punctuation\">(</span>n_encoder_layers<span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>\n        tl<span class=\"token punctuation\">.</span>LayerNorm<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n    <span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">return</span> tl<span class=\"token punctuation\">.</span>Cache<span class=\"token punctuation\">(</span>encoder<span class=\"token punctuation\">)</span> <span class=\"token keyword\">if</span> mode <span class=\"token operator\">==</span> <span class=\"token string\">'predict'</span> <span class=\"token keyword\">else</span> encoder\n\n  <span class=\"token keyword\">def</span> <span class=\"token function\">_EncDecBlock</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">return</span> _EncoderDecoderBlock<span class=\"token punctuation\">(</span>d_model<span class=\"token punctuation\">,</span> d_ff<span class=\"token punctuation\">,</span> n_heads<span class=\"token punctuation\">,</span> dropout<span class=\"token punctuation\">,</span>\n                                dropout_shared_axes<span class=\"token punctuation\">,</span> mode<span class=\"token punctuation\">,</span> ff_activation<span class=\"token punctuation\">)</span>\n\n  <span class=\"token comment\"># Input to model is encoder-side tokens and decoder-side tokens: tok_d, tok_e</span>\n  <span class=\"token comment\"># Model output is decoder-side vectors and decoder-side tokens: vec_d  tok_d</span>\n  <span class=\"token keyword\">return</span> tl<span class=\"token punctuation\">.</span>Serial<span class=\"token punctuation\">(</span>\n      tl<span class=\"token punctuation\">.</span>Select<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>  <span class=\"token comment\"># Copies decoder tokens for use in loss.</span>\n\n      <span class=\"token comment\"># Encode.</span>\n      tl<span class=\"token punctuation\">.</span>Branch<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> tl<span class=\"token punctuation\">.</span>PaddingMask<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>  <span class=\"token comment\"># tok_e masks tok_d tok_d</span>\n      _Encoder<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n\n      <span class=\"token comment\"># Decode.</span>\n      tl<span class=\"token punctuation\">.</span>Select<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token number\">2</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>  <span class=\"token comment\"># Re-orders inputs: tok_d masks vec_e .....</span>\n      tl<span class=\"token punctuation\">.</span>ShiftRight<span class=\"token punctuation\">(</span>mode<span class=\"token operator\">=</span>mode<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n      out_embedder<span class=\"token punctuation\">,</span>\n      _Dropout<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n      tl<span class=\"token punctuation\">.</span>PositionalEncoding<span class=\"token punctuation\">(</span>max_len<span class=\"token operator\">=</span>max_len<span class=\"token punctuation\">,</span> mode<span class=\"token operator\">=</span>mode<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n      tl<span class=\"token punctuation\">.</span>Branch<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> tl<span class=\"token punctuation\">.</span>EncoderDecoderMask<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>  <span class=\"token comment\"># vec_d masks ..... .....</span>\n      <span class=\"token punctuation\">[</span>_EncDecBlock<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">for</span> _ <span class=\"token keyword\">in</span> <span class=\"token builtin\">range</span><span class=\"token punctuation\">(</span>n_decoder_layers<span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>\n      tl<span class=\"token punctuation\">.</span>LayerNorm<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n      tl<span class=\"token punctuation\">.</span>Select<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> n_in<span class=\"token operator\">=</span><span class=\"token number\">3</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>  <span class=\"token comment\"># Drops masks and encoding vectors.</span>\n\n      <span class=\"token comment\"># Map vectors to match output vocab size.</span>\n      tl<span class=\"token punctuation\">.</span>Dense<span class=\"token punctuation\">(</span>output_vocab_size<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n  <span class=\"token punctuation\">)</span></code></pre></div>\n<p><code class=\"language-text\">Transformer</code> 는 인코더에 입력되는 토큰과 디코더에 입력되는 토큰 쌍 <code class=\"language-text\">(tok_d, tok_e)</code>를 입력받아 디코더 벡터와 디코더 토큰 쌍 <code class=\"language-text\">(vec_d, tok_d)</code>를 반환한다. <code class=\"language-text\">tl.Branch</code>로 입력값과 패딩마스크를 생성한 후 <code class=\"language-text\">_Encoder()</code>로 인코딩을 실행한다. <code class=\"language-text\">_Encoder</code>는 <code class=\"language-text\">n_encoder_layers</code>개의 인코더 블럭 <code class=\"language-text\">_EncoderDecoderBlock</code>을 포함하도록 정의되어 있다. 인코딩을 거치면 데이터는 <code class=\"language-text\">(vec_e, masks, tok_d)</code>가 되며 <code class=\"language-text\">tl.Select</code>로 순서를 뒤집어 teacher forcing과 positional encoding을 실행한다. 두번째 <code class=\"language-text\">tl.Branch</code>로 <code class=\"language-text\">(tok_d, masks)</code>를 인코더-디코더 블럭에 입력하며, 이 외의 값들은 이후 <code class=\"language-text\">tl.Select</code>로 제외시키는 것을 알 수 있다. <code class=\"language-text\">_EncoderDecoderBlock</code>은 앞에서 살펴본 대로다. 이로써 <code class=\"language-text\">Transformer</code>함수를 불러오는 것만으로  트랜스포머를 구현할 수 있다.</p>\n<h2 id=\"나가며\" style=\"position:relative;\"><a href=\"#%EB%82%98%EA%B0%80%EB%A9%B0\" aria-label=\"나가며 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>나가며</h2>\n<p>여러 내용을 다루다보니 글이 길어졌다. 트랜스포머 모델은 긴 시퀀스에서 맥락을 추출하는데에 사용되는 모델로 널리 쓰이고 있지만, 짧은 시퀀스를 분석하거나 맥락을 구하기 어려운 문제에 대해서는 적합하지 않을 수 있다. 한편으로는 트랜스포머 모델을 이미지에 적용한 사례나 다양한 과제를 한번에 수행하는 T5, 더 긴 시퀀스에 대해 효율적(<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>O</mi><mo stretchy=\"false\">(</mo><mi>N</mi><mi>l</mi><mi>o</mi><mi>g</mi><mi>N</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">O(NlogN)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">O</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.10903em;\">N</span><span class=\"mord mathdefault\" style=\"margin-right:0.01968em;\">l</span><span class=\"mord mathdefault\">o</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">g</span><span class=\"mord mathdefault\" style=\"margin-right:0.10903em;\">N</span><span class=\"mclose\">)</span></span></span></span>)으로 학습하는 리포머 모델 등 다양한 버전이 등장하고 있으므로, 트랜스포머의 기본적인 구조를 이해하는 것이 중요할 것이다. </p>\n<p>글로 정리하면서 나의 맹점에 대해 알 수 있었다. Residual이 왜 필요한지, 인코더나 디코더 블럭을 왜 여러번 실행하는지, 어텐션에서 학습하는 파라미터는 어느 부분인지 생각하지 않고 넘기다가 논문을 읽으면서 저자의 의도와 history를 조금 더 이해할 수 있었다. 또 글을 쓰면서 Trax 라이브러리의 코드를 부분적으로 들여다봤다. <code class=\"language-text\">Select()</code>나 <code class=\"language-text\">Residual()</code>, <code class=\"language-text\">Branch()</code>로 레이어를 쌓는 부분은 그림이 자연스럽게 떠오르는 개념을 코드로 표현해 직관적이라는 느낌이 들었고, 인코더나 디코더의 일부분을 따로 떼어서 사용하는 각각의 함수가 있어 사용 편의를 고려한 점이 느껴졌다. 큰 프로그램(라이브러리)의 코드이다보니 재사용되는 부분과 조금씩 차이나는 부분, 특히 메서드의 의존도를 미리 디자인해서 큰 그림을 염두에 두고 프로그램을 작성했다는 점이 느껴졌다. 결국 이론과 구현이 모두 중요하고 나름의 깊이가 있다는 것을 알 수 있었다.</p>\n<h2 id=\"참고-자료\" style=\"position:relative;\"><a href=\"#%EC%B0%B8%EA%B3%A0-%EC%9E%90%EB%A3%8C\" aria-label=\"참고 자료 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>참고 자료</h2>\n<ol>\n<li>Natural Language Processing with Attention Models, deeplearning.ai, Coursera, <a href=\"https://www.coursera.org/specializations/natural-language-processing\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://www.coursera.org/specializations/natural-language-processing</a></li>\n<li>Trax Library for Machine Learning, Github, <a href=\"https://github.com/google/trax\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://github.com/google/trax</a></li>\n<li>Transformers, Google AI blog, <a href=\"https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html</a></li>\n<li>Vaswani et al, 2017, <a href=\"https://arxiv.org/abs/1706.03762\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://arxiv.org/abs/1706.03762</a></li>\n<li>Jay Alammar on Github page, <a href=\"https://jalammar.github.io/illustrated-transformer/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://jalammar.github.io/illustrated-transformer/</a></li>\n</ol>","excerpt":"원 논문: Vaswani et al., 2017, \"Attention is all you need\" (Link to arxiv) 0. Transformer   트랜스포머는 Seq2Seq 모델과 비슷한 인코더-디코더 구조를 갖고 있지만, 보다 긴 시퀀…","tableOfContents":"<ul>\n<li>\n<p><a href=\"/transformer/#0-transformer\">0. Transformer</a></p>\n<ul>\n<li><a href=\"/transformer/#-scaled-dot-prooduct-attention\">🔔 Scaled-Dot prooduct Attention</a></li>\n<li><a href=\"/transformer/#-multi-head-attention\">🔔 Multi-Head Attention</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"/transformer/#1-encoder\">1. Encoder</a></p>\n<ul>\n<li><a href=\"/transformer/#-dimensionality-setting\">🔆 Dimensionality Setting</a></li>\n<li><a href=\"/transformer/#-positional-encoding\">🔆 Positional Encoding</a></li>\n<li><a href=\"/transformer/#-encoder-self-attention\">📣 Encoder Self-Attention</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"/transformer/#2-decoder\">2. Decoder</a></p>\n<ul>\n<li><a href=\"/transformer/#-teacher-forcing\">🔆 Teacher Forcing</a></li>\n<li><a href=\"/transformer/#-causal-self-attention\">📣 Causal Self-Attention</a></li>\n<li><a href=\"/transformer/#-encoder-decoder-attention\">📣 Encoder-Decoder Attention</a></li>\n</ul>\n</li>\n<li><a href=\"/transformer/#4-overall\">4. Overall</a></li>\n<li><a href=\"/transformer/#%EB%82%98%EA%B0%80%EB%A9%B0\">나가며</a></li>\n<li><a href=\"/transformer/#%EC%B0%B8%EA%B3%A0-%EC%9E%90%EB%A3%8C\">참고 자료</a></li>\n</ul>","fields":{"slug":"/transformer/"},"frontmatter":{"title":"Transformer (2017)","date":"Mar 31, 2022","tags":["NLP","Attention","Transformer"],"keywords":["AI doodle","Near"],"update":"Apr 14, 2022"}}},"pageContext":{"slug":"/transformer/","series":[],"lastmod":"2022-04-14"}},"staticQueryHashes":["2027115977","694178885"]}