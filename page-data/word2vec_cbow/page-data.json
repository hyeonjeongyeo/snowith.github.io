{"componentChunkName":"component---src-templates-post-tsx","path":"/word2vec_cbow/","result":{"data":{"markdownRemark":{"html":"<p><code class=\"language-text\">word2vec</code>은 2013년 구글에서 고안한 자연어 처리 아이디어로, 이에 기반한 모델은 <code class=\"language-text\">Continuous Bag-of-Words(CBOW)</code>와 <code class=\"language-text\">Skip-gram</code> 두가지가 있다. 이 글은 그 중에서 <code class=\"language-text\">CBOW</code> 모델을 원 논문과 deeplearning.ai 수업을 참고하여 정리한 글이다. </p>\n<p><strong>원 논문</strong>:</p>\n<ul>\n<li>Mikolov et. al., 2013, Efficient Estimation of Word Representations in Vector Space (<a href=\"https://arxiv.org/pdf/1301.3781.pdf\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">arxiv</a>)</li>\n<li>Mikolov et. al., 2013, Distributed Representations of Words and Phrases and their Compositionality (<a href=\"https://proceedings.neurips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">arxiv</a>)</li>\n</ul>\n<h2 id=\"0-cbow란\" style=\"position:relative;\"><a href=\"#0-cbow%EB%9E%80\" aria-label=\"0 cbow란 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>0. CBOW란</h2>\n<p>CBOW(Continuous Bag-of-Words, 연속되는 단어 주머니)는 </p>\n<ul>\n<li>텍스트 데이터를 벡터 공간에 표현하는 <code class=\"language-text\">단어 임베딩(word embedding)</code> 모델이자,  </li>\n<li><code class=\"language-text\">얕은 신경망(neural network)</code> 모델이며, </li>\n<li>스스로 훈련 데이터를 생성하는 <code class=\"language-text\">자기 지도 훈련(self-supervised learning)</code> 모델이다. </li>\n</ul>\n<p>CBOW 모델이 처음 소개될 때는 50-100 차원의 원 핫 벡터로 몇 백만개의 단어를 훈련시켰다. </p>\n<h3 id=\"-skip-gram과의-차이점\" style=\"position:relative;\"><a href=\"#-skip-gram%EA%B3%BC%EC%9D%98-%EC%B0%A8%EC%9D%B4%EC%A0%90\" aria-label=\" skip gram과의 차이점 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>📖 Skip-gram과의 차이점</h3>\n<p>CBOW 모델은 여러개의 단어 데이터를 입력하면 그에 상응하는 한개의 단어를 출력하는 <code class=\"language-text\">Many to One</code> (여러개 데이터를 입력받아 한개의 데이터를 출력하는 모델 구조) 모델이다. 반면 Skip-gram은 한개의 단어를 입력했을 때 그에 대응하는 여러개의 단어를 출력하는 <code class=\"language-text\">One to Many</code> 모델이다. 즉 두 모델의 구조는 <code class=\"language-text\">반전</code>되어있고, 입력값과 출력값이 서로 반대된다. </p>\n<h2 id=\"1-모델의-구조\" style=\"position:relative;\"><a href=\"#1-%EB%AA%A8%EB%8D%B8%EC%9D%98-%EA%B5%AC%EC%A1%B0\" aria-label=\"1 모델의 구조 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>1. 모델의 구조</h2>\n<p>CBOW는 얕은 신경망 모델로, 이 글에서는 한 개의 은닉층(hidden layer)를 가지는 신경망 모델을 고려한다.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/ebc25777763110fd6ff55934afa2cd4a/9ac09/cbow_model_architecture.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 37.83783783783784%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAICAYAAAD5nd/tAAAACXBIWXMAABYlAAAWJQFJUiTwAAABnElEQVQoz1WR2U7DMBBF8+MgfgB4Ar4AEA+I9QEhpAIvbIW2QCuoUEQwbfZ6SZvSxM7FNqSCkUZxxqMzd3wd/EaSJPA+CDzvA6pStlYqCdd15/VClraeT6e2Tj4HIISgqqoaAyeOYwRBAC4E0v4VaOiBpxx+zwUdRMgER9A5g+AjpMPI1nlMQdMAUbeBopghz3OEYWhFOYwxDfRhZvDOIWQ2xLDnYW9xDd2TK12tkF5v6s8M/cs29hbW4DVfocoMrLljVUkp4fs+hBbllGUJk5N8Cto5RqmBUX+Ai9VtvJ239DoS6e0OlPrC+00XFyvbGD66KGcc7GEX43GmVRYWatIRPMOXfhPGBUbtozmwsbKpgQ9m/hzo3fRwvrz1CxSg97tgjMIwci3IgB0plW6WVjptHdiVB913HCxt2JUrfVev3Gs0sb+wjrfrZ0gDvPtZ2Xhi1Cml4NTu6DPCpwayhIDFDKT9ihEJ7fTPuyNMc4FE/5PWC1g4Ak1CRI+nGqbwNxxjeW37RLtVzIp/DeaOc27f+W8Yd8fjybynzm885FTbsv2J6QAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"cbow model architecture\"\n        title=\"cbow model architecture\"\n        src=\"/static/ebc25777763110fd6ff55934afa2cd4a/fcda8/cbow_model_architecture.png\"\n        srcset=\"/static/ebc25777763110fd6ff55934afa2cd4a/12f09/cbow_model_architecture.png 148w,\n/static/ebc25777763110fd6ff55934afa2cd4a/e4a3f/cbow_model_architecture.png 295w,\n/static/ebc25777763110fd6ff55934afa2cd4a/fcda8/cbow_model_architecture.png 590w,\n/static/ebc25777763110fd6ff55934afa2cd4a/efc66/cbow_model_architecture.png 885w,\n/static/ebc25777763110fd6ff55934afa2cd4a/c83ae/cbow_model_architecture.png 1180w,\n/static/ebc25777763110fd6ff55934afa2cd4a/9ac09/cbow_model_architecture.png 2204w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span>\n<em>image by DeepLearning.AI</em></p>\n<p>모델의 흐름은 다음과 같다.</p>\n<ol>\n<li>텍스트 데이터를 원 핫 벡터로 변환한다.</li>\n<li>\n<p>첫번째 은닉층(hidden layer)을 거친다.</p>\n<ul>\n<li>활성화 함수 : ReLU</li>\n</ul>\n</li>\n<li>\n<p>두번째 결과층(output layer)을 거친다.</p>\n<ul>\n<li>활성화 함수 : Softmax</li>\n</ul>\n</li>\n<li>결과 벡터의 값 중 가장 큰 값으로 예측한다.</li>\n</ol>\n<p>모델을 이해하고 실제로 구현하기 위해서는 각 층의 차원을 정확히 알아야 한다.</p>\n<h3 id=\"-벡터의-차원\" style=\"position:relative;\"><a href=\"#-%EB%B2%A1%ED%84%B0%EC%9D%98-%EC%B0%A8%EC%9B%90\" aria-label=\" 벡터의 차원 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>📖 벡터의 차원</h3>\n<p>변수를 다음과 같이 정의할 때,</p>\n<ul>\n<li><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>V</mi></mrow><annotation encoding=\"application/x-tex\">V</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">V</span></span></span></span> : 단어 사전의 크기,  혹은 원-핫 벡터의 크기.</li>\n<li><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>N</mi></mrow><annotation encoding=\"application/x-tex\">N</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.10903em;\">N</span></span></span></span> : 임베딩 크기. 모델의 하이퍼파라미터이다.</li>\n<li><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>m</mi></mrow><annotation encoding=\"application/x-tex\">m</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">m</span></span></span></span> : 배치 크기. 한번에 훈련할 데이터의 개수이다.</li>\n</ul>\n<p>입력값 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>X</mi></mrow><annotation encoding=\"application/x-tex\">X</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.07847em;\">X</span></span></span></span>의 차원</p>\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>X</mi><mo>∈</mo><mi>M</mi><mo stretchy=\"false\">(</mo><mi>V</mi><mo separator=\"true\">,</mo><mi>m</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">X \\in M(V, m)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.72243em;vertical-align:-0.0391em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.07847em;\">X</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">∈</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.10903em;\">M</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">V</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathdefault\">m</span><span class=\"mclose\">)</span></span></span></span></span>\n<p>에 대해 각 층에 대한 벡터의 차원을 다음과 같이 정리할 수 있다.</p>\n<center>\n<table>\n<thead>\n<tr>\n<th>은닉층 벡터</th>\n<th>차원</th>\n<th>결과층 벡터</th>\n<th>차원</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>W</mi><mn>1</mn></msub></mrow><annotation encoding=\"application/x-tex\">W_1</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.83333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span></td>\n<td><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mo stretchy=\"false\">(</mo><mi>N</mi><mo separator=\"true\">,</mo><mi>V</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(N, V)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.10903em;\">N</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">V</span><span class=\"mclose\">)</span></span></span></span></td>\n<td><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>W</mi><mn>2</mn></msub></mrow><annotation encoding=\"application/x-tex\">W_2</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.83333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span></td>\n<td><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mo stretchy=\"false\">(</mo><mi>V</mi><mo separator=\"true\">,</mo><mi>N</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(V, N)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">V</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.10903em;\">N</span><span class=\"mclose\">)</span></span></span></span></td>\n</tr>\n<tr>\n<td><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>B</mi><mn>1</mn></msub></mrow><annotation encoding=\"application/x-tex\">B_1</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.83333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.05017em;\">B</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.05017em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span></td>\n<td><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mo stretchy=\"false\">(</mo><mi>N</mi><mo separator=\"true\">,</mo><mi>m</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(N, m)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.10903em;\">N</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathdefault\">m</span><span class=\"mclose\">)</span></span></span></span></td>\n<td><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>B</mi><mn>2</mn></msub></mrow><annotation encoding=\"application/x-tex\">B_2</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.83333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.05017em;\">B</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.05017em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span></td>\n<td><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mo stretchy=\"false\">(</mo><mi>V</mi><mo separator=\"true\">,</mo><mi>m</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(V, m)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">V</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathdefault\">m</span><span class=\"mclose\">)</span></span></span></span></td>\n</tr>\n<tr>\n<td><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>z</mi><mn>1</mn></msub></mrow><annotation encoding=\"application/x-tex\">z_1</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.58056em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.04398em;\">z</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.04398em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span></td>\n<td><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mo stretchy=\"false\">(</mo><mi>N</mi><mo separator=\"true\">,</mo><mi>m</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(N, m)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.10903em;\">N</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathdefault\">m</span><span class=\"mclose\">)</span></span></span></span></td>\n<td><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>z</mi><mn>2</mn></msub></mrow><annotation encoding=\"application/x-tex\">z_2</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.58056em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.04398em;\">z</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.04398em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span></td>\n<td><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mo stretchy=\"false\">(</mo><mi>V</mi><mo separator=\"true\">,</mo><mi>m</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(V, m)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">V</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathdefault\">m</span><span class=\"mclose\">)</span></span></span></span></td>\n</tr>\n<tr>\n<td><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>r</mi><mi>e</mi><mi>l</mi><mi>u</mi><mo stretchy=\"false\">(</mo><msub><mi>z</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">relu(z_1)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">r</span><span class=\"mord mathdefault\">e</span><span class=\"mord mathdefault\" style=\"margin-right:0.01968em;\">l</span><span class=\"mord mathdefault\">u</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.04398em;\">z</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.04398em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span></td>\n<td><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mo stretchy=\"false\">(</mo><mi>N</mi><mo separator=\"true\">,</mo><mi>m</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(N, m)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.10903em;\">N</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathdefault\">m</span><span class=\"mclose\">)</span></span></span></span></td>\n<td><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>s</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>m</mi><mi>a</mi><mi>x</mi><mo stretchy=\"false\">(</mo><msub><mi>z</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">softmax(z_2)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\">s</span><span class=\"mord mathdefault\">o</span><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span><span class=\"mord mathdefault\">t</span><span class=\"mord mathdefault\">m</span><span class=\"mord mathdefault\">a</span><span class=\"mord mathdefault\">x</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.04398em;\">z</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.04398em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span></td>\n<td><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mo stretchy=\"false\">(</mo><mi>V</mi><mo separator=\"true\">,</mo><mi>m</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(V, m)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">V</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathdefault\">m</span><span class=\"mclose\">)</span></span></span></span></td>\n</tr>\n</tbody>\n</table>\n</center>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>s</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>m</mi><mi>a</mi><mi>x</mi><mo stretchy=\"false\">(</mo><msub><mi>z</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo><mo>≡</mo><mover accent=\"true\"><mi>Y</mi><mo>^</mo></mover></mrow><annotation encoding=\"application/x-tex\">softmax(z_2) \\equiv \\hat{Y}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\">s</span><span class=\"mord mathdefault\">o</span><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span><span class=\"mord mathdefault\">t</span><span class=\"mord mathdefault\">m</span><span class=\"mord mathdefault\">a</span><span class=\"mord mathdefault\">x</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.04398em;\">z</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.04398em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">≡</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.9467699999999999em;vertical-align:0em;\"></span><span class=\"mord accent\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9467699999999999em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">Y</span></span></span><span style=\"top:-3.25233em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.25em;\">^</span></span></span></span></span></span></span></span></span> 이므로 예측값이 입력값과 같은 차원을 가지는 것을 알 수 있다. 즉 모델이 반환하는 벡터의 열 벡터는 입력 열 벡터와 순서가 같은 원 핫 벡터이다.</p>\n<h2 id=\"2-모델의-전처리\" style=\"position:relative;\"><a href=\"#2-%EB%AA%A8%EB%8D%B8%EC%9D%98-%EC%A0%84%EC%B2%98%EB%A6%AC\" aria-label=\"2 모델의 전처리 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>2. 모델의 전처리</h2>\n<p>CBOW 모델로 문장의 빈칸을 주위 단어에 기반해 예측하는 과제를 수행해보자. 다음 문장의 빈칸에 뭐가 들어갈까?</p>\n<p>\"npm은 Node.js의 ____ 관리를 위한 패키지 매니저이다.\"</p>\n<p>CBOW 모델을 구현하기 위해서는 텍스트 데이터를 토큰화 한 후, 데이터를 모델에 입력하는 형태로 변환하는 다음 작업이 필요하다.</p>\n<h3 id=\"--중심어center-word와-맥락-단어들context-words\" style=\"position:relative;\"><a href=\"#--%EC%A4%91%EC%8B%AC%EC%96%B4center-word%EC%99%80-%EB%A7%A5%EB%9D%BD-%EB%8B%A8%EC%96%B4%EB%93%A4context-words\" aria-label=\"  중심어center word와 맥락 단어들context words permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>📖  중심어(center word)와 맥락 단어들(context words)</h3>\n<p>자기 지도 학습은 사람이 라벨링을 할 필요가 없다는 장점이 있다. 그러기 위해서는 가공되지 않은 텍스트 데이터에서 훈련 데이터(<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>X</mi></mrow><annotation encoding=\"application/x-tex\">X</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.07847em;\">X</span></span></span></span>, 입력 데이터)와 훈련 타겟(<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>Y</mi></mrow><annotation encoding=\"application/x-tex\">Y</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.22222em;\">Y</span></span></span></span>, 참값)을 구분해서 자료화할 필요가 있다. </p>\n<p>CBOW 모델에서 예측할 대상(target)인 문장의 빈칸을 <code class=\"language-text\">중심어</code>로, 이 단어와 문장 내에서 인접한 단어를 <code class=\"language-text\">맥락 단어</code>로 이름지을 수 있다. <code class=\"language-text\">맥락 단어</code>는 중심어로 부터 거리 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>C</mi></mrow><annotation encoding=\"application/x-tex\">C</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.07153em;\">C</span></span></span></span> 만큼 떨어져 있는 인접한 단어들로 정의하며, <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>C</mi></mrow><annotation encoding=\"application/x-tex\">C</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.07153em;\">C</span></span></span></span>를 <code class=\"language-text\">맥락의 절반 크기(context half-size)</code>라고 하자. <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>C</mi></mrow><annotation encoding=\"application/x-tex\">C</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.07153em;\">C</span></span></span></span>는 모델의 성능을 좌우하는 하이퍼파라미터 중 하나이다. 왼쪽 맥락 단어 리스트와 중심어, 오른쪽 맥락 리스트의 크기를 모두 더한 값을 <code class=\"language-text\">윈도우</code>라고 일컫는다. </p>\n<p>예를 들면, 문장 한개로 구성된 데이터에 대해 다음과 같이 이해할 수 있다. </p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># given tokenized data and context half-size, </span>\n<span class=\"token comment\"># returns center word and list of context words </span>\n<span class=\"token keyword\">def</span> <span class=\"token function\">center_and_context_word</span><span class=\"token punctuation\">(</span>data<span class=\"token punctuation\">,</span> C<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">for</span> i <span class=\"token keyword\">in</span> <span class=\"token builtin\">range</span><span class=\"token punctuation\">(</span>C<span class=\"token punctuation\">,</span> <span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>data<span class=\"token punctuation\">)</span><span class=\"token operator\">-</span>C<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        center_word <span class=\"token operator\">=</span> data<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span>\n        context_words <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span>\n        <span class=\"token keyword\">for</span> j <span class=\"token keyword\">in</span> <span class=\"token builtin\">range</span><span class=\"token punctuation\">(</span>i<span class=\"token operator\">-</span>C<span class=\"token punctuation\">,</span> i<span class=\"token operator\">+</span>C<span class=\"token operator\">+</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n            <span class=\"token keyword\">if</span> j <span class=\"token operator\">!=</span> i<span class=\"token punctuation\">:</span>\n                context_words<span class=\"token punctuation\">.</span>append<span class=\"token punctuation\">(</span>data<span class=\"token punctuation\">[</span>j<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n        <span class=\"token keyword\">yield</span> center_word<span class=\"token punctuation\">,</span> context_words\n\nC <span class=\"token operator\">=</span> <span class=\"token number\">2</span> <span class=\"token comment\"># context half-size</span>\ndata <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token string\">\"npm은\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"Node.js의\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"패키지\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"관리를\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"위한\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"패키지\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"매니저이다\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\".\"</span><span class=\"token punctuation\">]</span>\n\ncenter_word<span class=\"token punctuation\">,</span> context_word <span class=\"token operator\">=</span> <span class=\"token builtin\">next</span><span class=\"token punctuation\">(</span>center_and_context_word<span class=\"token punctuation\">(</span>data<span class=\"token punctuation\">,</span> C<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>center_word<span class=\"token punctuation\">)</span> <span class=\"token comment\"># \"패키지\"</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>context_word<span class=\"token punctuation\">)</span> <span class=\"token comment\"># [\"npm은\", \"Node.js의\", \"관리를\", \"위한\"]</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>context_word <span class=\"token operator\">+</span> center_word<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span> <span class=\"token comment\"># 5, window</span></code></pre></div>\n<p>모델의 입력값은 <code class=\"language-text\">맥락 단어 벡터들의 평균값</code>을 취한다. 사실 CBOW 모델 이름에 Bag이 들어가는 이유는 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>C</mi></mrow><annotation encoding=\"application/x-tex\">C</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.07153em;\">C</span></span></span></span>의 범위 내에 있는 맥락 단어들이 문장에서의 순서에 관계없이 여겨지기 때문이고, 이 특징은 이후에 등장하는 Sequential 모델과 구분되는 차이점이다.</p>\n<h2 id=\"3-모델-훈련하기\" style=\"position:relative;\"><a href=\"#3-%EB%AA%A8%EB%8D%B8-%ED%9B%88%EB%A0%A8%ED%95%98%EA%B8%B0\" aria-label=\"3 모델 훈련하기 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>3. 모델 훈련하기</h2>\n<p>CBOW 모델은 신경망 모델이므로 일반적인 forward propagation, backward propagation, gradient descent 과정을 거친다. 세가지 과정을 <code class=\"language-text\">keras</code> 라이브러리에서 <code class=\"language-text\">Layer</code> 객체로 비교적 간단하게 구현할 수 있다.</p>\n<h3 id=\"-keras로-cbow-구현하기\" style=\"position:relative;\"><a href=\"#-keras%EB%A1%9C-cbow-%EA%B5%AC%ED%98%84%ED%95%98%EA%B8%B0\" aria-label=\" keras로 cbow 구현하기 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>📂 Keras로 CBOW 구현하기</h3>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> tf<span class=\"token punctuation\">.</span>keras <span class=\"token keyword\">import</span> layers\n\n<span class=\"token comment\"># Input size: (batch_size, vocab_size)</span>\ncbow_model <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>keras<span class=\"token punctuation\">.</span>Sequential<span class=\"token punctuation\">{</span>\n    <span class=\"token comment\"># 원 핫 벡터의 배치를 임베드한다</span>\n    input_layer <span class=\"token operator\">=</span> layers<span class=\"token punctuation\">.</span>Embedding<span class=\"token punctuation\">(</span>input_dim<span class=\"token operator\">=</span>vocab_size<span class=\"token punctuation\">,</span> output_dim<span class=\"token operator\">=</span>embed_dim<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> \n    <span class=\"token comment\"># relu 은닉층으로 비용이 음수값을 가지지 않게 한다</span>\n    hidden_layer <span class=\"token operator\">=</span> layers<span class=\"token punctuation\">.</span>Dense<span class=\"token punctuation\">(</span>units<span class=\"token operator\">=</span>embed_dim<span class=\"token punctuation\">,</span>activation<span class=\"token operator\">=</span><span class=\"token string\">'relu'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> \n    <span class=\"token comment\"># 원 핫 벡터의 배치를 확률로 출력한다</span>\n    output_layer <span class=\"token operator\">=</span> layers<span class=\"token punctuation\">.</span>Dense<span class=\"token punctuation\">(</span>input_dim<span class=\"token operator\">=</span>vocab_size<span class=\"token punctuation\">,</span> activation<span class=\"token operator\">=</span><span class=\"token string\">'softmax'</span><span class=\"token punctuation\">)</span>\n<span class=\"token punctuation\">}</span>\n\nbatch_size <span class=\"token operator\">=</span> <span class=\"token number\">256</span>\nepochs <span class=\"token operator\">=</span> <span class=\"token number\">10</span>\n\ncbow_model<span class=\"token punctuation\">.</span><span class=\"token builtin\">compile</span><span class=\"token punctuation\">(</span>\n    optimizer<span class=\"token operator\">=</span><span class=\"token string\">'Adam'</span><span class=\"token punctuation\">,</span> \n    loss<span class=\"token operator\">=</span>tf<span class=\"token punctuation\">.</span>keras<span class=\"token punctuation\">.</span>losses<span class=\"token punctuation\">.</span>CategoricalCrossentropy<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n<span class=\"token punctuation\">)</span>\n\ncbow_model<span class=\"token punctuation\">.</span>fit<span class=\"token punctuation\">(</span>\n    train_data<span class=\"token punctuation\">,</span> train_target<span class=\"token punctuation\">,</span> \n    batch_size<span class=\"token operator\">=</span>batch_size<span class=\"token punctuation\">,</span> epochs<span class=\"token operator\">=</span>epochs\n<span class=\"token punctuation\">)</span></code></pre></div>\n<h2 id=\"4-단어-임베딩-추출하기\" style=\"position:relative;\"><a href=\"#4-%EB%8B%A8%EC%96%B4-%EC%9E%84%EB%B2%A0%EB%94%A9-%EC%B6%94%EC%B6%9C%ED%95%98%EA%B8%B0\" aria-label=\"4 단어 임베딩 추출하기 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>4. 단어 임베딩 추출하기</h2>\n<p>단어 임베딩은 원 핫 벡터에 비해 <code class=\"language-text\">밀도가 높은(Dense) 벡터</code>로, 단어 임베딩에는 여러가지 이점이 있다. 첫째로 단어 임베딩 벡터간의 거리를 비교해서 의미론적(semantic)이고 문법론적인(syntactic) 정보를 얻을 수 있다. 둘째로 차원을 작게 만드는 것, 즉 <code class=\"language-text\">차원 축소(Dimensionality Reduction)</code>를 통해 계산 횟수를 획기적으로 줄일 수 있다. 벡터의 밀도가 높다는 것은 같은 데이터를 상대적으로 작은 차원으로 표현하는 것을 뜻한다. 반면에 벡터의 차원이 증가함에 따라 벡터를 계산하는 횟수는 기하급수적으로 늘어나게 되는데, 이 현상을 <code class=\"language-text\">차원의 저주(the curse of dimensionality)</code>라고 한다. 그 중에서 2차원이나 3차원 벡터는 시각화가 가능하므로 직관적인 이해에 도움이 된다.</p>\n<p>단어 임베딩은 CBOW 모델의 부산물이라고 할 수 있는데, 단어 임베딩은 훈련이 끝난 후 그 결과인 가중치 벡터로 부터 얻을 수 있다.</p>\n<p>단어 임베딩으로 선택할 수 있는 <code class=\"language-text\">옵션</code>은 다음과 같다.</p>\n<ul>\n<li>첫번째 가중치 벡터 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>W</mi><mn>1</mn></msub></mrow><annotation encoding=\"application/x-tex\">W_1</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.83333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> 의 열(column) 벡터</li>\n<li>두번째 가중치 벡터 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>W</mi><mn>2</mn></msub></mrow><annotation encoding=\"application/x-tex\">W_2</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.83333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> 의 행(row) 벡터</li>\n<li>두 가중치 벡터의 평균 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mn>1</mn><mi mathvariant=\"normal\">/</mi><mn>2</mn><mo>∗</mo><mo stretchy=\"false\">(</mo><msub><mi>W</mi><mn>1</mn></msub><mo>+</mo><msubsup><mi>W</mi><mn>2</mn><mi>T</mi></msubsup><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">1/2 *(W_1 + W_2^{T})</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\">1</span><span class=\"mord\">/</span><span class=\"mord\">2</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">∗</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.0913309999999998em;vertical-align:-0.25em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413309999999999em;\"><span style=\"top:-2.4518920000000004em;margin-left:-0.13889em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.13889em;\">T</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.24810799999999997em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span> 의 열 벡터</li>\n</ul>\n<p>마지막 경우는 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mn>1</mn><mi mathvariant=\"normal\">/</mi><mn>2</mn><mo>∗</mo><mo stretchy=\"false\">(</mo><msubsup><mi>W</mi><mn>1</mn><mi>T</mi></msubsup><mo>+</mo><msub><mi>W</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">1/2 * (W_1^{T} + W_2)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\">1</span><span class=\"mord\">/</span><span class=\"mord\">2</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">∗</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.0913309999999998em;vertical-align:-0.25em;\"></span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413309999999999em;\"><span style=\"top:-2.4518920000000004em;margin-left:-0.13889em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.13889em;\">T</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.24810799999999997em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span>의 행 벡터와 같다. 위의 모든 경우에 대해 <code class=\"language-text\">한개의 단어 임베딩 벡터의 크기</code>는 임베딩 크기 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>N</mi></mrow><annotation encoding=\"application/x-tex\">N</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.10903em;\">N</span></span></span></span>에 대해 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mo stretchy=\"false\">(</mo><mi>N</mi><mo separator=\"true\">,</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(N,1)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.10903em;\">N</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\">1</span><span class=\"mclose\">)</span></span></span></span> 또는 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo separator=\"true\">,</mo><mi>N</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(1, N)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">(</span><span class=\"mord\">1</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.10903em;\">N</span><span class=\"mclose\">)</span></span></span></span>인 것을 알 수 있다.</p>\n<h2 id=\"5-모델-평가하기\" style=\"position:relative;\"><a href=\"#5-%EB%AA%A8%EB%8D%B8-%ED%8F%89%EA%B0%80%ED%95%98%EA%B8%B0\" aria-label=\"5 모델 평가하기 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>5. 모델 평가하기</h2>\n<p>모델을 평가하는 방법에는 크게 두가지가 있다.</p>\n<h3 id=\"--내재적-평가와-외재적-평가\" style=\"position:relative;\"><a href=\"#--%EB%82%B4%EC%9E%AC%EC%A0%81-%ED%8F%89%EA%B0%80%EC%99%80-%EC%99%B8%EC%9E%AC%EC%A0%81-%ED%8F%89%EA%B0%80\" aria-label=\"  내재적 평가와 외재적 평가 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>📖  내재적 평가와 외재적 평가</h3>\n<p>내재적 평가(Intrinsic Evaluation)는 임베딩된 단어들의 의미론적이고 문법론적인 관계를 평가하는 방법이다. 유의어(Analogies) 평가나 클러스터링 알고리즘, 또는 PCA 같은 시각화 기법들이 내재적 평가에 포함된다. 반면 외재적 평가(Extrinsic Evaluation)는 모델의 전체적인 성능을 파악하는데에 사용되는 방법이다. 전체 모델을 평가할 수 있지만, 평가 시간이 오래 걸리며 개선 방법에 대한 직관을 얻기 어렵다는 단점이 있다.</p>\n<h3 id=\"--테스트-셋에-대해-내재적으로-평가하기\" style=\"position:relative;\"><a href=\"#--%ED%85%8C%EC%8A%A4%ED%8A%B8-%EC%85%8B%EC%97%90-%EB%8C%80%ED%95%B4-%EB%82%B4%EC%9E%AC%EC%A0%81%EC%9C%BC%EB%A1%9C-%ED%8F%89%EA%B0%80%ED%95%98%EA%B8%B0\" aria-label=\"  테스트 셋에 대해 내재적으로 평가하기 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>📂  테스트 셋에 대해 (내재적으로) 평가하기</h3>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/f9fe38237d093a563174b797d2e6eb7e/e1031/Semantic-Syntactic_Word_Relationship_test_set.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 33.108108108108105%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAHCAYAAAAIy204AAAACXBIWXMAAAsTAAALEwEAmpwYAAABDUlEQVQoz02RWa+CQAyF+f8/jUcIiyyyCDwgBpUdwZqvN5DbpJnT9kx7OmMURSFpmsr1epXb7SZxHEuWZZLnuebwJEkkCALF1Il931fMiUdRJGVZiuF53pl0XVcsyxJyYBz8eDzkfr/reThx0zR6HrXX6yVGGIZSVZU8n0+dSBOa0whVjuOog+HCqetatyBHDBeF7/dbDNM0xbZtJVwuFy1A5GQ1noP4aE6O2jAMquxYHXV93/+tDAkC9v1+tYjt+y6fz0cxdRRgXFyWRTGbHXiaJjFQxQeM46jNaMJ7YNu2ybquZxMuY13Xnfn/Ded5FoNV+N22bbXAFB6bC2CUoRLVcBgCRgB5uAyDT+4HL/EJNckeBBkAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Semantic Syntactic Word Relationship test set\"\n        title=\"Semantic Syntactic Word Relationship test set\"\n        src=\"/static/f9fe38237d093a563174b797d2e6eb7e/fcda8/Semantic-Syntactic_Word_Relationship_test_set.png\"\n        srcset=\"/static/f9fe38237d093a563174b797d2e6eb7e/12f09/Semantic-Syntactic_Word_Relationship_test_set.png 148w,\n/static/f9fe38237d093a563174b797d2e6eb7e/e4a3f/Semantic-Syntactic_Word_Relationship_test_set.png 295w,\n/static/f9fe38237d093a563174b797d2e6eb7e/fcda8/Semantic-Syntactic_Word_Relationship_test_set.png 590w,\n/static/f9fe38237d093a563174b797d2e6eb7e/e1031/Semantic-Syntactic_Word_Relationship_test_set.png 803w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span>\n<em>table from Mikolov et al., 2013, Efficient Estimation of Word Representations in Vector Space</em></p>\n<p>위의 표는 4개의 모델을 두가지 훈련 데이터에 대해 평가한 결과이다. 첫번째 훈련 데이터는 <code class=\"language-text\">의미론적(semantic)</code>이고 <code class=\"language-text\">문법론적(syntactic)</code> 관계 정확도인데, CBOW가 의미론적 정확도는 Skip-gram보다 두배 이상 떨어지지만 문법적 정확도에서는 조금 더 나은 것을 알 수 있다. 그렇지만 의미론적 정확도에 비해 문법론적 정확도에서 평가 모델들의 편차가 더 적었다. 두번째 데이터 셋에 대해서는 CBOW가 Skip-gram보다 단어 관계 평가가 조금 더 나은 것을 볼 수 있다.</p>\n<p><em>참고 : 논문에서는 1억개가 넘어가지 않는 vocab에 대해 CBOW 모델을 훈련했으며, <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>C</mi><mo>=</mo><mn>4</mn></mrow><annotation encoding=\"application/x-tex\">C=4</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.07153em;\">C</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">4</span></span></span></span> 설정에서 log-linear 분류로 최적의 결과를 얻었다고 한다.</em></p>\n<h2 id=\"출처\" style=\"position:relative;\"><a href=\"#%EC%B6%9C%EC%B2%98\" aria-label=\"출처 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>출처</h2>\n<ol>\n<li>Mikolov et al., 2013, Efficient Estimation of Word Representations in Vector Space</li>\n<li>Coursera, deeplearning.ai, NLP Specialization, Course 2, Natural Language Processing with Probabilistic Models</li>\n</ol>","excerpt":"은 2013년 구글에서 고안한 자연어 처리 아이디어로, 이에 기반한 모델은 와  두가지가 있다. 이 글은 그 중에서  모델을 원 논문과 deeplearning.ai 수업을 참고하여 정리한 글이다.  원 논문: Mikolov et. al., 2013,…","tableOfContents":"<ul>\n<li>\n<p><a href=\"/word2vec_cbow/#0-cbow%EB%9E%80\">0. CBOW란</a></p>\n<ul>\n<li><a href=\"/word2vec_cbow/#-skip-gram%EA%B3%BC%EC%9D%98-%EC%B0%A8%EC%9D%B4%EC%A0%90\">📖 Skip-gram과의 차이점</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"/word2vec_cbow/#1-%EB%AA%A8%EB%8D%B8%EC%9D%98-%EA%B5%AC%EC%A1%B0\">1. 모델의 구조</a></p>\n<ul>\n<li><a href=\"/word2vec_cbow/#-%EB%B2%A1%ED%84%B0%EC%9D%98-%EC%B0%A8%EC%9B%90\">📖 벡터의 차원</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"/word2vec_cbow/#2-%EB%AA%A8%EB%8D%B8%EC%9D%98-%EC%A0%84%EC%B2%98%EB%A6%AC\">2. 모델의 전처리</a></p>\n<ul>\n<li><a href=\"/word2vec_cbow/#--%EC%A4%91%EC%8B%AC%EC%96%B4center-word%EC%99%80-%EB%A7%A5%EB%9D%BD-%EB%8B%A8%EC%96%B4%EB%93%A4context-words\">📖  중심어(center word)와 맥락 단어들(context words)</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"/word2vec_cbow/#3-%EB%AA%A8%EB%8D%B8-%ED%9B%88%EB%A0%A8%ED%95%98%EA%B8%B0\">3. 모델 훈련하기</a></p>\n<ul>\n<li><a href=\"/word2vec_cbow/#-keras%EB%A1%9C-cbow-%EA%B5%AC%ED%98%84%ED%95%98%EA%B8%B0\">📂 Keras로 CBOW 구현하기</a></li>\n</ul>\n</li>\n<li><a href=\"/word2vec_cbow/#4-%EB%8B%A8%EC%96%B4-%EC%9E%84%EB%B2%A0%EB%94%A9-%EC%B6%94%EC%B6%9C%ED%95%98%EA%B8%B0\">4. 단어 임베딩 추출하기</a></li>\n<li>\n<p><a href=\"/word2vec_cbow/#5-%EB%AA%A8%EB%8D%B8-%ED%8F%89%EA%B0%80%ED%95%98%EA%B8%B0\">5. 모델 평가하기</a></p>\n<ul>\n<li><a href=\"/word2vec_cbow/#--%EB%82%B4%EC%9E%AC%EC%A0%81-%ED%8F%89%EA%B0%80%EC%99%80-%EC%99%B8%EC%9E%AC%EC%A0%81-%ED%8F%89%EA%B0%80\">📖  내재적 평가와 외재적 평가</a></li>\n<li><a href=\"/word2vec_cbow/#--%ED%85%8C%EC%8A%A4%ED%8A%B8-%EC%85%8B%EC%97%90-%EB%8C%80%ED%95%B4-%EB%82%B4%EC%9E%AC%EC%A0%81%EC%9C%BC%EB%A1%9C-%ED%8F%89%EA%B0%80%ED%95%98%EA%B8%B0\">📂  테스트 셋에 대해 (내재적으로) 평가하기</a></li>\n</ul>\n</li>\n<li><a href=\"/word2vec_cbow/#%EC%B6%9C%EC%B2%98\">출처</a></li>\n</ul>","fields":{"slug":"/word2vec_cbow/"},"frontmatter":{"title":"word2vec - Continuous Bag-of-Words(CBOW)","date":"Feb 25, 2022","tags":["NLP","word2vec","Word Embedding"],"keywords":["AI doodle","Near"],"update":"Jan 01, 0001"}}},"pageContext":{"slug":"/word2vec_cbow/","series":[],"lastmod":"2022-02-25"}},"staticQueryHashes":["2027115977","694178885"]}