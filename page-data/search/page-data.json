{"componentChunkName":"component---src-pages-search-tsx","path":"/search/","result":{"data":{"allMarkdownRemark":{"edges":[{"node":{"rawMarkdownBody":"\n이 글은 책 <파이썬 알고리즘 인터뷰>(박상길 지음)을 참고하여 코딩 테스트를 준비하기 위한 지식과 학습 방법을 정리하였다.\n\n### 문제 개수로 살펴본 유형별 출제빈도\n\n> 책의 목차에 따른 문제 개수를 센 것이며, 공부의 기준으로 삼기보다 참고사항으로 보기를 추천한다.\n\n2부 파이썬\n- 문자열 조작 (6) ⭐️⭐️\n\n3부 선형자료구조\n- 배열 (6) ⭐️⭐️\n- 연결 리스트 (7) ⭐️⭐️\n- 스택, 큐 (6) ⭐️⭐️\n- 데크, 우선순위 큐 (2)\n- 해시 테이블 (4) ⭐️\n\n4부 비선형 자료구조\n- 그래프 (7) ⭐️⭐️\n- 최단 경로문제 (2)\n- 트리 (13) ⭐️⭐️\n- 힙 (1)\n- 트라이 (2)\n\n5부 알고리즘\n- 정렬 (7) ⭐️⭐️\n- 이진검색 (5) ⭐️\n- 비트조작 (5) ⭐️\n- 슬라이딩 윈도우 (3)\n- 그리디 알고리즘 (5) ⭐️\n- 분할 정복 (2)\n- 다이나믹 프로그래밍 (4) ⭐️\n\n### 코딩 테스트 준비 방법\n- 코딩 테스트 플랫폼이 있다면 익숙해지도록 연습한다.\n- **코드 스니펫**을 만들어 둔다.\n    - 어려운 알고리즘이나 코드를 본인의 방식으로 작성해서 정리하면 찾아보기 편리하다.\n    - 시험시 참고할 수 없다고 해도 코드 스니펫을 통해 본인의 약점을 떠올리고, 보충하고, 유의하기 더 쉽다.\n- 문제를 풀 때는 모든 테스트 케이스를 통과하도록 연습한다.\n    - 코드를 고치고 실행하는 것을 반복(맞왜틀) 하기보다, 코드를 충분히 확인 한 후 제출하도록 습관을 들인다. 시간을 절약하고 실력을 높일 수 있다.\n- 예외 처리에 유의한다.\n- 파이썬으로 코딩 테스트를 보는 경우, 다른 언어에 비해 타임아웃에 더 유의한다.\n- 풀이 시간이 초과 되어도 끝까지 풀어보는 습관을 들인다.\n    - 코딩 테스트 리뷰 또한 중요하다.\n- REPL(Read-eval-print Loop) 방식으로 코드를 나누어 실행하면 버그를 줄이는데 많은 도움이 된다. \n    - 예를 들어 Jupyter Notebook을 활용할 수 있다.\n\n> [코딩 테스트를 위한 코드 스니펫 정리]()\n\n\n### 파이썬 코딩 스타일\n- 변수명은 Naming Convention에 따라 작성하고, 주석은 리뷰를 위해 가급적 다는 것이 좋다.\n    - Naming Convention은 따로 정의된 컨벤션이 없는 경우 PEP 8에서 언급된 Snake Case를 따른다. (`snake_case` vs. `camelCase`)\n    - 파이썬은 Dynamic Typing 언어로 제네릭이 필요없지만(to-be-specified-later 스타일로 재사용성을 높인다), PEP 484에서 언급된 Type Hint를 다는 것으로 버그 확률을 낮출 수 있다.\n- 코드의 간결함을 위해 리스트 컴프리헨션을 사용한다.\n- 구글 파이썬 스타일 가이드:\n    - 함수 기본 값으로 Mutable Object(리스트나 딕셔너리 등)를 사용하지 않는 것이 좋다. 대신 Immutable Object를 사용하여 None을 할당하는 것이 좋다.\n    - Boolean은 Implicit하게 (`if bool_var:`), 정수나 문자열은 Explicit하게(`if int_var == 0 or str_var == '':`) 활용한다.\n    - 가독성을 위해 최대 줄 길이는 80자로 맞춘다.\n- 파이써닉한 코드를 위해 따로 [강의](https://school.programmers.co.kr/learn/courses/4008)를 들을 수 있다. 혹은, 많은 문제를 풀고 매번 다른 사람의 풀이를 참고하면서 익힐 수도 있다. 또 다른 방법으로는 책 <파이썬 코딩의 기술> 이나 <전문가를 위한 파이썬>을 볼 수 있지만 코딩 테스트 준비를 위해 보기에는 과한 것 같다!\n\n> [PEP 8](https://peps.python.org/pep-0008/)\n\n> [PEP 484](https://peps.python.org/pep-0484/)\n\n> [Google Python Style Guide](https://google.github.io/styleguide/pyguide.html)\n\n\n코딩 테스트를 위한 것은 아니지만 코드 작성과 관련된 참고할 책은 다음과 같다.\n1. <파이썬 코딩의 기술>(브렛 슬라킨 지음)\n2. <전문가를 위한 파이썬>(루시아누 하말류 지음)\n3. <클린코드>(로버트 C. 마틴 지음)\n4. <프로그래밍 수련법>(브라이언 W. 커니핸, 롭 파이크 지음)","excerpt":"이 글은 책 <파이썬 알고리즘 인터뷰>(박상길 지음)을 참고하여 코딩 테스트를 준비하기 위한 지식과 학습 방법을 정리하였다. 문제 개수로 살펴본 유형별 출제빈도 책의 목차에 따른 문제 개수를 센 것이며, 공부의 기준으로 삼기보다 참고사항으로 보기를 …","fields":{"slug":"/python_coding_test1/"},"frontmatter":{"date":"Jul 27, 2022","title":"코딩 테스트 준비법 (파이썬)","tags":["Algorithms","Python"],"update":"Jan 01, 0001"}}},{"node":{"rawMarkdownBody":"\n이 글은 MySQL의 SQL 쿼리 중 DML(Data Manipulation Language), 즉 테이블의 행을 검색 및 변경하는 기능을 정리하고 또 찾아보기 위한 목적으로 작성했다.\n\n## 들어가며\n\n> [여기](https://www.w3schools.com/mysql/trymysql.asp?filename=trysql_select_all)서 웹으로 SQL 쿼리를 실행해 볼 수 있다.\n\n- SQL 쿼리는 대문자와 소문자를 구분하지 않는다. 즉, 쿼리를 대문자로 쓰던 소문자로 쓰던 섞어서 쓰던 같은 결과를 도출한다. 아래에서 다루겠지만, 함수뿐만이 아닌 문자열 데이터도 대문자와 소문자를 구분하지 않는다.\n\n- SQL 쿼리의 한 줄 주석은 `-- {주석}` 형태로 쓸 수 있다.\n\n\n## 테이블 내 탐색\n\n### SELECT\n\n데이터베이스에서 테이블의 행을 검색하기 위한 가장 기본적인 함수는 `SELECT {칼럼(column)의 이름, 또는 불러올 값}`이며 칼럼을 검색하는 경우 뒤에 `FROM {테이블}`로 칼럼이 속한 테이블 이름을 명시한다.\n\n```SQL\nSELECT \n    col1 AS First, \n    col2, 3, 'sql', NULL\nFROM table1\n```\n\n- 전체 행을 선택하기 위해 `SELECT *` 문을 쓸 수 있다.\n- `SELECT` 뒤에는 상수, 문자열, NULL 값을 쓸 수 있다.\n- 검색 결과에 가져올 칼럼의 이름을 바꾸려면 `SELECT {본래 이름} AS {바꿀 이름}`을 활용한다.\n\n\n나아가 `WHERE {행 조건}`으로 검색할 행의 조건을 지정할 수 있으며 `ORDER BY {정렬 기준}`으로 행을 정렬할 수 있다. 또한 `LIMIT {가져올 갯수}`를 통해 가져올 행의 개수를 설정할 수 있다.\n\n```SQL\nSELECT col1, col2, col3\nFROM table1\nWHERE condition1\nORDER BY sort_standard1\nLIMIT integer1\n```\n\n- `ORDER BY`는 디폴트로 오름차순으로 정리하며, `ORDER BY {정렬 기준} ASC`으로 오름차순을 명시할 수 있다. 내림차순은 `ORDER BY {정렬 기준} DESC`를 활용한다.\n- `LIMIT {건너뛸 갯수}, {가져올 갯수}`로 인자를 두개 입력할 수 있다. 이 경우, `건너뛸 갯수 : 건너뛸 갯수 + 가져올 갯수` 범위만큼 검색한다.\n\n\n행을 그룹으로 묶어서 검색할 수도 있다. `GROUP BY {그룹들}`은 그룹별 값을 도출하며, `HAVING {그룹 내 조건}`을 통해 그룹으로 묶은 뒤의 조건을 설정할 수 있다.\n\n```sql\nSELECT \n    col1, \n    COUNT (DISTINCT col2)\nFROM table1\nWHERE condition1\nGROUP BY col1\nHAVING in_group_condition1\n```\n\n- `GROUP BY {그룹들}`은 복수의 기준으로 그룹을 묶을 수 있다.\n- `WHERE`가 그룹하기 전에 조건을 기준으로 하는 필터라면, `HAVING`은 그룹한 이후의 필터이다. \n- `DISTINCT`는 중복 값을 제거하는 연산이며, 종합하여 `COUNT (DISTINCT {칼럼})`은 칼럼 내의 고유한 값들의 개수를 세어 반환한다.\n\n\n### 연산자\n\nSQL 쿼리의 연산자는 파이썬과 비슷하기도, 다르기도 하다.\n\n사칙연산  `+, - , *, /`과 나머지 연산 `%, MOD`을 사용할 수 있다.\n\n```sql\nSELECT (col1 + col2) / 2, col1 MOD 3\nFROM table1\n```\n\n논리 연산자로 `{값} IS /NOT/ {True 또는 False}`을 사용할 수 있다.\n```sql\nSELECT col1 IS TRUE, col2 IS NOT TRUE\nFROM table1\n```\n\n논리 연산자로 `AND 또는 &&`와 `OR 또는 ||`를 사용할 수 있다.\n```sql\nSELECT col1, col2\nFROM table1\nWHERE col1 IS TRUE AND col2 IS NOT TRUE\n```\n\n등호 `=`와 부등호 `!= 또는 <>`를 사용할 수 있다. 등호는 파이썬과 다르게 `=`로 나타냄에 주의하자.\n```sql\nSELECT col1, col1 = col2 AS Equality, col3\nFROM table1\nWHERE col3 <> 1\n```\n\n`/NOT/ BETWEEN {최소값} AND {최대값}`로 데이터의 범위를 검증할 수 있다. \n```sql\nSELECT col1, col2\nFROM table1\nWHERE col2 BETWEEN 0 AND 5\n```\n\n데이터가 어떤 값 사이에 있는지 `/NOT/ IN ({값들})`로 검증할 수 있다.\n```sql\nSELECT col1 IN ('good', 'fair', 'excellent'), col2\nFROM table1\n```\n\n최소값 `MIN({값들})`, 최대값 `MAX({값들})`, 개수 `COUNT({값들})`, 합 `SUM({값들})`, 평균 `AVG({값들})`을 구할 수 있다.\n```sql\nSELECT\n    MAX(col1),\n    MIN(col1),\n    COUNT(col1),\n    SUM(col1),\n    AVG(col1)\nFROM table1\n```\n- `COUNT({값들})`는 NULL 값을 제외한 행의 개수를 센다.\n\n\n### 문자열\n\n파이썬에는 정규표현식 개념이 있어서 패턴에 맞는 문자열을 검색할 수 있다. SQL에도 비슷한 기능이 있으며, `LIKE {...%패턴...}`형식 또는 `LIKE {..._패턴...}`으로 구현한다. 전자는 `%`에 해당되는 문자가 없거나 여러개의 문자가 해당될 수 있는 반면, 후자인 `_`에는 한 개의 문자만 대응 될 수 있다. 또한, 글의 초반에 언급한 대로 SQL 문자열은 대문자와 소문자를 구분하지 않는다.\n\n```sql\nSELECT\n    'SQL' LIKE '%sql%', -- True\n    'SQL' LIKE 'sql%',  -- True\n    'SQL' LIKE 'S%',    -- True\n    'SQL' LIKE '%L',    -- True\n    'SQL' LIKE 'S%L',   -- True\n    'SQL' LIKE '_ql',   -- True\n    'SQL' LIKE '_q_',   -- True\n    'SQL' LIKE '__ql_', -- False\n```\n\n또한 `REPLACE({문자열}, {대체될 문자열}, {대체할 문자열})`을 활용해 부분 문자열을 문자열에서 찾아 다른 문자열로 대체할 수 있다.\n```sql\nSELECT\n    REPLACE('It is 12 AM', 'AM', 'PM') -- It is 12 PM\n```\n\n입력한 문자열을 순서대로 이어주는 파이썬의 join과 비슷한 기능을 하는 함수도 있다. `CONCAT ({문자열들})`은 입력 문자열을 그대로 이어주고, `CONCAT_WS({연결 문자열}, {문자열들})`을 통해 첫번쨰 인자로 전달한 문자열을 이용해 다음 인자로 주어진 문자열들을 잇는다.\n\n```sql\nSELECT CONCAT('Hi', ' ', 'there ', '!'), -- Hi there !\nSELECT CONCAT_WS('-', 2022, 7, 8, 'AM') -- 2022-7-8-AM\n```\n- 문자열이 아닌 정수값을 입력해도 문자열로 이은 결과를 반환한다.\n\n\n문자열을 왼쪽 또는 오른쪽에서 자를 수도 있다. 왼쪽에서부터 자르기 위해서 `LEFT({대상 문자열}, {자를 길이})`를 활용하고, 오른쪽에서 부터 자르기 위해 `RIGHT({대상 문자열}, {자를 길이})`를 활용한다.\n\n```sql\nSELECT\n    LEFT('yyyy-mm-dd hh:mm', 10) AS DATE, -- yyyy-mm-dd\n    RIGHT('yyyy-mm-dd hh:mm', 5) AS TIME -- hh:mm\n```\n\n문자열의 왼쪽 또는 오른쪽에 패딩을 할 수 있다. 왼쪽 패딩을 위해서는 `LPAD ({대상 문자열}, {패딩 횟수}, {패딩 문자})`을, 오른쪽 패딩을 위해서는 `RPAD ({대상 문자열}, {패딩 횟수}, {패딩 문자})`을 활용한다.\n\n```sql\nSELECT\n  LPAD('This is padding', 3, '.'), -- ...This is padding\n  RPAD('This is padding', 3, '.')  -- This is padding...\n```\n\n공백을 줄여주는 함수도 있다. `TRIM {문자열}`을 통해 입력 문자열의 양쪽 공백을 제거할 수 있다.\n```sql\nSELECT\n    TRIM(' trim me ') -- 'trim me'\n```\n\n### 시간/날짜\n\nMySQL에는 날짜와 시간을 나타내는 변수 자료형 DATETIME이 있다. \n\nDATETIME 자료형을 생성하는 방법은 `DATE({날짜 문자열})` 또는 `TIME({시간 문자열})`을 이용하는 것이다.\n\n```sql\nSELECT\n  '2022-7-1' = '2022-07-01', -- FALSE\n  DATE('2022-7-1') = DATE('2022-07-01'), -- TRUE\n  '1:18:4' = '01:18:04', -- FALSE\n  TIME('1:18:4') = TIME('01:18:04'); -- TRUE\n```\n\n- `DATE`와 `TIME`에 전달하는 문자열은 0을 제외해도 무방하다.\n\nDATETIME에서 연도, 월, 요일명, 날짜, 시간, 분, 초를 추출하는 함수는 각각 `YEAR({DATETIME})`, `MONTH({DATETIME})`, `DAYNAME({DATETIME})`, `DAY({DATETIME})`, `HOUR({DATETIME})`, `MINUTE({DATETIME})`, `SECOND({DATETIME})`이다.\n\n```sql\nSELECT\n  YEAR(NOW()) AS YEAR,\n  MONTH(NOW()) AS MONTH,\n  DAYNAME(NOW()) AS DAYNAME,\n  DAY(NOW()) AS DAY,\n  HOUR(NOW()) AS HOUR,\n  MINUTE(NOW()) AS MINUTE,\n  SECOND(NOW()) AS SECOND\n```\n- `NOW()`는 현재 날짜와 시간을 DATETIME 자료형으로 반환한다.\n\n### 조건문\n\n파이썬의 if 조건문에 비할 바는 아니지만, SQL에서도 조건에 따라 다른 값을 반환하도록 지정할 수 있다. `IF ({조건}, {조건이 참일 때 반환값}, {조건이 거짓일 때 반환값})` 형식으로 사용한다.\n\n```sql\nSELECT \n    IF (col1 > 3, 'Bigger than 3', 'Equal or Smaller than 3')\nFROM table1\n```\n\n반환하고자 하는 경우의 수를 더 세세하게 나눌 수도 있다. `CASE WHEN {경우1} THEN {결과값1} ... ELSE {이외의 경우 반환값} END` 형식으로 나타내며, `WHEN {경우} THEN {결과값}`은 추가할 수 있다. 만약 앞서 나온 조건에 해당되는 경우 이미 반환값이 결정되었을 것이므로, 뒤에 나온 조건은 앞서 나온 조건들에 해당되지 않는 경우만 포함한다.\n\n```sql\nSELECT\n    CASE\n        WHEN -1 > 0 THEN '-1은 양수다.'\n        WHEN -1 = 0 THEN '-1은 0이다.'\n        ELSE '-1은 음수다.'\n    END\n```\n\n검색한 행에 NULL값이 있고 이 값만 변경하고자 하면 `IF ({비교 대상} = NULL, {바꿀 값}, {비교 대상})`으로 표현할 수 있다. 특별히 이와 같은 작업을 하는 함수가 있으며, `IFNULL({비교 대상}, {바꿀 값})`으로 간단하게 표현한다.\n\n```sql\nSELECT\n    IFNULL(col1, '0')\nFROM table1\n```\n\n\n\n## 테이블 합치기\n\n### 서브쿼리\n\n서브쿼리란 쿼리를 통해 검색하고자 하는 값을 임시로 불러와 쿼리 안에 쿼리를 사용하는 방법이다. 바로 다음에 나오는 `JOIN`과 같이 외부 테이블의 값을 가져오는 방법 중의 하나이지만, 데이터를 저장하지 않고 임시로 사용한다는 점에서 차이가 있다. 서브 쿼리는 괄호내에 쿼리문을 작성하는 `(SELECT ... FROM ...)` 형식으로 사용할 수 있다. 한가지 예는 다음과 같다. \n\n```sql\nSELECT col1, col2\nFROM table1\nWHERE col2 IN (SELECT col3 FROM table2)\nORDER BY col1\n```\n\n여기서는 `WHERE`의 조건문에 서브쿼리를 사용하여 외부 테이블의 값을 가져왔다. \n\n\n### JOIN\n\n원하는 정보가 여러 테이블에 흩어져 있는 경우, 몇 개의 외부 테이블을 합치는 명령어가 필요하다. Pandas 패키지에서 데이터프레임을 병합하는 경우를 생각해보자(사실 pandas 공식 문서에는 merge를 SQL의 JOIN을 들어 설명하고 있지만, 이미 pandas를 사용해본 경험이 있다면 이해가 빠를 것이다). `df1.merge(df2, how='inner', on='col1')`명령어는 df1에 df2를 양쪽 데이터프레임에 공통적으로 존재하는 col1의 값들이 일치되도록 병합한다. 반면 `df1.merge(df2, how='left', on='col1')`은 왼쪽, 즉 df1의 키만 보존하며 두 데이터프레임을 병합한다.\n\n이처럼 테이블을 합치는 방법은 여러가지가 있으며, 여기서는 INNER, OUTER, CROSS JOIN을 소개한다. \n\n\n#### INNER JOIN\n\nINNER JOIN을 위해서는 합치고자 하는 테이블에 공통된 키가 있어야 한다. 쿼리는 `JOIN {외부 테이블} ON {공통된 칼럼}` 형식이다. 공통된 키에 대해 병합되기 때문에 공통된 칼럼에 대한 두 테이블의 교집합을 떠올릴 수 있다(단, 엄밀히 테이블은 집합이 아니다!).\n\n```sql\nSELECT A.col1, A.col2, B.col2, C.col2\nFROM table1 A\nJOIN table2 B\n  ON A.col1 = B.col1\nJOIN table3 C\n  ON A.col2 = C.col3\nWHERE condition1\nORDER BY sort_standard1\n```\n- `FROM table1 A`는 table1을 A로 명명하고, `JOIN table2 B`는 table2를 B로 명명한다. 이후 어느 테이블에서 행을 가져오는지 명시할 때 새로 명시한 이름을 사용할 수 있다.\n- 위의 예에서 처럼 여러개의 테이블을 병합할 수 있다. 쿼리 형식 `JOIN {외부 테이블} ON {공통된 칼럼}`만 지키면 된다.\n- 또한, `WHERE`나 `ORDER BY`등 `SELECT`에서 익힌 조건문들도 그대로 사용할 수 있다는 걸 잊지 말자.\n\n\n그렇다면 하나의 테이블을 두번 JOIN할 일은 없을까? 의외로 하나의 테이블을 다시 외부 테이블처럼 JOIN하는 활용처도 있으며, 이를 SELF JOIN이라고 한다.\n\n```sql\nSELECT\n    A.col1, A.col2 AS val,\n    B.col1, B.col2 AS next_val\nFROM table1 A \nJOIN table2 B\n  ON A.col2 + 1 = B.col2\n```\n이처럼 하나의 칼럼에 lag를 두는 경우, 서로 다른 칼럼을 가져오는 것처럼 JOIN으로 불러와 활용할 수 있다. \n\n#### OUTER JOIN\n\nOUTER JOIN은 공통된 키가 아닌 하나의 테이블이 가지는 키를 기준으로 합치는 방법이며, 합치고자 하는 방향에 따라 LEFT JOIN과 RIGHT JOIN으로 나뉜다. 쿼리는 `LEFT JOIN {외부 테이블}`과 `RIGHT JOIN {외부 테이블}`이다.\n\n```sql\nSELECT A.col1, B.col1, A.col2, A.col3\nFROM table1 A\nLEFT JOIN table2 B\n       ON A.col2 = B.col2 AND A.col3 = B.col3\n```\n\n- `LEFT JOIN`을 `RIGHT JOIN`으로 바꿔 쓸 수 있다. \n- LEFT JOIN은 table1(또는 A)의 키를 기준으로 합치며, RIGHT JOIN은 table2(또는 B)의 키를 기준으로 합친다. 기준이 되지 않는 테이블에 있는 칼럼 값이 NULL이더라도 기준이 되는 칼럼 값이 존재하면 반환하므로, 결과값으로 반환되는 테이블의 행의 갯수는 기준이 되는 칼럼의 합치기 전 행의 갯수와 같다.\n\n#### CROSS JOIN\n두 칼럼 값의 모든 조합 쌍을 나타내는 카르테지안 프로덕트(Cartesian Product)를 실행하는 방법도 있다. `CROSS JOIN {외부 테이블}` 명령으로 구현하며, 이를 CROSS JOIN이라고 한다.\n\n```sql\nSELECT A.col1, B.col2\nFROM table1 A\nCROSS JOIN table2 B\n```\n결과값은 `A.col1`와 `B.col2`의 모든 조합 썅을 담은 테이블이다.\n\n\n\n## 테이블 및 데이터 생성/수정/삭제\n\n### 테이블 생성/수정/삭제\n\n테이블을 생성하기 위해 `CREATE TABLE {테이블 이름} ({칼럼1}{칼럼1 자료형}{칼럼1 옵션들}, ..., {칼럼N}{칼럼N 자료형}{칼럼N 옵션들})` 명령어를 활용할 수 있다.\n\n```sql\nCREATE TABLE table1 (\n    col1 INT AUTO_INCREMENT PRIMARY KEY,\n    col2 VARCHAR(10) NOT NULL,\n    col3 VARCHAR(10) UNIQUE NOT NULL,\n    col5 INT UNSIGNED,\n    col6 INT DEFAULT 0\n)\n```\n- 옵션은 생략할 수 있다.\n- `AUTO_INCREMENT`는 행을 생성 할 때마다 자동으로 1씩 증가하는 칼럼 옵션이다.\n- `UNIQUE`는 중복 입력만 불가한 칼럼 옵션이다.\n- `NOT NULL`은 NULL 값만 입력 불가한 칼럼 옵션이다.\n- `PRIMARY KEY`는 중복 입력과 NULL 값 입력이 불가한 칼럼 옵션이다. `UNIQUE`와 `NOT NULL`을 합한 기능을 제공한다.\n- `UNSIGNED`는 숫자 자료형에만 적용 가능하며, 양수만 입력받는 칼럼 옵션이다.\n\n당연하게도 테이블 이름이나 컬럼의 옵션, 또는 컬럼 자체를 수정할 일이 발생한다. 테이블의 이름을 변경하는 명령어는 `ALTER TABLE {테이블 이름} RENAME TO {바꿀 이름}`이다. 테이블을 삭제하기 위해서는 `DROP TABLE {테이블 이름}`을 활용한다.\n\n```sql\nALTER TABLE table1 RENAME TO better_table,\nDROP TABLE better_table\n```\n\n컬럼의 이름 또는 자료형을 변경하는 명령어는 `ALTER COLUMN {칼럼 이름} {바꿀 이름} {바꿀 자료형}`이다. 즉 칼럼 이름을 통해 칼럼에 접근해 자료형을 수정할 수 있다. 컬럼을 추가하는 함수는 `ADD COLUMN {추가할 칼럼 이름} {자료형} {옵션}`이며, 삭제는 테이블과 비슷하게 `DROP COLUMN {칼럼 이름}`을 활용한다.\n\n```sql\nALTER COLUMN col1 col1 INT,\nALTER COLUMN col2 cool_col VARCHAR(10), \nDROP COLUMN col3,\nADD COLUMN col1_2 INT AFTER col1\n```\n- 함수들을 `,`로 구분해 여러 함수를 한번에 실행할 수 있다.\n\n### 데이터 추가/삭제/변형\n\n테이블과 칼럼을 수정하는 방법을 알았으니, 데이터를 수정하는 방법도 필요한 것을 알 수 있다. 데이터란 `SELECT`문으로 검색하던 일련의 값들이 담긴 행을 의미한다.\n\n데이터를 추가하기 위해서는 `INSERT INTO {테이블 이름} ({칼럼1, 칼럼2, 칼럼3}) VALUES ({칼럼1 값, 칼럼2 값, 칼럼3 값})`을 활용한다. \n\n```sql\nINSERT INTO table1 ({col1, col2, col3})\n     VALUES ({val1, val2, val3})\n```\n\n특정 조건에 해당하는 데이터를 삭제하기 위해서는 `DELETE FROM {테이블 이름} WHERE {조건문}`을 활용한다. 이 때 삭제되는 것은 칼럼이 아닌 행임에 유의한다. 테이블을 초기화하기 위해서는 `TRUNCATE {테이블 이름}`을 활용한다.\n\n```sql\nDELETE FROM table1\n      WHERE ISNULL(col1)\n```\n\n테이블 내의 데이터들을 특정 조건으로 필터링한 뒤에 필터링 된 데이터만 바꾸고 싶은 경우가 있다. 이때는 `UPDATE {테이블 이름} SET {변경 사항들} WHERE {조건문}`을 사용한다.\n\n```sql\nUPDATE table1\nSET \n    col1 = 'changed',\n    col2 = 2\nWHERE col3 IN (\n        SELECT col1\n        FROM table2\n    )\n```\n\n- `WHERE`로 시작하는 조건문 없이, 모든 행을 변경할 수는 없다. \n\n\n\n## 참고자료\n1. 얄코의 MySQL, https://www.yalco.kr/lectures/sql/\n3. 프로그래머스 SQL 고득점 Kit, https://school.programmers.co.kr/learn/challenges?tab=sql_practice_kit\n1. 책 <데이터베이스 첫걸음>, 미크, 기무라 메이지\n4. MySQL 8.0 Reference Manual, https://docs.oracle.com/cd/E17952_01/mysql-8.0-en/index.html","excerpt":"이 글은 MySQL의 SQL 쿼리 중 DML(Data Manipulation Language), 즉 테이블의 행을 검색 및 변경하는 기능을 정리하고 또 찾아보기 위한 목적으로 작성했다. 들어가며 여기서 웹으로 SQL 쿼리를 실행해 볼 수 있다. SQ…","fields":{"slug":"/sql_cheat_sheet/"},"frontmatter":{"date":"Jul 06, 2022","title":"SQL Cheat Sheet","tags":["Database","SQL"],"update":"Jan 01, 0001"}}},{"node":{"rawMarkdownBody":"\n## 들어가며\n머신 러닝 프로젝트를 수행할 때 글로벌 상수(constants)를 정의해야 할 일이 생긴다. 예를 들면 시퀀스 데이터의 경우 최대 길이 `max_length`를 설정하고 데이터를 조건에 따라 설정해둔 최대 길이로 자른다(truncate)던지 vocabulary 파일을 불러오고 또 결과값을 저장할 파일 경로를 설정해둔다던지 하는 것이다. 더 보편적으로는 학습을 위한 `batch_size`과 `epoch`를 정해둘 필요가 있다. 여러 노트북에서 이런 설정값(configuration)들이 많아질 경우 1️⃣class 형태로 저장해두고 불러서 쓰거나 2️⃣parser로 처리하는 경우를 보았는데 후자의 경우는 제대로 이해하지 못하고 있었다. 그러다가 Python의 내장 패키지인 `configparser`를 쉽고 친절하게 설명해주는 영상을 찾아 배운 점을 정리하고자 한다.\n\n> [configparser에 대한 친절한 설명 영상 & 글 보기](https://kishstats.com/python/2018/03/07/python-config-parser.html)\n\n## configparser 활용하기\n\n### 1. Config 파일 쓰기\n\n가장 먼저 할 일은 `ConfigParser` 객체를 생성하는 것이다.\n```python\nfrom configparser import ConfigParser\n\nconfig = ConfigParser()\n```\n\n다음과 같이 `section`과 `option`을 설정할 수 있다.\n- config['option_name'] = { 'option_name': value }\n\n```python\n# section for settings\nconfig['settings'] = {\n    # config options of settings section.\n    'debug': 'true',\n    'secret_key': 'abc123',\n    'log_path': '/my_app/log'\n}\n\n# section for database\nconfig['db'] = {\n    'db_name': 'myapp_dev',\n    'db_host': 'localhost',\n    'db_port': '8889'\n}\n\n# section for files\nconfig['files'] = {\n    'use_cdn': 'false',\n    'images_path': '/my_app/images'\n}\n```\n\nconfiguration 객체에 내용 작성을 끝내면 파일 객체를 생성해 파일 형태로 저장한다.\n```python\n# write a config file \nwith open('./dev.ini', 'w') as f:\n      config.write(f)\n```\n\n### 2. Config 파일 읽기\n\n마찬가지로 객체를 생성한 후, 저장한 Config 파일을 읽어들인다.\n- parser.read('file_name')\n\n```python\nfrom configparser import ConfigParser\n\nparser = ConfigParser()\nparser.read('dev.ini')\n```\n\n필요한 섹션과 파일을 다음과 같이 읽을 수 있다.\n- parser.sections()\n- parser.options('section_name')\n- parser.get('section_name', 'option_name')\n\n```python\nprint(parser.sections())  # ['settings', 'db', 'files']\nprint(parser.options('settings'))  # ['debug', 'secret_key', 'log_path']\nprint(parser.get('settings', 'secret_key'))  # abc123\nprint('db' in parser)  # True\n```\n\n특정 자료값으로 바로 읽어들일 수 있는 함수가 있다.\n- parser.getint()\n- parser.getfloat()\n- parser.getboolean()\n\n이 함수들의 인자로 `fallback`을 설정할 수 있는데, 읽어들이려는 값이 parser 내에 존재하지 않는 경우 default 값으로 반환하는 값이다. 이름처럼 Python `dict`에서 `get(key, (default_value))` 함수를 쓰는 것과 비슷하다. (거의 같다.)\n\n```python\nprint(parser.get('db', 'db_port'), type(parser.get('db', 'db_port')))  # 8889 <class 'str'>\nprint(int(parser.get('db', 'db_port')))  # 8889 (as int)\nprint(parser.getint('db', 'db_default_port')) # 8889 (as int)\nprint(parser.getint('db', 'db_default_port', fallback=3306))  # 3306\n```\n\n## 3. String Interpolation 사용하기\n\n예를 들어서 config를 설정하는 도중에 다른 section의 option의 값을 불러와서 문자열 안에서 활용하고 싶다면 `ConfigParser()`가 문자열을 해석하도록 인자를 전달해야 한다. 예를 들어, 다음과 같이 `settings` section에 있는 option값을 `files` section에서 이용하고자 한다. \n\n- '${section_name : option_name}other_strings'\n\n```python\nfrom configparser import ConfigParser\n\nconfig = ConfigParser()\n\nconfig['settings'] = {\n    'debug': 'true',\n    'secret_key': 'abc123',\n    'log_path': '/my_app/log',\n    'python_version': '3',\n    'packages_path': '/usr/local'\n}\n\n# ...\n\nconfig['files'] = {\n    'use_cdn': 'false',\n    'images_path': '/my_app/images',\n    'python_path': '${settings:packages_path}/bin/python${settings:python_version}'\n}\n```\n\n그러면 Config 파일을 읽을 때 문자열을 해석할 수 있도록 `ConfigParser`에 인자 `ExtendedInterpolation()`을 전달한다. (method call이 되어야 한다.)\n\n```python\nfrom configparser import ConfigParser, ExtendedInterpolation\n\nparser = ConfigParser(interpolation=ExtendedInterpolation())\nparser.read('dev.ini')\n\nprint(parser.get('files', 'python_path'))  # /usr/local/bin/python3\n```\n\n> [More about Interpolation on Python docs](https://docs.python.org/ko/3/library/configparser.html#interpolation-of-values)\n\n\n## 나가며\n`ConfigParser`에서 `section`이나 `option`을 읽어들이는 것은 Pandas로 `DataFrame`의 `index`와 `column`을 읽어들이는 것과 비슷했다. 새로운 내용을 이해하는데에 기존의 지식이 도움이 되고 있다. 이 기능은 특히 앱 개발에서 설정값을 모아두기 위해 유용하게 쓰일 것 같다. \n\n사실 parser는 여러가지가 있는데: `parser`, `argparse`, `configparser` 등이 있다. 다음 기회에는 `argparse`에 대해 알아보고, script로 모델을 학습하는 방법을 배우고자 한다.\n\n## 참고 자료\n1. KishStats blog, https://kishstats.com/python/2018/03/07/python-config-parser.html\n2. Python docs, https://docs.python.org/ko/3/library/configparser.html#interpolation-of-values","excerpt":"들어가며 머신 러닝 프로젝트를 수행할 때 글로벌 상수(constants)를 정의해야 할 일이 생긴다. 예를 들면 시퀀스 데이터의 경우 최대 길이 를 설정하고 데이터를 조건에 따라 설정해둔 최대 길이로 자른다(truncate)던지 vocabulary …","fields":{"slug":"/parser_1/"},"frontmatter":{"date":"Apr 11, 2022","title":"configparser","tags":["Python","Machine Learning"],"update":"Jan 01, 0001"}}},{"node":{"rawMarkdownBody":"\n### 문제 링크 ♟\n> [LeetCode, 51. N-Queens I](https://leetcode.com/problems/n-queens/)\n\n> [LeetCode, 52. N-Queens II](https://leetcode.com/problems/n-queens-ii/)\n\n\n## 문제 해결 아이디어\n\n체스 보드는 `8 x 8` 크기이므로 본래는 8-Queen 문제였으나, 이 문제들은 `8`을 `n`으로 바꾸어 문제를 한단계 더 추상화했다.\n\n51번은 `n x n` 보드에 `n`개의 체스 퀸을 위치하는 모든 방법을 출력하는 문제이고, 52번은 같은 설정에서 퀸을 위치하는 방법의 개수를 출력하는 문제이다. 즉, 51번에서 행렬을 직접 출력해야 한다는 점을 빼면 완전히 같은 문제이다.\n\n이 글에서는 **backtracking**으로 n-퀸 문제를 해결하는 방법을 소개하고자 한다. \n\n 이 문제를 해결하기 위해서는 퀸들이 위치할 수 있는 모든 지점을 탐색해야 한다. 따라서 체스 보드 위에 행이나 열을 선택해서 첫번째 행이나 열부터 퀸을 위치시키고 (같은 행이나 열에 퀸이 위치해서는 안되므로) 다음 행이나 열에 `0`부터 `n-1` 위치 까지 하나씩 퀸을 위치 시킬 수 있는지 탐색한다. 만약 특정 위치에 퀸이 위치 할 수 없다면 이전 행이나 열로 돌아가 그 다음 위치를 탐색할 수 있다. 이것이 백 트래킹으로, 백 트래킹이란 해를 탐색하다가 특정 경우에서 해가 아닌 것을 확인할 경우 이전의 상태로 돌아가는 방법을 말한다. 즉 완전 탐색과 유사하지만, 해가 아닌 것을 확인할 경우 바로 탐색을 종료하고 다음 경우의 수로 넘어가는 방법이다.\n\n재귀함수로 구현하면 `0`부터 `n-1`까지 경우를 탐색해 재귀 함수를 실행한 후 해가 아닌 경우 함수를 종료하기만 하면 백 트래킹을 구현할 수 있다. 만약 퀸이 특정 위치에 있을 수 있다면 다음 행이나 열 위치에 대해서 재귀 함수를 실행하면 된다. 이 방법으로는 첫번째 행 또는 열에 대해 백 트래킹을 실행하는 것으로 `n`번째 행 또는 열까지 퀸을 놓는 경우를 모두 탐색할 수 있다. \n\n`path`는 초기화된 `n` 길이의 배열로 퀸의 위치를 저장하는 배열이다. 퀸은 같은 행이나 열에 위치할 수 없으므로 `path[x] = y`라고 할 때 체스 보드 위의 (x, y)지점에 퀸이 있다는 것을 표현할 수 있다. 이때 `path` 내의 모든 x와 y는 중복되지 않을 것이다.\n\n과정을 요약하면:\n- `path` 배열을 초기화 한다. `path`는 배열 인덱스 `i`를 행으로 하는 퀸의 열 위치를 원소값으로 한다.\n- backtrack을 구현한다.\n    - 배열 `path`와 `nx`가 주어졌을 때, 보드의 `(nx, ny)` 위치에 퀸을 위치시켜도 될지를 탐색해서, 만약 가능하다면:\n    - 보드가 `n`개 퀸으로 채워졌으면 카운트를 증가시키거나 배열을 결과값에 추가하고,\n    - 아직 보드가 채워지지 않았으면 `path[nx]`에 `ny`값을 저장하고 `nx+1` 값에 대해 backtrack을 재귀로 구현한다.\n* tip: `path`가 주어졌을 때 행렬을 출력하는 함수를 구현해서 backtrack에서 부르면 편리하다.\n\n\n## 파이썬 구현 코드\n### 51.\n```python\nclass Solution:\n    def solveNQueens(self, n: int) -> List[List[str]]:\n        \n        # implement backtrack\n        def backtrack(path, nx):\n            # search for (nx, ny) positions to place Queen\n            for ny in range(n):\n                if valid(path, nx, ny):\n                    path[nx] = ny\n                    # if the board is completed,\n                    # use path to return a matrix\n                    if nx == n-1:\n                        ret.append(return_val(path))\n                    else:\n                        backtrack(path, nx+1)\n        \n        # return True if placing queen at board[nx][ny] is valid\n        def valid(path, nx, ny):\n            for i in range(nx):\n                x, y = i, path[i]\n                # if \n                if y == ny or abs(x - nx) == abs(y - ny):\n                    return False\n            return True\n        \n        # given path, return the board in matrix form\n        def return_val(path):\n            board = [['.'] * n for _ in range(n)]\n            for y in range(n):\n                board[path[y]][y] = 'Q'\n            board = [''.join(l) for l in board]\n            return board\n        \n        ret = []\n        path = [-1] * n  # a queen is on board[x][path[x]]\n        backtrack(path, 0)\n        \n        return ret\n```\n\n### 52.\n```python\nclass Solution:\n    def totalNQueens(self, n: int) -> int:\n        \n        # implement backtrack\n        def backtrack(path, nx):\n            # search for (nx, ny) positions to place Queen\n            for ny in range(n):\n                if valid(path, nx, ny):\n                    path[nx] = ny\n                    # if the board is completed,\n                    # increase count by 1\n                    if nx == n-1: \n                        self.cnt += 1\n                    else:\n                        backtrack(path, nx+1)\n\n        # return True if placing queen at board[nx][ny] is valid\n        def valid(path, nx, ny):\n            for i in range(nx):\n                x, y = i, path[i]\n                # if another queen is at the same line or diagonal, \n                # return False\n                if y == ny or abs(x - nx) == abs(y - ny):\n                    return False\n            return True\n        \n        self.cnt = 0\n        path = [-1] * n  # a queen is on board[x][path[x]]\n        backtrack(path, 0)\n        \n        return self.cnt\n```","excerpt":"문제 링크 ♟ LeetCode, 51. N-Queens I LeetCode, 52. N-Queens II 문제 해결 아이디어 체스 보드는  크기이므로 본래는 8-Queen 문제였으나, 이 문제들은 을 으로 바꾸어 문제를 한단계 더 추상화했다. 51번…","fields":{"slug":"/leetcode_n-queen/"},"frontmatter":{"date":"Apr 07, 2022","title":"LeetCode(51, 52). N-Queens I, II","tags":["Algorithms","LeetCode","Implementation"],"update":"Jan 01, 0001"}}},{"node":{"rawMarkdownBody":"\n### 문제 링크\n> [LeetCode, 54. Spiral Matrix I](https://leetcode.com/problems/spiral-matrix/)\n\n> [LeetCode, 59. Spiral Matrix II](https://leetcode.com/problems/spiral-matrix-ii/)\n\n\n## 문제 해결 아이디어\n\n이 문제는 달팽이 문제라고도 하며, 알고리즘을 배우기 시작할 때 몇 번 마주쳤던 구현 문제다. 꼭 달팽이를 시계방향으로 탐색하는 것이 아니더라도 반시계 방향으로 탐색하거나 만다라 모양 등으로 탐색하도록 문제를 변형할 수 있다.\n\n이 글에서 소개하는 두 문제는 거의 같은 방법으로 해결할 수 있다. 54번 문제는 행렬을 읽어들이는 문제고, 59번 문제는 주어진 조건에 맞는 행렬을 만들어서 출력하는 문제다. 행렬을 읽는지 쓰는지에 관계없이 행렬의 원소에 같은 순서로 접근하는 것으로 코드의 많은 부분을 재사용 할 수 있다.\n\n특정 규칙에 따라 방향을 바꾸기 위해 **방향 벡터** `dv`를 설정해 문제에 접근했다. 방향 벡터에는 방향을 틀 때 **업데이트 할 인덱스 변화값**을 저장한다. 여기서는 행렬의 끝에 다다르거나 이미 읽거나 쓴 값에 접근할 경우 행렬의 인덱스를 업데이트해서 (시계 방향으로 돌 수 있도록)**오른쪽 방향으로 90도 회전**했다.\n\n요약하면:\n- 행렬의 끝에 다다르지 않고, 이미 읽거나 쓴 값이 아닌경우에 한해 다음을 반복한다.\n- 지금 인덱스의 행렬값을 읽거나 쓴다.\n- 인덱스를 업데이트하고 읽거나 쓴 표시를 남긴다. 이때, 업데이트 할 인덱스 값이 범위를 넘었거나 이미 읽거나 쓴 값을 가리키는 경우 방향을 튼 인덱스 값으로 업데이트한다.\n\n\n## 파이썬 구현 코드\n\nLeetCode에서는 Python으로 문제 답안을 Solution class 형태로 제출해야 한다. LeetCode의 좋은 점은 제한 시간이 명시되어 있고 답안 제출 시에도 running time과 memory usage를 출력하고 다른 사람의 답에 비해 얼마나 빠르거나 느린지 알려준다는 점이다. \n\n### 54.\n```python\nclass Solution:\n    def spiralOrder(self, matrix: List[List[int]]) -> List[int]:\n        \n        ret = []\n        INF = 1e9\n        \n        x, y, k = 0, 0, 0\n        m, n = len(matrix), len(matrix[0])\n        dv = [(0, 1), (1, 0), (0, -1), (-1, 0)]\n        \n        while -1 < x < m and -1 < y < n and matrix[x][y] != INF:\n            \n            # read the element value\n            ret.append(matrix[x][y])\n\n            # update values\n            matrix[x][y] = INF\n            nx = x + dv[k][0]\n            ny = y + dv[k][1]\n            \n            # if nx or ny is out of boundary, update k\n            if nx in [-1, m] or ny in [-1, n] or matrix[nx][ny] == INF:\n                k = (k+1) % 4\n                x = x + dv[k][0]\n                y = y + dv[k][1]\n            else:\n                x, y = nx, ny\n            \n        return ret\n```\n\n### 59.\n```python\nclass Solution:\n    def generateMatrix(self, n: int) -> List[List[int]]:\n        \n        # initiate a matrix grid\n        grid = [[0 for _ in range(n)] for _ in range(n)]\n        \n        num = 1\n        x, y, k = 0, 0, 0\n        dv = [(0, 1), (1, 0), (0, -1), (-1, 0)]\n        \n        while -1 < x < n and -1 < y < n and grid[x][y] == 0:\n            \n            # write number on the grid\n            grid[x][y] = num\n            \n            # update values\n            num += 1\n            nx = x + dv[k][0]\n            ny = y + dv[k][1]\n            \n            # if nx or ny is out of boundary, update k\n            if nx in [-1, n] or ny in [-1, n] or grid[nx][ny] != 0:\n                k = (k+1) % 4\n                x = x + dv[k][0]\n                y = y + dv[k][1]\n            else:\n                x, y = nx, ny\n                \n        return grid\n            \n```","excerpt":"문제 링크 LeetCode, 54. Spiral Matrix I LeetCode, 59. Spiral Matrix II 문제 해결 아이디어 이 문제는 달팽이 문제라고도 하며, 알고리즘을 배우기 시작할 때 몇 번 마주쳤던 구현 문제다. 꼭 달팽이를 시…","fields":{"slug":"/leetcode_spiral_matrix/"},"frontmatter":{"date":"Apr 07, 2022","title":"LeetCode(54, 59). Spiral Matrix I, II","tags":["Algorithms","LeetCode","Implementation"],"update":"Jan 01, 0001"}}},{"node":{"rawMarkdownBody":"\n\n원 논문:\n> Vaswani et al., 2017, \"Attention is all you need\" ([Link to arxiv](https://arxiv.org/abs/1706.03762))\n\n## 0. Transformer\n\n 트랜스포머는 Seq2Seq 모델과 비슷한 인코더-디코더 구조를 갖고 있지만, 보다 긴 시퀀스를 효율적으로 다룰수 있는 모델로 환영받았다. 트랜스포머는 새로운 어텐션을 도입했다. 2014년에 등장한 어텐션(Bahdanau et al., 2014)이 RNN 네트워크의 성능을 향상시키는 활용된 것과 달리, 2017년의 어텐션은 신경망을 이용하지 않고 행렬 곱으로 이루어진 방식을 제안하면서 자연어 처리에 있어 획기적인 성능 향상을 불러왔다. \n\n![](transformer.png)\n<center>\n트랜스포머 모델의 구조, Vaswani et al., 2017\n</center>\n\n</br>\n\n트랜스포머 모델은 크게 봐서 왼쪽의 인코더와 오른쪽의 디코더를 가지는 구조이다. RNN 모델에서와 마찬가지로 인코더의 결과값을 디코더의 입력값과 결합해서 연산을 거쳐 결과값을 도출한다. 다만 RNN과의 차이점은 시퀀스를 **동시에** 처리한다는 점이다. 즉 시퀀스 모델처럼 시퀀스 데이터에 순서대로 접근해서 하나씩 처리하는 것이 아니라 전체 시퀀스를 한 번에 연산한다. 따라서 분산 연산이 용이하며 RNN 모델에서 발생하는 vanishing gradient 현상에 효과적으로 대처해서 보다 긴 시퀀스를 다룰 수 있게 되었다.\n\n트랜스포머 네트워크에 사용되는 어텐션은 행렬곱을 이용한 순차적이지 않은 (따라서 병렬 연산이 가능한) 어텐션이며, 이 어텐션을 Scaled-Dot product 어텐션이라고 한다.  \n\n### 🔔 Scaled-Dot prooduct Attention\n\nScaled-Dot prooduct 어텐션에는 세개의 행렬 $Q$, $K$, $V$ 입력이 필요하다:\n\n- $Q$, Queries : 비교하고자 하는 시퀀스로, 키 K와 유사도를 측정한다.\n- $K$, Keys : 비교 대상이 되는 시퀀스로, 쿼리 Q와 유사도를 측정한다.\n- $V$, Values : $QK$의 가중치가 곱해지는 행렬이다.\n\n어텐션이란 행렬 $Q$와 $K$-$V$ 쌍을 결과값에 매핑하는 함수로, 결과값은 $V$의 가중치 합으로 계산되고, 각각의 값 $v$에 할당된 가중치는 $q$와 그에 대응하는 $k$로부터 계산된다. 이 어텐션이 이름 붙은 이유는 어텐션을 연산하는 방법에 있다.\n\n$$\nAttention(Q, K, V) = softmax(\\frac{Q \\cdot K^T}{\\sqrt{d_k}}) \\cdot V\n$$\n\n첫 번째 행렬 곱(dot product)은 쿼리 Q와 키 K의 유사도를 분석하고, 두번째 곱은 softmax 함수값으로 얻어진 가중치를 발류 $V$에 할당한다. $Q$와 $K$의 곱은 $K$의 차원 $d_k$의 루트로 조정(scaled) 하는데, 이 스케일링은 reguralization효과를 가진다. 그 다음 softmax 함수를 거쳐 $Q$와 $K$의 유사도를 가중치로 변환해 마지막으로 그 값을 행렬 $V$에 반영해 모든 쿼리의 어텐션을 얻을 수 있다. \n\n![](attention.png)\n<center>\n어텐션, Badnau et al., 2015\n</center>\n\n</br>\n\n어텐션은 쿼리와 키의 heatmap을 생성하는 것과 같다. 전체 시퀀스를 한 번에 단어 쌍 단위 $(q, k)$로 살펴보기 때문에, 단어가 문장에서 등장하는 순서에 관계없이 유사도를 계산할 수 있다. 위의 예처럼 어순이 다른 영어와 프랑스어 문장에서도 단어의 대응 관계를 제대로 파악할 수 있다.\n\n### 🔔 Multi-Head Attention\n\n![](multi-head-attention.png)\n<center>\nMulti-Head Attention, from deeplearning.ai\n</center>\n\n</br>\n\n트랜스포머의 어텐션의 또다른 특징은 어텐션이 머리(heads)를 여러개 가진다는 것이다. 입력값에 대해 어텐션을 여러번 수행한 후 결과를 결합(concatenate)해서 최종적으로 하나의 행렬을 만드는 것을 뜻한다. $d_{model}$ 차원의 $Q$, $K$, $V$에 대해 일회 어텐션을 적용하는 대신 $Q$, $K$, $V$를 `n_heads`번 선형 투사(linear projection)해서 서로다른 학습된 선형 투사들에 대해 어텐션을 적용한다. 각 선형 투사를 $Q$에 대해 $W^Q$, $K$에 대해 $W^K$, $V$에 대해 $W^V$라고 하며 각각 $(n_{seq}, d_k)$, $(n_{seq}, d_k)$, $(n_{seq}, d_v)$ 차원을 가진다. 어텐션을 적용하면 한개의 결과값에 대해 $(n_{seq}, d_v)$ 차원을 가지므로 `n_heads`개의 결과값을 결합한 행렬 다시 선형 투사해서 어텐션 결과값을 얻는다. 매번 학습할 때마다 다른 가중치를 가지므로 여러번 어텐션을 적용함으로서 시퀀스 데이터 사이의 다양한 관계를 학습할 수 있다. 이때 여러번의 어텐션을 순서대로 실행해야 할 필요가 없으므로 병렬 연산을 하기 용이한 구조다. \n\n📂 다음 코드는 Trax `SplitIntoHeads`와 `MergeHeads` 메서드로, Multi-Head 어텐션의 일부분을 구현한다. \n\n```python\ndef SplitIntoHeads(n_heads, merged_batch_and_head=True):\n  \"\"\"Returns a layer that reshapes an array for multi-head computation.\"\"\"\n  def f(x):\n    batch_size, seq_len, d_feature = x.shape\n    if d_feature % n_heads != 0:\n      raise ValueError(\n          f'Feature embedding dimensionality ({d_feature}) is not a multiple'\n          f' of the requested number of attention heads ({n_heads}).')\n\n    d_head = d_feature // n_heads\n\n    # (b_size, seq_len, d_feature) --> (b_size*n_heads, seq_len, d_head)\n    x = x.reshape((batch_size, seq_len, n_heads, d_head))\n    x = x.transpose((0, 2, 1, 3))\n    if merged_batch_and_head:\n      x = x.reshape((batch_size * n_heads, seq_len, d_head))\n    return x\n  return Fn('SplitIntoHeads', f)\n```\n\n`SplitIntoHeads`는 `d_feature`(그림에서는 `d_model`)가 Head의 개수 `n_heads`의 정수배일 것을 강제한다. 위의 그림처럼 `(batch, seq_len, d_feature)`을 입력받아 `(b_size*n_heads, seq_len, d_head)`차원을 출력하고 있다. 그 다음 `PureAttention`처럼 1개 Head에 대한 어텐션을 `n_heads`만큼 수행한 후, 여러 Head를 `MergeHeads`로 결합한다.\n\n```python\ndef MergeHeads(n_heads, merged_batch_and_head=True):\n  \"\"\"Returns a layer that rejoins heads, after multi-head computation.\"\"\"\n  def f(x):\n    if merged_batch_and_head:\n      dim_0, seq_len, d_head = x.shape\n      if dim_0 % n_heads != 0:\n        raise ValueError(\n            f\"Array's leading dimension ({dim_0}) is not a multiple of the\"\n            f\" number of attention heads ({n_heads}).\")\n\n      batch_size = dim_0 // n_heads\n      x = x.reshape((batch_size, n_heads, seq_len, d_head))\n    else:\n      batch_size, _, seq_len, d_head = x.shape\n\n    # (b_size, n_heads, seq_len, d_head) --> (b_size, seq_len, d_feature)\n    x = x.transpose((0, 2, 1, 3))\n    x = x.reshape((batch_size, seq_len, n_heads * d_head))\n    return x\n  return Fn('MergeHeads', f)\n```\n\n함수 `f`는 `(batch_size, n_heads, seq_len, d_head)` 차원의 입력값을 어텐션 블럭의 입력값 차원 `(b_size, seq_len, d_feature)`로 변형해서 반환한다. \n\n\n## 1. Encoder\n\n널리 쓰이는 트랜스포머 사전훈련 모델인 BERT의 경우 인코더만 (`bert-large`의  경우) 24개 쌓은 네트워크다. LSTM을 생각해 보면 인코더의 구조는 상대적으로 단순해 보이지만, 인코더만으로도 많은 작업을 수행할 수 있음을 알 수 있다.\n\n![](transformer_encoder.png)\n<center>\n인코더 구조, from deeplearning.ai\n</center>\n\n</br>\n\n인코더는 크게 **두개의 레이어**로 구성된다. Multi-Head Attention과 FeedForward이다. FeedForward는 일련의 훈련 가능한 신경망으로 구성된 블럭이다. 0️⃣ 입력값을 Embedding하고 Positional Encoding을 적용한 후에, 한 개의 **인코더 블럭**은 1️⃣ Residual을 적용한 Multi-Head Attention을 실행하고 2️⃣ 다시 Residual을 적용한 FeedForward 레이어를 실행한다. 모델을 깊게 만들기 위해 이 인코더 블럭을 여러번 실행한다.\n\nResidual 레이어는 함수 $Fn(x_1, x_2, ...)$에 대해 다음을 뜻한다.\n\n$$\nResidual(Fn)(x_1, x_2, ...) = Fn(x_1, x_2, ...) + x_1\n$$\n\n즉 입력값의 첫번째 값을 함수의 결과값에 더하는 기능을 한다. Residual 레이어를 적용하는 이유는 shortcut 연결 기능을 하기 때문이다. 특히 깊은 네트워크를 학습할 때 Residual이 효과적임이 검증되었다.\n\n> Further study - Residual 네트워크를 사용하는 이유 : [He et al., 2015](https://arxiv.org/pdf/1512.03385.pdf)\n\n📂 다음 코드는 Trax 라이브러리의 `TransformerEncoder` 모델이다.\n\n```python\ndef TransformerEncoder(vocab_size,\n                       n_classes=10,\n                       d_model=D_MODEL,\n                       d_ff=D_FF,\n                       n_layers=N_LAYERS,\n                       n_heads=N_HEADS,\n                       max_len=MAX_SEQUENCE_LENGTH,\n                       dropout=DROPOUT_RATE,\n                       dropout_shared_axes=DROPOUT_SHARED_AXES,\n                       mode=MODE,\n                       ff_activation=FF_ACTIVATION_TYPE):\n    \n  def _Dropout():\n    return tl.Dropout(rate=dropout, shared_axes=dropout_shared_axes, mode=mode)\n\n  def _EncBlock():\n    return _EncoderBlock(d_model, d_ff, n_heads, dropout, dropout_shared_axes,\n                         mode, ff_activation)\n\n  return tl.Serial(\n      tl.Branch([], tl.PaddingMask()),  # Creates masks from copy of the tokens.\n      tl.Embedding(vocab_size, d_model),\n      _Dropout(),\n      tl.PositionalEncoding(max_len=max_len),\n      [_EncBlock() for _ in range(n_layers)],\n      tl.Select([0], n_in=2),  # Drops the masks.\n      tl.LayerNorm(),\n      tl.Mean(axis=1),\n      tl.Dense(n_classes),\n  )\n```\n\n`TransformerEncoder`는 토큰화된 텍스트를 `n_classes`개로 분류한다. 함수 반환값의 첫줄에 등장하는 `tl.Branch`는 입력값을 받아서 각각의 함수를 병렬적으로 실행한다. 즉 입력값을 리스트`[]`로 만든 값과 패딩마스크 `tl.PaddingMask()` 값 두개를 반환할 것이다. 두 값 모두 임베딩과 positional encoding을 거쳐 인코더 블럭에 입력된다. 인코더 블럭의 코드에서 데이터와 마스크 쌍 `(activations, mask)`을 입력받는 것을 확인할 수 있다.\n\n```python\ndef _EncoderBlock(d_model,\n                  d_ff,\n                  n_heads,\n                  dropout,\n                  dropout_shared_axes,\n                  mode,\n                  ff_activation):\n  \"\"\"Returns a list of layers that implements a Transformer encoder block.\n  The input to the block is a pair (activations, mask) where the mask was\n  created from the original source tokens to prevent attending to the padding\n  part of the input. The block's outputs are the same type/shape as its inputs,\n  so that multiple blocks can be chained together.\n  \"\"\"\n  def _Attention():\n    return tl.Attention(d_model, n_heads=n_heads, dropout=dropout, mode=mode)\n\n  # ...\n\n  return [\n      tl.Residual(\n          tl.LayerNorm(),\n          _Attention(),\n          _Dropout(),\n      ),\n      tl.Residual(\n          tl.LayerNorm(),\n          _FFBlock(),\n          _Dropout(),\n      ),\n  ]\n```\n\n여기서 `tl.Attention`은 `n_heads`개의 머리를 가지는 multi-head 셀프-어텐션이며, Attention 레이어와 FeedForward 블럭이 Residual을 거치는 것을 확인할 수 있다. 어텐션 블럭을 지난 후에는 마스크가 필요 없으므로 `tl.Select()`로 `(activations, mask)`쌍에서 앞의 값만 취하고 `tl.LayerNorm()`과 같은 쿼리에 해당하는 열에 대한 덧셈 `tl.Mean(axis=1)`, 그리고 `n_classes`개의 `tl.Dense()` 층을 거쳐 마무리한다.\n\n### 🔆 Dimensionality Setting\n\nResidual 레이어를 적용하기 위해서는 어텐션의 입력값과 결과값의 차원이 같아야한다. 앞의 Multi-Head Attention 그림을 참고하여 배치 크기를 `batch`, 입력 시퀀스 길이를 `length`, 그리고 어텐션의 차원을 `d_model`로 설정하자. Q, K, V가 $(batch, length, d_{model})$ 차원을 가지며, $W^Q$, $W^K$, $W^V$는 각각 $(batch, length, d_k)$, $(batch, length, d_k)$, $(batch, length, d_v)$ 차원이라고 하자. 위의 설정에서 $d_k = d_v$로 두며, 그림에서는 이 값이 $d_{head}$로 나타나 있다. 어텐션을 수행하고 난 후 i번 째 어텐션은 $Z_i \\in (batch, length, d_v)$ 차원이 되는데, $n_{heads}$개의 어텐션을 결합하고 난 후 처음 입력값과 같은 차원을 얻기 위해 $ n_{heads} = d_{model} / d_v$로 설정한다. 요약하면:\n\n$$\nd_k = d_v = d_{model} / n_{heads}\n$$\n\n\n### 🔆 Positional Encoding\n\n인코딩에 앞서, 단순히 단어 임베딩을 통해 $QK^T$ 2차원 행렬을 계산하면 시퀀스 모델과 달리 단어의 문장 내 위치 정보를 반영할 수 없다. 그러나 어순은 맥락을 파악하는데에 중요한 단서가 된다. 예를 들어 문장 내에서 같은 단어가 등장해도 각 단어는 다른 의미를 가리킬 수 있고, 언어마다 문법 구조에 따라 어순이 다르며, 같은 단어를 사용해도 어순에 따라 다른 의미를 내포할 수 있다.\n\n따라서 위치 정보를 반영하기 위해 위치에 따른 임의의 값을 설정해 Q, K와 V의 임베딩에 **더하는**데, 이 것을 positional encoding이라고 한다. Trax에서는 여러가지 positional encoding 방법을 지원하고 있는데 ([link](https://trax-ml.readthedocs.io/en/latest/trax.layers.html?highlight=positional%20encoding#module-trax.layers.research.position_encodings)), 원 논문에서는 차원 `i`와 위치 `pos`에 대한 sine 곡선으로 표현했다.\n\n$$\n\\begin{aligned}\nPE_{(pos,2i)} &= sin(pos/10000^{2i/d_{model}}) \\\\\nPE_{(pos,2i+1)} &= cos(pos/10000^{2i/d_{model}})\n\\end{aligned}\n$$\n\n즉 positional encoding의 각 차원은 사인 곡선에 대응한다. PE는 $2\\pi$에서 부터 $10000 \\cdot 2\\pi$까지의 기하학적 형태를 나타낸다. 이렇게 함으로서 상수 $k$에 대해 상대적인 위치인 $PE(pos+k)$를 $PE(pos)$의 선형 함수로 나타낼 수 있다. trax.layers.Attention에서 정의하고 있는 `PositionalEncoding` 도 같은 방법을 적용했다.\n\n📂 다음 코드는 Trax 라이브러리의 `PositionalEncoding` 레이어다.\n\n```python\nclass PositionalEncoding(base.Layer):\n  \n  # ...\n\n  def forward(self, inputs):\n    \"\"\"Returns the input activations, with added positional information.\"\"\"\n    weights = self.weights\n\n    # ...\n\n    emb = fastmath.dynamic_slice_in_dim(\n        weights, self.state, inputs.shape[1], axis=1)\n    self.state += inputs.shape[1]\n    return inputs + emb\n\n  def init_weights_and_state(self, input_signature):\n    \"\"\"Randomly initializes the positional encoding vectors.\n    \"\"\"\n    d_feature = input_signature.shape[-1]\n    if self._d_feature is not None:\n      d_feature = self._d_feature\n    pe = np.zeros((self._max_len, d_feature), dtype=np.float32)\n    position = np.arange(0, self._max_len)[:, np.newaxis]\n    div_term = np.exp(\n        np.arange(0, d_feature, 2) * -(np.log(10000.0) / d_feature))\n    pe[:, 0::2] = np.sin(position * div_term)\n    pe[:, 1::2] = np.cos(position * div_term)  # [self._max_len, d_feature]\n    if self._use_bfloat16:\n      pe = pe.astype(jnp.bfloat16)\n    w = jnp.array(pe)  # Trainable parameters, initialized above.\n    # ...\n```\n\n`init_weights_and_state` 함수는 `ShapeDtype` 객체를 입력받아서 임베딩 크기 `(max_len, d_feature)` 크기의 벡터 `pe`를 초기화 한다. `pe` 벡터의 짝수 행에는 `positon * div_term`의 sine 값을 할당하고 홀수 행에는 cosine 값을 할당한다. 이 값을 `weights`로 전달해 함수`forward`에서 `emb` 값으로 입력값에 더해 전달하고 있다. 즉 사인과 코사인 값으로 위치정보를 인코딩해 입력값에 더하는 방식으로 positional encoding을 수행한다.\n\n### 📣 Encoder Self-Attention \n\n인코더는 셀프-어텐션 레이어를 활용한다. 셀프-어텐션은 주어진 데이터의 부분값과 다른 부분들의 관계를 파악하는 방법이다. 즉 문장 데이터에서 셀프-어텐션은 문장 내의 단어 문맥을 파악한다. 앞서 $Q$, $K$, $V$의 값은 어텐션에 따라 다르다고 했는데, 셀프-어텐션 레이어에서는 모든 $Q$, $K$, $V$가 같은 시퀀스, 즉 인코더의 이전 레이어의 결과값에서 온다. 주어진 문장이 있을 때, 임의의 단어 $w$에 대응하는 임베딩에 대한 가중치를 학습할 수 있다. 임베딩에 가중치를 곱해 얻은 쿼리 $q$에 대해 모든 $k \\in K$와 dot product로 비교해서 유사도 점수를 얻을 수 있다. 그 다음 softmax 함수를 통해 모든 가중치를 더해서 1이 되는 양수값으로 변환한다. 그 후 두번째 dot product로 단어 $w$에 대응하는 다른 단어들의 $v$값들을 구해 다시 학습한 가중치 행렬을 곱함으로서 모든 가중치 합을 구한다. 이것이 단어 $w$에 대한 어텐션이다.\n\n![](attention2.png)\n<center>\n셀프-어텐션, Vaswani et al., 2017\n</center>\n\n</br>\n\n위 그림은 _\"making\"_ 단어에 대한 어텐션을 표현하고 있다. 위 어텐션의 $Q$, $K$, $V$는 모두 한 문장 _\"It is in this spirit that a majority of American governments have passed new laws since 2009 making the registration or voting process more difficult. <EOS> <pad> ...\"_에서 얻어진다. 다른 색깔은 다른 head를 나타내며 색이 선명할수록 관계도가 높다. _\"making\"_과 연관된 head는 _\"making ... more difficult\"_ 구문을 완성한다.\n\n물론 효율을 위해 우리는 행렬 단위로 어텐션을 연산한다. 주어진 문장의 단어 임베딩이 임베딩 차원 emb에 대해 $(n_{seq}, emb)$ 차원이라고 하자. 우리는 $(emb, d_{model})$차원의 가중치 행렬 $W^Q$, $W^K$, 그리고 $W^V$을 학습한다. 단어에 대해 했던 것과 마찬가지로 임베딩 행렬에 가중치 행렬을 곱해서 각각 $(n_{seq}, d_{model})$ 차원의 행렬 $Q$, $K$, 그리고 $V$를 도출할 수 있다. 이후의 어텐션 연산은 scaled-dot product 어텐션에서 살펴본 것과 같다. Multi-head 셀프-어텐션을 실행한다면, $i \\in n_{heads}$에 대해 $W^Q_i$, $W^K_i$, $W^V_i$를 훈련하고 $Q_i$, $K_i$, $V_i$로 어텐션을 연산한 후, 어텐션 결과값 $Z_i$를 결합한 $Z$에 학습한 $(n_{seq}, d_v)$ 차원의 가중치 $W^O$ 행렬을 곱해 최종적으로 multi-head 어텐션을 만들 수 있다.\n\n📂 다음 코드는 Trax 라이브러리의 `Attention` 모델로, Multi-Head 셀프-어텐션을 수행한다.\n\n```python\ndef Attention(d_feature, n_heads=1, dropout=0.0, mode='train'):\n  \"\"\"Returns a layer that maps `(vectors, mask)` to `(new_vectors, mask)`.\n  This layer type represents one pass of multi-head self-attention, from vector\n  set to vector set, using masks to represent out-of-bound (e.g., padding)\n  positions. ...\n  \"\"\"\n  return cb.Serial(\n      cb.Select([0, 0, 0]),\n      AttentionQKV(d_feature, n_heads=n_heads, dropout=dropout, mode=mode),\n  )\n```\n\n디코더에서 살펴보겠지만, `AttentionQKV`는 $Q$, $K$, $V$를 다른 입력에서 가져올 수 있다. 따라서 셀프-어텐션을 구현하기 위해 `Attention`은 `Select`로 첫번째 입력값을 3개로 복제한 값을 `AttentionQKV`에 전달한다.\n\n또 인코더의 셀프-어텐션은 **Padding Mask**를 활용한다. \n\n```python\ndef PaddingMask(pad=0):\n  \"\"\"Returns a layer that maps integer sequences to padding masks.\n  The layer expects as input a batch of integer sequences. The layer output is\n  an N-D array that marks for each sequence position whether the integer (e.g.,\n  a token ID) in that position represents padding -- value ``pad`` -- versus\n  text/content -- all other values. The padding mask shape is\n  (batch_size, 1, 1, encoder_sequence_length), such that axis 1 will broadcast\n  to cover any number of attention heads and axis 2 will broadcast to cover\n  decoder sequence positions. ...\n  \"\"\"\n  def f(x):\n    if len(x.shape) != 2:\n      raise ValueError(\n          f'Input to PaddingMask must be a 2-D array with shape '\n          f'(batch_size, sequence_length); instead got shape {x.shape}.')\n    batch_size = x.shape[0]\n    sequence_length = x.shape[1]\n    content_positions = (x != pad)\n    return content_positions.reshape((batch_size, 1, 1, sequence_length))\n  return Fn(f'PaddingMask({pad})', f)\n```\n\n즉 패딩 토큰 `pad`로 설정된 값과 같은 부분을 $0$으로 바꾼다. 출력 차원은 `(batch_size, 1, 1, sequence_length)`으로, 어텐션과 차원을 맞추기 위해 1번과 2번 축을 추가한다.\n\n\n## 2. Decoder\n\n트랜스포머의 디코더는 두 가지 어텐션을 거친다. 첫 번째 어텐션은 인코더에서와 같은 셀프-어텐션이고, 두번째는 인코더-디코더 어텐션이다. 인코더만 사용해 모델을 만들 수 있었던 것처럼, 디코더만 사용해서 모델을 형성할 수도 있다. 디코더만 사용할 때에는 Multi-Head 셀프-어텐션만 사용할 수 있다.\n\n![](transformer_decoder.png)\n<center>\n디코더 구조, from deeplearning.ai\n</center>\n\n</br>\n\n인코더에서와 마찬가지로 디코더에서도 입력값 $Q$, $K$, $V$를 임베드하고 Positional Encoding 처리를 한 후에 Residual을 포함한 1️⃣ Multi-Head Attention과 2️⃣ FeedForward 레이어를 거친다. 디코더 블럭을 여러번 거친 후에 훈련 가능한 Linear 레이어와 Softmax 함수를 거치는데, 이 부분은 수행하고자 하는 과제에 따라 변경할 수 있다.\n\n📂 다음 코드는 Trax 라이브러리의 `TransformerLM`로, 디코더만 구현된 함수이다.\n\n```python\ndef TransformerLM(vocab_size,\n                  d_model=D_MODEL,\n                  d_ff=D_FF,\n                  n_layers=N_LAYERS,\n                  n_heads=N_HEADS,\n                  max_len=MAX_SEQUENCE_LENGTH,\n                  dropout=DROPOUT_RATE,\n                  dropout_shared_axes=DROPOUT_SHARED_AXES,\n                  mode=MODE,\n                  ff_activation=FF_ACTIVATION_TYPE):\n\n  # ...\n\n  def _DecBlock():\n    return _DecoderBlock(d_model, d_ff, n_heads, dropout, dropout_shared_axes,\n                         mode, ff_activation)\n\n  return tl.Serial(\n      tl.ShiftRight(mode=mode),  # Teacher Forcing\n      tl.Embedding(vocab_size, d_model),\n      _Dropout(),\n      tl.PositionalEncoding(max_len=max_len, mode=mode),\n      [_DecBlock() for _ in range(n_layers)],\n      tl.LayerNorm(),\n      tl.Dense(vocab_size),\n  )\n```\n\n`_DecoderBlock`은 다음과 같다.\n\n```python\ndef _DecoderBlock(d_model,\n                  d_ff,\n                  n_heads,\n                  dropout,\n                  dropout_shared_axes,\n                  mode,\n                  ff_activation):\n  # ...\n\n  return [\n      tl.Residual(\n          tl.LayerNorm(),\n          _CausalAttention(),\n          _Dropout(),\n      ),\n      tl.Residual(\n          tl.LayerNorm(),\n          _FFBlock(),\n          _Dropout(),\n      ),\n  ]\n```\n\n`TransformerLM` 함수는 Teacher Forcing을 거쳐 임베딩, positional encoding 처리 후 `n_layers`만큼의 디코더 블럭을 거쳐 `tl.LayerNorm()`과 `vocab_size`만큼의 `tl.Dense()` 레이어을 통과하는 구조를 가지고 있다. 결과적으로 사전에 주어진 단어들을 통해 다음에 올 단어를 vocabulary 내의 토큰으로 예측하는 언어 모델(language model)을 수행한다.\n\n\n### 🔆 Teacher Forcing\n디코더 블럭에 들어가기에 앞서, Teacher Forcing 기법을 활용해 모델의 훈련 속도를 높일 수 있다. RNN 모델인 Seq2Seq 모델은 바로 전 레이어의 예측 값을 다음 레이어의 입력값으로 사용한다. 때문에 모델 훈련 초기의 (덜 훈련된) 나쁜 예측 값이 계속해서 모델 훈련에 영향을 줄 가능성이 크다. 이런 문제를 완화하기 위해 이전 레이어의 예측값이 아닌 실제 타켓 값을 다음 레이어의 입력값으로 사용하는 것을 Teacher Forcing이라고 한다. 마치 선생님이 직접 이렇게 하라고 지도해주는 것과 같다. 훈련 초기에 타겟 값에 수렴하는 것을 돕기 때문에, 이 방법으로 모델 훈련 속도를 획기적으로 높일 수 있다.\n\n위의 코드에서 나타난 `ShiftRight` 레이어가 teacher forcing 역할을 한다. 즉 (한 시점 미래 값인) 바로 오른 쪽 값을 가져오는 것으로 학습하는 모델을 교정할 수 있다. 위의 코드에서 `mode`가 인자로 들어간 이유도, 학습 외에 예측을 수행할 때는 teacher forcing을 적용하지 않기 때문이다.\n\n그렇지만 Teacher Forcing은 모델을 훈련하는 과정에서 실제 타겟값을 노출하기 때문에 모델의 안정성, 즉 보다 일반적인 예에 대한 예측 능력이 떨어질 수 있다. 이렇게 훈련 중에 Label에 노출되는 경우를 **Exposure Bias**가 있다고 한다. 이 때문에 curriculum learning 방법에서는 FeedForward의 학습 초기에만 이전 레이어의 예측값을 타겟 값으로 대체하고 학습 후기에는 대체 하지 않는다.\n\n### 📣 Causal Self-Attention\n\n디코더의 입력값에 대한 어텐션을 실행할 때도 어텐션에서와 마찬가지로 문맥을 위해 셀프-어텐션을 실행할 수 있다. 단, 디코더의 셀프-어텐션은 **Causal Mask**가 필요하다. 앞에서와 같이 기계 번역 과제를 고려해보자. RNN은 디코더의 입력값에 순차적으로 접근해 매번 인코더의 결과값과 해당 시점의 디코더 입력값을 비교할 것이다. 그렇지만 어텐션은 모든 시점의 데이터를 한번에 볼 수 있으므로 특정 시점에서 모델의 타깃인 오른쪽 값에 대한 접근(attend)을 방지해야 한다.\n\n📂 다음 코드는 Trax의 `_causal_mask`로, 인자로 받은 `length` 길이 정방 행렬의 lower triangular 행렬을 반환한다.  \n\n```python\ndef _causal_mask(length):\n  # Not all backends define jnp.tril. However, using np.tril is inefficient\n  # in that it creates a large global constant. TODO(kitaev): try to find an\n  # alternative that works across all backends.\n  if fastmath.is_backend(fastmath.Backend.JAX):\n    return jnp.tril(jnp.ones((1, length, length), dtype=np.bool_), k=0)\n  else:\n    return np.tril(np.ones((1, length, length), dtype=np.bool_), k=0)\n```\n\n`DotProductCausalAttention` 어텐션은 1개 Head 어텐션을 구현하는 함수로 `CausalAttention`으로 구현될 수 있다. 주목해서 볼 점은 모델이 예측을 수행하지 않을 때만 Causal Mask를 활용하는 점이다.\n\n```python\nclass DotProductCausalAttention(base.Layer):\n  \"\"\"Layer that computes attention strengths by masking out the \"future\".\n  Causal attention uses masking to prevent a given sequence position from\n  attending to positions greater than / following it. This is used, for\n  example, when training autoregressive sequence models, or when decoding a\n  sequence symbol by symbol.\n  This layer performs the core per-head attention calculation. The layer\n  assumes that any splitting into attention heads precedes it, and that any\n  merging of attention heads will follow it.\n  \"\"\"\n  # ...\n\n  def forward(self, inputs):\n    \"\"\"Returns attention-computed activations.\n    Args:\n      inputs: A (queries, keys, values) tuple.\n    \"\"\"\n    q, k, v = inputs\n\n    # ...\n\n    if self._mode == 'predict':\n      self.state, mask = _fast_inference_update_state(\n          inputs, self.state,\n          mask_for_predict=mask_for_predict)\n      # ...\n    else:\n      sequence_length = q.shape[-2]\n      mask = _causal_mask(sequence_length)\n\n    activations, attn_strengths = _per_head_attention(\n        q, k, v, mask, dropout=self._dropout, mode=self._mode, rng=self.rng)\n    #...\n    return activations\n\n    # ...\n```\n\n\n### 📣 Encoder-Decoder Attention\n\n이제 디코더의 입력값과 인코더의 입력값을 분석하는 과제가 남았다. 인코더-디코더 블럭의 입력값은 `(vec_d, mask, vec_e)`로 패딩 마스크 `mask`를 이용해 패딩된 값에 대해서는 어텐션을 수행하지 않는다. \n\n📂 다음 코드는 Trax 라이브러리의 `AttentionQKV` 함수로, `Attention`이 셀프-어텐션만 수행하는 것과 달리 $Q$와 $K-V$를 다른 데이터에서 가져오는 것을 허용한다. \n\n```python\ndef AttentionQKV(d_feature, n_heads=1, dropout=0.0, mode='train',\n                 cache_KV_in_predict=False, q_sparsity=None,\n                 result_sparsity=None):\n  # ...\n\n  return cb.Serial(\n      cb.Parallel(_SparsifiableDense(q_sparsity),\n                  _CacheableDense(),\n                  _CacheableDense()),\n      _PureAttention(),\n      _SparsifiableDense(result_sparsity),\n  )\n```\n\n`cb.Parallel`은 $Q$에 해당하는 입력을 밀도 `q_sparsity`를 가지는 행렬로, 그리고 $K$와 $V$에 해당하는 입력을 `d_feature`만큼의 `Dense` 레이어로 두고, `_PureAttention`과 일종의 훈련가능한 `Dense` 레이어를 통과한다.\n\n최종적으로 Trax 라이브러리의 `_EncoderDecoderBlock` 함수를 보자.\n\n```python\ndef _EncoderDecoderBlock(d_model,\n                         d_ff,\n                         n_heads,\n                         dropout,\n                         dropout_shared_axes,\n                         mode,\n                         ff_activation):\n\n  def _Dropout():\n    return tl.Dropout(rate=dropout, shared_axes=dropout_shared_axes, mode=mode)\n\n  def _AttentionQKV():\n    return tl.AttentionQKV(d_model, n_heads=n_heads, dropout=dropout,\n                           mode=mode, cache_KV_in_predict=True)\n\n  def _CausalAttention():\n    return tl.CausalAttention(d_model, n_heads=n_heads, mode=mode)\n\n  def _FFBlock():\n    return _FeedForwardBlock(d_model, d_ff, dropout, dropout_shared_axes, mode,\n                             ff_activation)\n\n  return [                             # vec_d masks vec_e\n      tl.Residual(\n          tl.LayerNorm(),\n          _CausalAttention(),\n          _Dropout(),\n      ),\n      tl.Residual(\n          tl.LayerNorm(),\n          tl.Select([0, 2, 2, 1, 2]),  # vec_d vec_e vec_e masks vec_e\n          _AttentionQKV(),             # vec_d masks vec_e\n          _Dropout(),\n      ),\n      tl.Residual(\n          tl.LayerNorm(),\n          _FFBlock(),\n          _Dropout(),\n      ),\n  ]\n```\n\n`_EncoderDecoderBlock`은 디코더 벡터, 마스크, 인코더 벡터 쌍인 `(vec_d, masks, vec_e)`를 입력받는다. 여기서 디코더의 두가지 어텐션을 모두 실행하고 있음에 유의한다. 첫째로 `_CausalAttention`을 적용한 후 `(vec_d, vec_e, vec_e, masks, vec_e)`로 데이터를 정렬한다. 앞에서 부터 세 개 입력이 `_AttentionQKV()`레이어를 거쳐서 입력값과 같은 차원인 `(vec_d, masks, vec_e)`를 얻어 FeedForward 블럭을 거친다. 즉 인코더-디코더 어텐션에서는 디코더 벡터를 $Q$로, 인코더 벡터를 $K$와 $V$로 입력받는 것을 확인할 수 있다.\n\n\n## 4. Overall\n\n다음은 인코딩과 디코딩을 직관적으로 보여주는그림으로, 영어 문장을 프랑스어 문장으로 번역하는 예시를 보이고있다.\n\n![](https://3.bp.blogspot.com/-aZ3zvPiCoXM/WaiKQO7KRnI/AAAAAAAAB_8/7a1CYjp40nUg4lKpW7covGZJQAySxlg8QCLcBGAs/s1600/transform20fps.gif) \n*Cool gif from [Google AI blog](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html)*\n\nTransformer 모델은 인코더나 디코더만으로도 쓰임이 있지만, 기계 번역과 같은 시퀀스-투-시퀀스 과제에는 인코더와 디코더를 모두 활용해야 한다. 아래의 `Transformer` 모델은 앞서 살펴 본 인코더와 디코더 블럭을 활용해 전체 트랜스포머 모델을 반환한다.\n\n📂 다음 코드는 Trax 라이브러리의 `Transformer` 모델이다.\n\n```python\ndef Transformer(input_vocab_size,\n                output_vocab_size=None,\n                d_model=D_MODEL,\n                d_ff=D_FF,\n                n_encoder_layers=N_LAYERS,\n                n_decoder_layers=N_LAYERS,\n                n_heads=N_HEADS,\n                max_len=MAX_SEQUENCE_LENGTH,\n                dropout=DROPOUT_RATE,\n                dropout_shared_axes=DROPOUT_SHARED_AXES,\n                mode=MODE,\n                ff_activation=FF_ACTIVATION_TYPE):\n  \"\"\"Returns a full Transformer model.\n  This model is an encoder-decoder that performs tokenized string-to-string\n  (\"source\"-to-\"target\") transduction:\n  \"\"\"\n\n  # ...\n\n  def _Dropout():\n    return tl.Dropout(rate=dropout, shared_axes=dropout_shared_axes, mode=mode)\n\n  def _EncBlock():\n    return _EncoderBlock(d_model, d_ff, n_heads, dropout, dropout_shared_axes,\n                         mode, ff_activation)\n\n  def _Encoder():\n    encoder = tl.Serial(\n        in_embedder,\n        _Dropout(),\n        tl.PositionalEncoding(max_len=max_len, mode=encoder_mode),\n        [_EncBlock() for _ in range(n_encoder_layers)],\n        tl.LayerNorm(),\n    )\n    return tl.Cache(encoder) if mode == 'predict' else encoder\n\n  def _EncDecBlock():\n    return _EncoderDecoderBlock(d_model, d_ff, n_heads, dropout,\n                                dropout_shared_axes, mode, ff_activation)\n\n  # Input to model is encoder-side tokens and decoder-side tokens: tok_d, tok_e\n  # Model output is decoder-side vectors and decoder-side tokens: vec_d  tok_d\n  return tl.Serial(\n      tl.Select([0, 1, 1]),  # Copies decoder tokens for use in loss.\n\n      # Encode.\n      tl.Branch([], tl.PaddingMask()),  # tok_e masks tok_d tok_d\n      _Encoder(),\n\n      # Decode.\n      tl.Select([2, 1, 0]),  # Re-orders inputs: tok_d masks vec_e .....\n      tl.ShiftRight(mode=mode),\n      out_embedder,\n      _Dropout(),\n      tl.PositionalEncoding(max_len=max_len, mode=mode),\n      tl.Branch([], tl.EncoderDecoderMask()),  # vec_d masks ..... .....\n      [_EncDecBlock() for _ in range(n_decoder_layers)],\n      tl.LayerNorm(),\n      tl.Select([0], n_in=3),  # Drops masks and encoding vectors.\n\n      # Map vectors to match output vocab size.\n      tl.Dense(output_vocab_size),\n  )\n```\n\n`Transformer` 는 인코더에 입력되는 토큰과 디코더에 입력되는 토큰 쌍 `(tok_d, tok_e)`를 입력받아 디코더 벡터와 디코더 토큰 쌍 `(vec_d, tok_d)`를 반환한다. `tl.Branch`로 입력값과 패딩마스크를 생성한 후 `_Encoder()`로 인코딩을 실행한다. `_Encoder`는 `n_encoder_layers`개의 인코더 블럭 `_EncoderDecoderBlock`을 포함하도록 정의되어 있다. 인코딩을 거치면 데이터는 `(vec_e, masks, tok_d)`가 되며 `tl.Select`로 순서를 뒤집어 teacher forcing과 positional encoding을 실행한다. 두번째 `tl.Branch`로 `(tok_d, masks)`를 인코더-디코더 블럭에 입력하며, 이 외의 값들은 이후 `tl.Select`로 제외시키는 것을 알 수 있다. `_EncoderDecoderBlock`은 앞에서 살펴본 대로다. 이로써 `Transformer`함수를 불러오는 것만으로  트랜스포머를 구현할 수 있다.\n\n\n## 나가며\n\n여러 내용을 다루다보니 글이 길어졌다. 트랜스포머 모델은 긴 시퀀스에서 맥락을 추출하는데에 사용되는 모델로 널리 쓰이고 있지만, 짧은 시퀀스를 분석하거나 맥락을 구하기 어려운 문제에 대해서는 적합하지 않을 수 있다. 한편으로는 트랜스포머 모델을 이미지에 적용한 사례나 다양한 과제를 한번에 수행하는 T5, 더 긴 시퀀스에 대해 효율적($O(NlogN)$)으로 학습하는 리포머 모델 등 다양한 버전이 등장하고 있으므로, 트랜스포머의 기본적인 구조를 이해하는 것이 중요할 것이다. \n\n글로 정리하면서 나의 맹점에 대해 알 수 있었다. Residual이 왜 필요한지, 인코더나 디코더 블럭을 왜 여러번 실행하는지, 어텐션에서 학습하는 파라미터는 어느 부분인지 생각하지 않고 넘기다가 논문을 읽으면서 저자의 의도와 history를 조금 더 이해할 수 있었다. 또 글을 쓰면서 Trax 라이브러리의 코드를 부분적으로 들여다봤다. `Select()`나 `Residual()`, `Branch()`로 레이어를 쌓는 부분은 그림이 자연스럽게 떠오르는 개념을 코드로 표현해 직관적이라는 느낌이 들었고, 인코더나 디코더의 일부분을 따로 떼어서 사용하는 각각의 함수가 있어 사용 편의를 고려한 점이 느껴졌다. 큰 프로그램(라이브러리)의 코드이다보니 재사용되는 부분과 조금씩 차이나는 부분, 특히 메서드의 의존도를 미리 디자인해서 큰 그림을 염두에 두고 프로그램을 작성했다는 점이 느껴졌다. 결국 이론과 구현이 모두 중요하고 나름의 깊이가 있다는 것을 알 수 있었다.\n\n\n## 참고 자료\n1. Natural Language Processing with Attention Models, deeplearning.ai, Coursera, https://www.coursera.org/specializations/natural-language-processing\n2. Trax Library for Machine Learning, Github, https://github.com/google/trax\n3. Transformers, Google AI blog, https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html\n4. Vaswani et al, 2017, https://arxiv.org/abs/1706.03762\n5. Jay Alammar on Github page, https://jalammar.github.io/illustrated-transformer/\n","excerpt":"원 논문: Vaswani et al., 2017, \"Attention is all you need\" (Link to arxiv) 0. Transformer  트랜스포머는 Seq2Seq 모델과 비슷한 인코더-디코더 구조를 갖고 있지만, 보다 긴 시퀀스…","fields":{"slug":"/transformer/"},"frontmatter":{"date":"Mar 31, 2022","title":"Transformer (2017)","tags":["NLP","Attention","Transformer"],"update":"Apr 14, 2022"}}},{"node":{"rawMarkdownBody":"\n\n이 글은 deeplearning.ai의 NLP Specialization를 참고하여 나이브 베이즈 모델을 텍스트 정서 분석에 초점을 맞춰 정리한 글입니다.\n\n> [Github에서 Naive Bayes 코드 보기](https://github.com/snowith/nlp_model_practices/blob/main/naive_bayes/naive_bayes_sentiment.ipynb)\n\n\n## 0. 모델 개략\n나이브 베이즈 모델은 분류 과제를 위한 확률 모델이다. 훈련 데이터에 등장하는 모든 단어의 빈도를 세어서 각 데이터에 대한 조건부 확률의 비율을 계산하므로 분류 과제를 수행하는데 적합하다.\n\n### 나이브 베이즈 모델은\n- 훈련과 예측을 빠르게 수행할 수 있으므로 baseline 모델로 적합하다.\n- 문장에 있는 각 단어들이 독립적이라고 가정하기 때문에, 문장 내 단어들의 관계를 측정하거나 문장 내의 빈칸을 채우는 등의 과제에는 적합하지 않다.\n- 훈련 데이터 내 단어들이 등장하는 빈도에 기반하기 때문에, 훈련 데이터에 포함되지 않은 새로운 단어에 대한 예측이나 단어의 순서를 판단하는 과제에는 적합하지 않다.\n- 감정 분석, 저자 분류, 스팸 필터링, 문서 요약, 동음이의어 구분 등의 과제에 활용할 수 있다.\n\n이 글에서는 나이브 베이즈 모델로 이진 분류를 수행하는 상황을 가정하겠다. 특히 어떤 문장을 입력 받아서 문장이 긍정적인 정서를 내포하고 있으면 `1`을, 부정적인 정서를 내포하고 있으면 `0`을 반환하는 감정 분석(sentiment analysis) 과제를 수행한다.\n\n```python\ninput_s = 'This is my best day ever.'\nmodel(input_s) # 1, 긍정\n\ninput_s = 'the class was in a terrible mood...'\nmodel(input_s) # 0, 부정\n```\n_위와 같은 작업을 수행하는 모델 `model`을 얻는 것이 목적이다._\n\n\n\n## 1. 조건부 확률과 베이즈 룰\n모델을 살펴보기에 앞서 조건부 확률과 베이즈 룰에 대해 알아보자.\n\n### 🎲 조건부 확률(Conditional Probability) 이란?\n모수에서 조건 A가 만족될 확률을 $P(A)$, 조건 B가 만족될 확률을 $P(B)$라고 하자. 이때 B에 대한 A의 조건부 확률 $P(A|B)$ 는 조건 B를 만족하는 표본에서 조건 A를 만족하는 표본을 선택할 확률을 의미한다. 즉 $P(A|B)$는 조건 A와 B를 모두 만족하는 표본을 선택할 확률인 $P(A \\cap B)$에 모수에서 조건 B를 만족하는 표본을 선택할 확률 $P(B)$를 나눈 값으로 정의된다.\n\n$$\nP(A|B) =\\frac{P(A \\cap B)}{P(B)}\n$$\n\n조건부 확률은 뽑을 샘플의 범위를 표본 대신 조건으로 제한하는 효과가 있다. \n\n### 🎲 베이즈 정리란\n위의 정의로 부터 두개의 조건부 확률을 표현 할 수 있다.\n\n$$\nP(A|B) = \\frac{P(A \\cap B)}{P(B)} \\\\\nP(B|A) = \\frac{P(A \\cap B)}{P(A)} \n$$\n\n예를 들어 조건 `A`가 `20대`이고 조건 `B`가 `심장병`이라고 하자. 몇 개의 병원에서 표본 집단을 모아서 **심장병에 걸린 사람이 20대일 확률**을 조사하고자 한다. 우리는 표본 집단 데이터베이스로 부터 **심장병이 걸린 사람의 비율**과 **20대의 비율**을 알고있으며, 나아가 **20대 중에서 심장병에 걸린 사람의 비율**을 알 수 있다. 베이즈 정리는 세가지 정보로부터 심장병에 걸린 사람이 20대일 확률을 도출한다. 대수 연산을 통해 $P(A|B)$를 $P(B|A)$에 대해 다음과 같이 표현할 수 있다.\n\n$$\nP(A|B) = \\frac{P(A)}{P(B)} \\times P(B|A) ...... (*)\n$$\n\n두 조건의 조건부 확률의 관계를 나타내는 수식 (*)을 **베이즈 정리**라고 한다. 이때 $P(A)$를 **사전확률(prior)**, $P(A|B)$를 **사후확률(posterior)**, 그리고 $P(B|A)$를 **우도(likelihood)**라고 부른다.\n\n\n## 3. 조건부 빈도 세기\n텍스트가 내포하는 감정을 이진 분류하기 위해 분류 클래스를 $class \\in \\{positive, negative\\}$로 정의하자. $m$개의 단어를 포함하는 corpus에 속하는 단어 $w_i \\in corpus$에 대해 우리가 구하고자 하는 값은 $P(class|w_i)$, 즉 단어가 주어졌을 때 단어가 특정 class에 속할 확률을 구하는 것이다. 베이즈 정리를 떠올려 보면:\n\n$$\nP(class|w_i) = \\frac{P(class) \\cdot P(w_i|class)}{P(w_i)}\n$$\n\n이며 $P(w_i)$는 $w_i$에 대한 상수값이므로 확률을 계산할 때 무시할 수 있다. 우리는 문장을 입력받아 각 단어의 조건부 확률을 계산해서 나이브 가정에 따라 한 문장의 조건부 확률을 반환하고자 한다. 따라서 **나이브 베이즈** 모델의 아이디어를 수식으로 나타내면 예측하고자 하는 문장 $sentence$에 속한 모든 단어 $w_i \\in sentence$ ($i=1, .., n$)에 대해 다음과 같이 쓸 수 있다: \n\n$$\n\\hat{y} = argmax_{class} P(class) \\prod_{i=1}^{n}P(w_i|class)\n$$\n\n위 식은 최대 우도 추정(Maximum Likelihood Estimation, MLE)의 아이디어이기도 하다. 우선은 corpus에 대한 조건부 빈도인 $P(w_i|class)$를 계산해야 한다.\n\n다시 베이즈 정리에 의해, 클래스에 대한 조건부 확률을 다음과 같이 계산할 수 있다.\n\n$$\n\\begin{aligned}\nP(w | class) &= \\frac{P(w \\cap class)}{P(class)} \\\\\n&= \\frac{freq(w, class)}{N_{class}}\n\\end{aligned}\n$$\n\n위 식에서 $freq(w, class)$는 $class$에서 $w$가 나타나는 횟수로, $P(w \\cap class)$와 같다. $N_{class}$는 클래스에 포함되는 모든 단어의 빈도이다.\n\n### 🎲 Laplacian Smoothing\nLaplacian Smoothing은 조건부 확률이 0이 되는 것을 방지하기 위해 사용하는 기법이다. 위에서 본 최대 우도 추정에 따르면 모든 특성에 대해 likelihood를 곱하게 되는데, 만약 corpus에 없는 단어가 들어오면 다른 특성들에 관계없이 예측값이 0이 될 것이다. 분자값에 bias를 1 더하면 우리가 원하는 분류를 수행할 수 있다. \n\n$$\nP(w|class) = \\frac{freq(w, class) + 1}{N_{class} + V_{class}}\n$$\n\n$V_{class}$는 클래스에 등장하는 **유일한** 단어의 개수이다. 분모에는 $V_{class}$를 더함으로서 모든 단어에 대한 likelihood가 1이 넘지 않도록 설정할 수 있다:\n\n$$\n\\sum_{w}P(w|class) = \\frac{\\sum_w freq(w,class) + V_{class}}{N_{class} + V_{class}}\n$$\n\n\n\n## 3. Likelihood 계산하기\n앞에서 표현한 최대 우도 추정 방식을 조금 변형해, 이 글에서는 **Likelihood-ratio** 방법을 통해 분류 작업을 수행하고자 한다. \n\n우선 ratio란 분류 $class$에 대한 조건부 확률의 비율이다. 임의의 단어 $w_i$에 대해 $ratio(w_i)$는 다음과 같이 정의할 수 있다.\n\n$$\nratio(w_i) = \\frac{P(w_i|Pos)}{P(w_i|Neg)}\n$$\n\nLikelihood란 표본을 결합 확률로 나타낸 함수이며, 여기서는 입력 문장$s$가 임의의 $class$일 확률을 의미한다. 여기서는 Likelihood를 ratio에 대해 정의하자. 즉 모든 입력값 $w_i \\in s$에 대해 ratio를 곱한 값으로 표현한다.\n\n$$\nlikelihood(s) = \\prod^{m}_{i=1}\\frac{P(w_i|Pos)}{P(w_i|Neg)}\n$$\n\n만약 입력값의 모든 단어 $w_i$가 corpus의 긍정적인 라벨과 부정적인 라벨에서 같은 빈도로 나타났다면 likelihood 값은 `1`로 나타날 것이다. 이 결과를 긍정적이지도 부정적이지도 않은 **중립 값**이라고 볼 수 있다. 반면 분모 분자는 빈도 수이므로 likelihood는 음의 값을 가질 수 없고, 분모 $P(w_i|Neg)$가 분자 $P(w_i|Pos)$ 보다 커질 수록 0에 가까워지고 반대의 경우 양의 무한대 값에 가까워질 수 있다.\n\n### 🎲 Naive 란?\n베이즈 모델이 **naive**(순진하다)는 말은 모수의 모든 표본이 상호 독립적이고 완전하다고 가정하는 것을 뜻한다. 즉 머신 러닝 모델에서는 데이터의 모든 특성들(features)을 알 수 있고, 나아가 특성들이 서로 독립적이라고 가정하는 것을 뜻한다. 예를 들어 한 문장을 데이터 한개라고 하면, 문장에 속한 단어를 데이터의 특성들로 볼 수 있고 나이브 베이즈 모델은 이 단어들이 상호 연관(covariate) 되어있지 않다고 가정한다. \n\n단어 $w_i (i= 1, ..., n)$을 포함하는 문장이 class에 속할 확률은 결합 확률 $P(class, w_1, ..., w_n)$인데, 연쇄 법칙에 따르면:\n\n$$\n\\begin{aligned}\nP(class, w_1, ..., w_n) \n&= P(class) \\cdot P(w_1, ..., w_n) \\\\\n&= P(class) \\cdot P(w_1|class) \\cdot P(w_2, ..., w_n) \\\\\n&= P(class) \\cdot P(w_1|class) \\cdot P(w_2|class, w_1) \\cdot P(w_3, ..., w_n) \\\\\n&= ...\n\\end{aligned} \n$$\n\n이렇게 문장의 조건부 확률을 앞에 등장한 단어들과 class에 대한 조건부 확률 곱으로 나타낼 수 있다. 여기서 나이브 가정은 특성들 간의 관계를 독립적이라고 가정하므로 임의의 쌍 $i \\neq j$에 대해 $P(w_i|class) = P(w_i|class, w_j)$를 만족한다. 따라서 문장의 조건부 확률을 보다 간단하게 표현할 수 있다.\n\n$$\n\\begin{aligned}\nP(class, w_1, ..., w_n) \n&= P(class) \\cdot P(w_1|class) \\cdot ... \\cdot P(w_n|class) \\\\\n&= P(class) \\cdot \\prod_{i=1}^{n} P(w_i|class)\n\\end{aligned} \n$$\n\n현실 세계의 많은 현상이 상호 의존적임에도 불구하고, 나이브 베이즈 모델은 오랫동안 효율적이고 효과적인 모델로 활용되어왔다.\n\n> [나이브 베이즈 모델의 효과 분석 자료 보기](https://web.archive.org/web/20171210133853/http://www.research.ibm.com/people/r/rish/papers/RC22230.pdf)\n\n### 🎲 로그값으로 계산하기\n확률은 0과 1 사이의 값이므로, 확률을 여러번 곱하면 전산적으로 언더플로우의 위험이 커진다. 너무 큰 값이나 너무 작은 값을 다루는 전형적인 방법은 로그를 사용하는 것이다. 로그를 취한 log likelihood는 log ratio의 합으로 쓸 수 있다.\n\n$$\n\\begin{aligned}\nlog\\_ ratio(w_i) &= log \\frac{P(w_i|Pos)}{P(w_i|Neg)} \\\\\nlog\\_ likelihood &= \\sum^{m}_{i=1} log \\frac{P(w_i|Pos)}{P(w_i|Neg)}\n\\end{aligned}\n$$\n\n한가지 개념을 추가하자면, log ratio의 값을 lambda 함수로 표현하기도 한다. 즉 $\\lambda(w)$를 log ratio로 표현하면 likelihood를 더 간단하게 쓸수 있다.\n\n$$\n\\begin{aligned}\n\\lambda(w_i) &= log\\frac{P(w_i|Pos)}{P(w_i|Neg)} \\\\\nlog\\_ likelihood(s) &= \\sum^{n}_{i=1} \\lambda(w_i)\n\\end{aligned}\n$$\n\n로그를 취하게 되면 likelihood의 중립 값은 $1$에서 $log 1 = 0$으로 변하게 된다. $0$에서 양의 무한대에 대한 로그값은 음의 무한대에서 양의 무한대이므로 log likelihood의 값의 범위도 $0$을 중립 값으로하는 음의 무한대에서 양의 무한대 값을 반환할 것이다.\n\n### 🎲 사전확률\n예를 들어 코로나 팬데믹에 대한 트윗을 모아서 정서 분석을 수행한다면, 부정적인 트윗이 긍정적인 트윗보다 많을 것이다. 현실 데이터 corpus에서 분류 클래스가 균등하게 나눠지는 경우는 드물기 때문에, 데이터의 불균형을 보정하기 위한 절차가 필요하다. \n\n나이브 베이즈 모델에서는 사전확률이 이 역할을 수행한다. Likelihood와 마찬가지로 사전확률을 class의 비율로 정의하고 로그값을 취할 수 있다. \n\n$$\nlog\\_prior = log \\frac{P(Pos)}{P(Neg)}\n$$\n\n어떤 입력값에 대해 log likelihood가 0이라고 하면, 예측값은 log prior와 거의 같을 것이다. \n\n## 4. 나이브 베이즈 모델\n다시 나이브 베이즈 연산을 보면, 입력 문장 $s$에 대해:\n\n$$\nNB = log\\frac{P(Pos)}{P(Neg)} + \\sum^{n}_{i=1} \\lambda(w_i)\n$$\n\n위의 식으로 계산되며, \n\n$$\nNB = log\\_prior + log\\_likelihood\n$$\n\n로 정리할 수 있다. 나이브 베이즈 모델의 연산의 순서는 다음과 같이 정리할 수 있다. \n\n0. 훈련 데이터를 전처리 한다.\n1. 토큰화 된 단어의 빈도 $freq(w, class)$를 계산한다.\n2. 모든 훈련 데이터의 단어에 대해 훈련해 log prior와 log likelihood 값을 구한다.\n    - 모든 훈련 데이터의 단어에 대해 $P(w|Pos)$와 $P(w|Neg)$ 값을 구한다.\n    - 모든 훈련 데이터의 단어에 대해 $P(Pos)$와 $P(Neg)$ 값을 구한다.\n3. 훈련한 모델의 가중치로 정서를 분류한다.\n\n\n### 📂 구현하기\n\n#### 1. 토큰화 된 단어의 빈도 $freq(w, class)$를 계산한다.\n```python\ndef get_freq(dd, train_x, train_y):\n    '''\n    Get frequency dictionary from the training data.\n    input:\n        dd : a defaultdict of integer.\n        train_x : list of tokened sentences of training data.\n        train_y : list of 0 or 1 corresponding to the train_x. \n    return:\n        result : dictionary of (key, value) = (word label pair, frequency).\n    '''\n    for label, sentence in zip(train_y, train_x):\n        for word in process(sentence):\n            dd[(word, label)] += 1\n\n    return dd\n\n# count frequency dictionary from train_x and train_y.\nfreqs = get_freq(defaultdict(int), train_x, train_y)\n```\n\n#### 2. 모든 훈련 데이터의 단어에 대해 훈련해 log prior와 log likelihood 값을 구한다.\n```python\ndef train_naive_bayes(freqs, train_x, train_y):\n    '''\n    Train Naive Bayes model, that is, get prior and likelihood from the training data.\n    return:\n        log_prior : an integer. P(Pos) / P(Neg) value.\n        log_likelihood : a dictionary of (key, value) = (word, log likelihood)\n    '''\n    # log_likelihood relies on words\n    log_likelihood = {}\n    # log prior value relies on the corpus\n    log_prior = 0\n\n    # get unique words from the frequency dict\n    vocab = list(set(freq.keys()))\n    V = len(vocab)\n\n    # get N_pos and N_neg\n    N_pos = N_neg = 0\n    for pair in freqs.keys():\n        # if label is 1(> 0), the word is positive.\n        if pair[1] > 0:\n            N_pos += freqs[pair]\n        # if label is 0, the word is negative.\n        else:\n            N_neg += freqs[pair]\n\n    # get log likelihood\n    for w in vocab:\n        # get positive and negative frequency of word w.\n        freq_pos = freqs.get((w, 1), 0)\n        freq_neg = freqs.get((w, 0), 0)\n\n        # get P(w|Pos) and P(w|Neg).\n        p_w_pos = (freq_pos + 1) / (N_pos + V)\n        p_w_neg = (freq_neg + 1) / (N_neg + V)\n\n        log_likelihood[w] = np.log(p_w_pos) - np.log(p_w_neg)\n\n    # to compute log_prior,\n    # get the number of positive and negative labels\n    num_label = len(train_y)\n    num_pos = len(train_y[train_y == 1])\n    num_neg = len(train_y[train_y == 0])\n\n    # log prior = log(P(Pos)) - log(P(Neg))\n    log_prior = np.log(num_pos / num_label) - np.log(num_neg / num_label)\n\n    return log_prior, log_likelihood\n\n# get log prior and log likelihood from the training data\n# so that we can train on test data.\nlog_prior, log_likelihood = train_naive_bayes(freqs, train_x, train_y)\n```\n\n#### 3. 훈련한 모델의 가중치로 정서를 분류한다.\n```python\ndef predict_naive_bayes(s, log_prior, log_likelihood):\n    '''\n    input:\n        s : a list. Input sentence.\n        log_prior : log prior from trained naive bayes.\n        log_likelihood : log likelihood from trained naive bayes.\n    return:\n        log_prob : float between 0 and 1. probability that s is positive.\n    '''    \n    \n    words = proprocess(s)\n\n    log_prob = 0\n\n    for w in words:\n        if w in log_likelihood:\n            log_prob += log_likelihood[w]\n\n    log_prob += log_prior\n\n    return log_prob\n\n# print probability of test data.\ntest_data = 'hope you get well soon. it hurts to see you ill 😢'\nprint('prediction:', predict_naive_bayes(test_data, log_prior, log_likelihood))\n\n# output: 3.5905424260671044 -- 긍정 정서로 예측했다.\n```\n\n\n## 참고 자료\n1. Coursera, deeplearning.ai, Natural Language Processing with Classification and Vector Spaces, week 2 \n2. Wikipedia, Naive Bayes Classification, https://en.wikipedia.org/wiki/Naive_Bayes_classifier\n3. Wikipedia, Additive Smooothing, https://en.wikipedia.org/wiki/Additive_smoothing\n4. Wikipedia, Likelihood-ratio test, https://en.wikipedia.org/wiki/Likelihood-ratio_test","excerpt":"이 글은 deeplearning.ai의 NLP Specialization를 참고하여 나이브 베이즈 모델을 텍스트 정서 분석에 초점을 맞춰 정리한 글입니다. Github에서 Naive Bayes 코드 보기 0. 모델 개략 나이브 베이즈 모델은 분류 과…","fields":{"slug":"/naive_bayes/"},"frontmatter":{"date":"Mar 28, 2022","title":"나이브 베이즈 분류","tags":["NLP","Classification","Sentiment Analysis"],"update":"Jan 01, 0001"}}},{"node":{"rawMarkdownBody":"\n## 최단 경로 알고리즘\n\n그래프는 간선에 가중치 정보를 추가할 수 있다. 이런 그래프를 가중 그래프(weighted graph) 라고 하며, 자연스럽게 출발 노드에서 특정 노드로 가는 경로의 가중치 합이 최소가 되는 경로를 찾는 문제가 발생한다. 깊이 우선 탐색(DFS)에서 이동하는 `경로의 개수를 최소화`하는 것이 목적이었다면 이 문제에서는 가중 그래프에서 `경로 가중치의 합을 최소화` 하는 것이 목적이며, 이 문제를 `최단 경로 알고리즘`이라고 부른다.\n\n이 글에서는 두개의 최단 경로 알고리즘을 정리하고자 한다.\n\n1. Dijkstra 알고리즘\n2. Floyd-Warshall 알고리즘\n\n## 1. Dijkstra 알고리즘\nDijkstra 알고리즘은 최단 경로 알고리즘으로, 시작 노드에서 모든 노드까지의 최단 경로를 계산한다. Dijkstra 알고리즘은 매 시점에서 가장 비용이 적은 노드를 선택하는 **그리디 알고리즘**이며, 이 때문에 가중치가 모두 양수(positive number)인 유향(directed) 그래프에 대해서만 작동한다.\n\n### 📂 우선 순위 큐를 활용한 Dijkstra 알고리즘 구현\n_최단 거리를 정렬하기 위해 우선 순위 큐를 사용하지만, 큐를 사용하지 않고서도 알고리즘을 구현할 수 있다._\n\nDijkstra 알고리즘은 우선 순위 큐를 활용한 재귀 함수로 구현할 수 있다. \n1. 저장된 최단 거리가 입력 받은 거리보다 짧은 경우, 함수를 종료한다.\n2. 1번에서 끝나지 않은 경우, 현재 노드에서 모든 연결된 노드로 가는 경로를 고려해, 이미 저장된 경로와 거리를 비교한다. 저장된 최단 거리가 계산한 거리보다 긴 경우, 더 짧은 거리로 `dist` 배열의 해당 값을 변경한다.\n3. 우선 순위 큐가 빌 때 까지 위 과정을 반복한다.\n\n#### 파이썬 코드\n```python\nfrom heapq import heappush, heappop\nfrom collections import defaultdict\n\n# 노드 개수 n과 간선 개수 m을 입력받는다.\nn, m = map(int, input().split())\n\n# 시작 노드를 입력 받는다.\nstart = int(input())\n\n# 1차원 배열 graph를 초기화한다.\ngraph = defaultdict(list)\n\n# graph[i]는 (node_number, weight)를 원소로 하는 리스트다.\nfor _ in range(m):\n    i, j, w = map(int, input().split())\n    graph[i].append(j, w)\n\n# graph와 노드 개수 n을 입력받아\n# `시작 노드`에서 `모든 노드`까지의 최단 거리를 반환한다. \ndef shortest_path(graph, n, start):\n\n    # distance[i] : 시작 노드에서 노드 i 까지의 최단 거리\n    # 큰 값으로 초기화한다.\n    INF = int(1e9)\n    distance = [INF] * (n + 1)\n\n    def dijkstra(start):\n        # q의 원소: (shortest_distance, node_number)\n        # 시작노드에서 시작노드까지의 거리는 0이다.\n        hq = []\n        heappush(hq, (0, start))\n        distance[start] = 0\n\n        # q가 존재하는 한 계속한다.\n        while hq:\n            dist, now = heappop(q)\n            # 저장된 최단 거리가 계산한 거리보다 짧은 경우, \n            # 변경하지 않는다.\n            if distance[now] < dist:\n                continue\n            # 연결된 노드에 대해 새로운 경로의 거리를 비교한다.\n            for n, d in graph[now]:\n                w_dist = dist + d\n                # 저장된 최단 거리가 계산한 거리보다 긴 경우, \n                # 더 짧은 거리로 dist 값을 변경한다.\n                if w_dist < distance[n]:\n                    distance[n] = w_dist\n                    heappush(hq, (w_dist, n))\n\n    # 시작노드에 대해 dijkstra를 구현한다.\n    dijkstra(start)\n    return distance\n\nshortest = shortest_path(graph, n, start))\nfor i in range(n):\n    print(f\"Shortest Path from node {start} to node {i}: {shortest[i+1]}\")\n```\n시간 복잡도는 우선 순위 큐의 정렬에 의해 노드 개수 $N$과 가중치 개수 $M$에 대해 $O(MlogN)$이다.\n\n## 2. Floyd-Warshall 알고리즘\n\nFloyd-Warshall 알고리즘은 모든 노드에서 모든 노드까지의 최단 경로를 계산하는 **다이내믹 프로그래밍** 알고리즘이다. 따라서 점화식을 알기만 하면 구현이 비교적 간단하다는 장점이 있다.\n\n`노드 i`에서 `노드 j`로 가는 임의의 경로가 있다고 하자. 만약 i에서 j로 가는 다른 경로가 있다면 이 경로는 임의의 경로가 지나지 않는 다른 노드를 거쳐갈 것이다. 다른 노드를 임의로 `노드 k`라고 할때, `노드 i`에서 `노드 k`로 다시 `노드 k`에서 `노드 j`로 가는 경로와 그렇지 않은 경로를 비교할 수 있다. $dist[i][j]$를 노드 i에서 j로 가는 최단 경로라고 정의하면 다음과 같이 점화식을 쓸 수 있다.\n$$\ndist[i][j] = min(dist[i][j], dist[i][k] + dist[k][j])\n$$\n따라서 노드 개수 N에 대해 $(N, N)$크기의 2차원 배열에 임의의 `노드 i`에서 임의의 `노드 j`로 가는 최단 경로를 저장한다. \n\n### 📂 Floyd-Warshall 알고리즘 구현\n1. 노드 개수 $N$에 대해 $(N, N)$ 크기의 2차원 배열을 초기화한다.\n2. 위의 점화식을 이용해 모든 k에 대해 2차원 배열을 순회한다.\n\n#### 파이썬 코드\n```python\n# 노드 개수 n과 간선 개수 m을 입력받는다.\nn, m = map(int, input().split())\n\n# 2차원 dist 배열을 큰 값으로 초기화 한다.\n# dist[i][j] : 노드 i에서 노드 j로 가는 최단 경로\nINF = 1e9\ndist = [INF for _ in range(n)] for _ in range(n)\n\n# 간선 개수만큼 간선 정보를 2차원 배열에 입력받는다.\nfor _ in range(m):\n    i, j, w = map(int, input().split())\n    dist[i][j] = w\n\n# dist와 노드 개수 n을 입력받아\n# `모든 노드`에서 `모든 노드`까지의 최단 거리를 저장한 2차원 배열을 반환한다. \nfor k in range(n+1):\n    for i in range(n+1):\n        for j in range(n+1):\n            # 저장되어 있는 최단 경로 dist[i][j]와\n            # 노드 k를 거치는 경로 dist[i][k] + dist[k][j]를 비교한다.\n            dist[i][j] = min(dist[i][j], dist[i][k] + dist[k][j])\n\nfor i in range(n):\n    for j in range(n):\n        print(f\"Shortest Path from node {i} to node {j}: {dist[i+1][j+1]}\")\n```\n\nFloyd-Warshall 알고리즘은 삼중 for문에 의해 시간 복잡도가 $O(N^3)$이므로 노드 개수가 많은 그래프는 수행시간에 유의해야 한다. \n\n---\n\n## 참고자료\n1. Youtube, (이코테 2021 강의 몰아보기) 7. 최단 경로 알고리즘, https://www.youtube.com/watch?v=acqm9mM1P6o","excerpt":"최단 경로 알고리즘 그래프는 간선에 가중치 정보를 추가할 수 있다. 이런 그래프를 가중 그래프(weighted graph) 라고 하며, 자연스럽게 출발 노드에서 특정 노드로 가는 경로의 가중치 합이 최소가 되는 경로를 찾는 문제가 발생한다. 깊이 우…","fields":{"slug":"/shortest_path/"},"frontmatter":{"date":"Mar 21, 2022","title":"최단 경로 알고리즘","tags":["Algorithms","Shortest Path","Dijkstra Algorithm","Floyd-Warshall Algorithm"],"update":"Jan 01, 0001"}}},{"node":{"rawMarkdownBody":"\n> 문제: [프로그래머스](https://programmers.co.kr/learn/courses/30/lessons/12978)\n\n## 문제 해결 아이디어\n\n그래프와 각 간선의 가중치가 주어질 때, 시작 노드에서 출발해 지나는 간선의 가중치를 모두 더해 도착할 때까지 `가중치의 합`이 `K` 이하가 되는 노드를 모두 찾는 문제다. 즉 시작노드에서 특정 노드까지 가중치 합을 최소로 하는 경로를 찾아야 하며, 이런 문제를 `최단 경로 알고리즘`이라고 부른다.\n\n> [최단 경로 알고리즘 알아보기](https://snowith.github.io/shortest_path/)\n\n이 문제는 Dijkstra 알고리즘을 구현하여 해결할 수 있다. \n1. 주어진 road의 정보를 한쪽 노드를 키로 하고 반대쪽 노드와 가중치를 값으로 하는 딕셔너리 형태로 저장한다.\n2. 다익스트라 알고리즘을 재귀 함수로 구현한다.\n    - 입력 받은 경로와 dist 배열에 저장된 값을 비교하여 업데이트한다.\n    - 시작 노드의 최소 경로에서 연결 된 노드를 연결한 경로에 대해 다익스트라 알고리즘을 실행한다. \n    \n3. 시작 노드인 1번 노드와 최단거리 0에 대해 다익스트라 알고리즘을 실행한다.\n4. K보다 작은 값들을 세어 반환한다.\n\n## 파이썬 코드\n```python\nfrom collections import defaultdict\n\ndef solution(N, road, K):\n    \n    # 시작 노드에서 각 노드까지의 거리를 큰 값으로 초기화 한다.\n    # 인덱스는 '주어진 노드 - 1'로 설정한다.\n    INF = 1e9\n    dist = [INF] * (N + 1)\n    dist[0] = 0\n\n    # road의 정보를 딕셔너리로 저장한다.\n    graph = defaultdict(list)\n    for a, b, c in road:\n        graph[a].append([b,c])\n        graph[b].append([a,c])\n    \n    # dijkstra를 구현한다.\n    # graph 딕셔너리와 시작노드 v, 최단 경로(시간) time이 주어졌을 때\n    # 함수 밖의 dist 배열에 경로의 최소값을 저장한다.\n    def dijkstra(start_node, time):\n        # 최단 경로만 저장한다.\n        if dist[start] > time:\n            dist[start] = time\n        # 연결된 노드에 대해 dijkstra를 실행한다.\n        for next_, t in graph[start]:\n            if t + time < dist[next_]:\n                dijkstra(graph, next_, t + time)\n    \n    # 1번 노드의 최단 경로가 0인 것에서 출발해서\n    # 모든 노드의 최단 경로를 찾는다.\n    dijkstra(graph, 1, 0)\n\n    # K 보다 작은 dist의 값들을 세어 반환한다.\n    cnt = 0\n    for n in range(N):\n        if dist[n] <= K:\n            cnt += 1\n\n    return cnt\n```\n","excerpt":"문제: 프로그래머스 문제 해결 아이디어 그래프와 각 간선의 가중치가 주어질 때, 시작 노드에서 출발해 지나는 간선의 가중치를 모두 더해 도착할 때까지 이  이하가 되는 노드를 모두 찾는 문제다. 즉 시작노드에서 특정 노드까지 가중치 합을 최소로 하는…","fields":{"slug":"/programmers. 배달/"},"frontmatter":{"date":"Mar 19, 2022","title":"programmers. 배달","tags":["Algorithms","Graph","Shortest Path","Dijkstra Algorithm"],"update":"Jan 01, 0001"}}},{"node":{"rawMarkdownBody":"\n## Greedy algorithm\n\n(전체가 아닌) 부분에서 최적의 해를 구하는 것이 전체 문제의 최적 해가 될때 사용하는 알고리즘으로, 구현하기는 어렵지 않지만 정당성을 증명해야 답을 보장할 수 있다. 다음의 문제들에서 그리디 알고리즘을 사용해 정당성 증명을 연습할 수 있다.\n\n> 모든 문제의 출처: [이코테 유튜브](https://www.youtube.com/watch?v=m-9pAwq1o3w&list=PLRx0vPvlEmdAghTr5mXQxGpHjWqSz0dgC&index=1)\n\n\n</br>\n\n\n### <문제> 거스름 돈\n\n`n`원을 `10원`, `50원`, `100원`, `500원` 동전으로 거슬러 주려고 할 때, 거슬러주는 동전의 개수를 최소화하려고 한다. 이때 거슬러 주는 동전의 최소 개수를 반환하는 문제이다.\n\n#### 문제 해결 아이디어\n\n- 전략: 가장 큰 화폐 단위부터 거슬러 준다.\n- 정당성 증명: 거슬러 줄 동전 중 큰 단위가 항상 작은 단위의 배수이므로, 같은 돈을 거슬러 주는 방법 중 가장 적은 동전을 사용하는 방법은 가능한 한 가장 큰 단위의 동전을 사용하는 것이다.\n- 즉 동전의 단위가 500원, 400원, 100원으로 주어진 경우에는 그리디 알고리즘으로 해답을 구할 수 없다.\n\n#### 파이썬 코드\n\n```python\ncoins = [500, 100, 50, 10]\ncnt = 0\n\nfor coin in coins:\n\tcnt += n // coin\n\tn %= coin\n\nprint(count) \n```\n\n\n\n### <문제> 1이 될 때까지\n\n1이 아닌 숫자 `N`과 `K`가 주어진다. 오직 두 가지 연산만 할 수 있는데, N이 K로 나누어지는 경우 `N을 K로 나눌` 수 있고, 그 외의 경우에는 `N에서 1을 뺄` 수 있다. 여기서 N을 1로 만드는 최소 연산 횟수를 구하는 문제다.\n\n#### 문제 해결 아이디어\n\n- 전략 : 숫자 N이 K로 나누어지는 경우의 수를 최대한으로 하되, 만약 N이 K로 나누어지지 않으면 1을 뺀다.\n- 정당성 증명: 만약 N이 K로 나누어지면, N과 K는 1이 아니므로 N 나누기 K 는 N - 1보다 작은 값이 된다. 언제나 1을 빼는 것으로 N을 1로 만들 수 있으므로, 매번 N을 최대한 작은 수로 만드는 방법으로 전체 연산 횟수를 최소화 할 수 있다.\n\n#### 파이썬 코드\n\n```python\nn, k = map(int, input().split())\ncnt = 0\n\nwhile n >= k:\n\tif n % k == 0:\n\t\tcnt += 1\n\t\tn //= k\n\telse:\n\t\tcnt += n % k\n\t\tn -= n % k\n\nprint(cnt + n - 1)\n```\n\n\n### <문제> 곱하기 혹은 더하기\n\n0 또는 양수인 임의의 수들 `s`가 주어질 때, 두 수를 더하거나 곱해서 만들 수 있는 가장 큰 수를 반환하는 문제다.\n\n#### 문제 해결 아이디어\n\n- 전략 : 피연산자 두개 중 하나라도 0이나 1이면 더하고, 그 외의 경우면 곱한다.\n- 정당성 증명: 연산의 왼쪽 숫자를 임의의 수 N이라 할 때, 모든 수는 0 또는 양수이므로 `N * 0 = 0` 보다 `N + 0 = N` 이 같거나 크고, `N * 1 = N` 보다 `N + 1` 이 더 크다. 반면, 2이상 9이하의 정수 X에 대해 `N * X` 보다 `N + X` 가 같거나 작다. 곱셈과 덧셈은 교환법칙이 성립하므로 피연산자의 순서에 관계없이 법칙이 성립한다.\n\n#### 파이썬 코드\n\n```python\ns = input()\nret = int(s[0])\n\nfor i in range(1, len(s)):\n\tnum = int(s[i])\n\tif num <= 1 or ret <= 1: \n\t\tret += num\n\telse:\n\t\tret *= num\n\nprint(ret)\n```\n\n\n### <문제> 모험가 길드\n\n어떤 마을에 모험가들이 있다. 모험가들은 제각기 공포도가 있는데, 공포도가 `i`인 사람은 `i`명 이상이 속한 그룹에 들어야 모험을 나갈 수 있다. 모험가들의 공포도가 주어질 때, 모험을 나가는 그룹의 수를 최대화해서 반환하는 문제다. 단, 모험가들이 마을에 남아있는 경우도 허용된다.\n\n#### 문제 해결 아이디어\n\n- 전략: 공포도가 적은 사람부터 순서대로 그룹을 꾸리되, 그룹의 최소 정원이 만족되면 다음 그룹으로 편성한다.\n- 정당성 증명: 주어진 정보에 대해 그룹수가 최대가 되도록 꾸려진 임의의 편성이 있다고 하자. 이 편성의 한 그룹에 대해 공포도가 적은 사람부터 그룹의 최소 정원을 만족하도록 그룹을 만들고 이외의 모험가들을 제외시켜도 여전히 그룹의 수는 최대이다. 따라서 위의 전략은 최적의 해를 보장한다.\n\n#### 파이썬 코드\n\n```python\nn = int(input())\nfears = list(map(int, input().split()))\nfears.sort()\n\n# cnt : 각 그룹에 포함된 사람의 수\n# ret : 전체 그룹의 수\ncnt, ret = 0, 0\n\nfor fear in fears:\n\tcnt += 1\n\t# fear가 cnt를 넘으면,\n\t# 전체 그룹 수를 1 증가하고 cnt를 초기화 한다. \n\tif cnt >= fear:\n\t\tret += 1\n\t\tcnt = 0\n\t\nprint(ret)\n```\n\n</br>\n\n### 참고자료\n\n- Coursera, Algorithmic Toolbox, Greedy Algorithm\n","excerpt":"Greedy algorithm (전체가 아닌) 부분에서 최적의 해를 구하는 것이 전체 문제의 최적 해가 될때 사용하는 알고리즘으로, 구현하기는 어렵지 않지만 정당성을 증명해야 답을 보장할 수 있다. 다음의 문제들에서 그리디 알고리즘을 사용해 정당성 …","fields":{"slug":"/greedy/"},"frontmatter":{"date":"Mar 09, 2022","title":"그리디 알고리즘의 정당성 증명","tags":["Algorithms","Greedy"],"update":"Jan 01, 0001"}}},{"node":{"rawMarkdownBody":"\n> 문제 : [프로그래머스](https://programmers.co.kr/learn/courses/30/lessons/72412)\n\n## 문제 해결 아이디어\n효율성 통과가 까다롭다고 느꼈던 문제중 하나로, 점수를 제외한 `검색 조건의 개수`가 `4개`에 불과하다는 점을 이용해서 모든 조건 조합을 해시한 다음 이진 검색을 통해 해결할 수 있었다.\n\n코드는 문제에서 요구하는 순서대로 작성하면 된다. 우선 `info`에서 주어진 정보로 `점수를 제외한 조건`들을 해시의 키로, `점수 조건`을 해시값으로 저장한다. 이때, 점수를 제외한 조건들을 모두 `'-'`로 대체하는 경우도 고려하도록 한다. 이때 파이썬 `combinations`를 이용해 모든 조합을 구했다. 그 다음 해시값인 리스트를 정렬하고, `query`에 주어진 `점수를 제외한 조건`을 해시에서 참조해서 주어진 `점수 조건`과 같거나 큰 값들 중 최소값을 구한다. 전체 길이에서 최소값의 인덱스를 빼서 반환하면 된다.\n\n### 시간 복잡도 계산하기\n문제가 되는 것은 시간 복잡도이다. 우선 주어진 `info`배열의 크기가 최대 `50,000`이므로, 점수를 제외한 모든 조건 조합을 해시하는 경우 해시의 키가 형성되는 횟수를 세어야 한다. 모든 조건에서 `'-'`가 대신 들어가는 경우를 고려해도 $\\sum^{4}_{i=0} \\binom{4}{i}$개로, 이 값은 상수이므로 선형시간안에 해결할 수 있다. \n해시값을 정렬 하는데에는 $O(NlogN)$ 시간이 소요된다. 마지막 과정인 `query`에 대해서도 `for문`을 거쳐야하고, `query`가 최대 `100,000`개 이므로 효율적인 탐색이 필요하다. 이진탐색을 활용하면 $O(NlogN)$ 시간 복잡도로 문제를 해결할 수 있다. \n\n전체 과정의 시간 복잡도는 $O(N) + O(NlogN) + O(NlogN) = O(NlogN)$이 된다.\n\n\n## 파이썬 코드\n```python\nfrom itertools import combinations\n\ndef solution(info, query):\n\n    hsh = {}\n    for inf in info:\n        # info의 값 중 조건 분류와 점수를 따로 저장한다. \n        conds = inf.split()[:-1]\n        score = int(inf.split()[-1])\n        \n        # 모든 조건의 조합을 해시한다. \n        for i in range(5):\n            combs = list(combinations(range(4), i))\n            for comb in combs:\n                # 순서대로 '-'를 삽입하는 키를 생성한다.\n                temp = conds.copy()\n                for j in comb:\n                    temp[j] = '-'\n                new_cond = ' and '.join(temp)\n                \n                # hsh의 조건 조합 키에 점수을 추가한다.\n                if new_cond not in hsh:\n                    hsh[new_cond] = [score]\n                else:\n                    hsh[new_cond] += [score]\n\n    # 이진 검색을 위해 hsh값 리스트 내의 값들을 정렬한다.  \n    for values in hsh.values():\n        values.sort()\n    \n    ret = []   \n    for q in query:\n        # query의 값 중 조건 분류와 점수를 따로 저장한다.\n        cond = ' '.join(q.split()[:-1])\n        score = int(q.split()[-1])\n        \n        # hsh에 해당하는 키가 있는 경우\n        if cond in hsh:\n            # hsh의 값에서 점수가 query에서 요구하는 점수 \n            # score와 같거나 큰 점수를 이진 탐색한다.\n            left, right = 0, len(hsh[cond])\n            while left <= right and left != len(hsh[cond]):\n                half = (left + right) // 2\n                if hsh[cond][half] >= score:\n                    right = half - 1\n                else:\n                    left = half + 1\n            # 해시값 길이에서 leftmost 인덱스를 뺀 값을 저장한다.\n            ret.append(len(hsh[cond]) - left)\n        # 만약 hsh에 해당 키가 없는 경우, 0을 저장한다.\n        else:\n            ret.append(0)\n\n    return ret\n```","excerpt":"문제 : 프로그래머스 문제 해결 아이디어 효율성 통과가 까다롭다고 느꼈던 문제중 하나로, 점수를 제외한 가 에 불과하다는 점을 이용해서 모든 조건 조합을 해시한 다음 이진 검색을 통해 해결할 수 있었다. 코드는 문제에서 요구하는 순서대로 작성하면 된…","fields":{"slug":"/programmers. 순위 검색/"},"frontmatter":{"date":"Mar 05, 2022","title":"programmers. 순위 검색","tags":["Algorithms","programmers","Hashing","Binary Search"],"update":"Jan 01, 0001"}}},{"node":{"rawMarkdownBody":"\n> 문제 : [프로그래머스](https://programmers.co.kr/learn/courses/30/lessons/43163)\n\n\n## 문제 해결 아이디어\n\n단어가 알파벳 하나 차이로 다른 경우 단어 노드를 간선으로 연결한 후 시작 단어에서 타깃 단어에 이르기 까지 깊이 우선 탐색을 이용하여 해결할 수 있다.\n\n- 그래프 생성: 단어 2개 조합에서 알파벳이 하나 차이나는 단어를 그래프에 저장한다.\n- 최소 변환 횟수 찾기: 깊이 우선 탐색을 이용해, 타겟 단어에 이르기까지 최소 거리를 업데이트하며 연결된 노드를 탐색한다.\n\n\n## 파이썬 코드\n```python\nfrom collections import defaultdict\nfrom itertools import combinations\n\ndef solution(begin, target, words):\n    \n    graph = defaultdict(list)\n    combs = list(combinations(words + [begin], 2))\n    \n    # 2개 단어 조합 중에서 알파벳이 하나 차이나면 graph에 저장\n    for comb in combs:\n        w1, w2 = comb[0], comb[1]\n        sameness = sum([1 if c1 == c2 else 0 for c1, c2 in zip(w1, w2)])\n        if sameness == len(w1) - 1:\n            graph[w1].append(w2)\n            graph[w2].append(w1)\n        \n    # 깊이 우선 탐색 구현\n    min_d = 100\n    visited = []\n    # stack에 [노드, begin에서의 거리] 형태로 저장\n    stack = [[begin, 0]]\n    while stack:\n        v, d = stack.pop()\n        if v == target and min_d > d:\n            min_d = d\n        if v not in visited:\n            visited.append(v)\n            for u in graph[v]:\n                stack.append([u, d + 1])\n    \n    return min_d if min_d != 100 else 0\n```","excerpt":"문제 : 프로그래머스 문제 해결 아이디어 단어가 알파벳 하나 차이로 다른 경우 단어 노드를 간선으로 연결한 후 시작 단어에서 타깃 단어에 이르기 까지 깊이 우선 탐색을 이용하여 해결할 수 있다. 그래프 생성: 단어 2개 조합에서 알파벳이 하나 차이나…","fields":{"slug":"/programmers. 단어 변환/"},"frontmatter":{"date":"Feb 26, 2022","title":"programmers. 단어 변환","tags":["Algorithms","programmers","Graph","DFS"],"update":"Jan 01, 0001"}}},{"node":{"rawMarkdownBody":"\n> 문제 : [프로그래머스](https://programmers.co.kr/learn/courses/30/lessons/42627)\n\n## 문제 해결 아이디어\n\n- 하드디스크가 비어있을 때 요청이 들어오면 바로 처리한다.\n- 하드디스크가 작업 중일 때 요청이 들어오면 진행 중인 작업을 끝내고, 작업 소요 시간을 최소 힙으로 정렬하여 힙의 순서대로 처리한다.\n\n이때 `start`와 `end`로 현재 작업 시간의 양쪽 끝을 설정해서 지속적으로 업데이트 할 수 있다. 또 루프를 시작할 때 요청시간이 `end`와 작거나 같은 것으로 탐색하므로, 아무 것도 처리하지 않는 빈 시간에 대해 주어진 리스트 `jobs`의 인덱스 `i`는 그대로 두고 `end`를 1씩 증가시킨다. \n\n`jobs`의 길이, 즉 힙이 정렬할 원소 개수의 최대값은 500 이하이고, 작업 소요시간은 1,000이하이지만 다음코드에서 한번에 처리(`end += now[0]`)하므로 시간내에 문제를 해결할 수 있다.\n\n## 파이썬 코드\n```python\nfrom heapq import heappush, heappop\n\ndef solution(jobs):\n\n    start, end = -1, 0\n    ret, hq = [], []\n\n    i = 0\n    # 힙에서 하나를 꺼낼때 마다 start와 end를 업데이트한다\n    while i < len(jobs):\n        # 요청시간이 작업 도중인 경우, 힙에 추가하기\n        for job in jobs:\n            if start < job[0] <= end:\n                heappush(hq, [job[1], job[0]])\n        # 힙이 비어 있지 않으면\n        # 힙에 있는 소요 시간이 가장 작은 작업을 처리한다\n        if len(hq) > 0:\n            now = heappop(hq)\n            start = end\n            end += now[0]\n            ret.append(end - now[1])\n            i += 1\n        # 힙이 비었으면, 즉 현재 작업이 끝날 때까지 요청이 없으면\n        # 요청이 들어올 때까지 end를 증가시킨다\n        else:\n            end += 1\n\n    # 문제에서 요구하는 작업 소요시간 평균을 반환한다\n    return sum(ret) // len(ret)\n```","excerpt":"문제 : 프로그래머스 문제 해결 아이디어 하드디스크가 비어있을 때 요청이 들어오면 바로 처리한다. 하드디스크가 작업 중일 때 요청이 들어오면 진행 중인 작업을 끝내고, 작업 소요 시간을 최소 힙으로 정렬하여 힙의 순서대로 처리한다. 이때 와 로 현재…","fields":{"slug":"/programmers. 디스크 컨트롤러/"},"frontmatter":{"date":"Feb 26, 2022","title":"programmers. 디스크 컨트롤러","tags":["Algorithms","programmers","Heap"],"update":"Jan 01, 0001"}}},{"node":{"rawMarkdownBody":"\n> 문제 : [프로그래머스](https://programmers.co.kr/learn/courses/30/lessons/12927)\n\n## 문제 해결 아이디어\n문제에서 정의한 야근 피로도는 야근을 시작한 시점에서 남은 일의 작업량을 모두 제곱하여 더한 값으로 해석할 수 있다. 수식으로 쓰면 문제의 목표는\n$$\nmin(\\sigma_i {work_i}^2) \n$$\n이고 주어진 리스트에 대해 최대값을 최소화해서 목표를 달성할 수 있다. (남은 작업량이 주어지고 그 중에서 1만 처리할 수 있을때, 가장 큰 값을 1 처리하는 것이 전체 식을 최소화한다는 것을 증명할 수 있다.) 따라서 임의 시점에서 가장 작업량이 큰 일을 1만큼 처리해야 하며, 작업량을 최대힙으로 정렬해서 과제를 수행할 수 있다.\n\nn은 1,000,000 이하의 자연수이고 최대 $2*n$번 힙 연산이 수행되므로 $O(nlogn)$ 시간 내에 문제를 해결할 수 있다.\n\n## 파이썬 코드\n```python\nimport heapq\n\ndef solution(n, works):\n    if sum(works) <= n: return 0\n    \n    yageun = []\n    for work in works:\n        heapq.heappush(yageun, -work)\n        \n    # 한 시간 씩 작업하되, 가장 작업량이 큰 일을 1만큼 처리한다\n    while n != 0:\n        work = heapq.heappop(yageun)\n        heapq.heappush(yageun, work + 1)\n        n -= 1\n        \n    # 힙에 남은 원소에 대해 야근 피로도를 계산한다\n    return sum(list(map(lambda x: x ** 2, yageun)))\n```","excerpt":"문제 : 프로그래머스 문제 해결 아이디어 문제에서 정의한 야근 피로도는 야근을 시작한 시점에서 남은 일의 작업량을 모두 제곱하여 더한 값으로 해석할 수 있다. 수식으로 쓰면 문제의 목표는 이고 주어진 리스트에 대해 최대값을 최소화해서 목표를 달성할 …","fields":{"slug":"/programmers. 야근 지수/"},"frontmatter":{"date":"Feb 26, 2022","title":"programmers. 야근 지수","tags":["Algorithms","programmers","Heap"],"update":"Jan 01, 0001"}}},{"node":{"rawMarkdownBody":"\n> 문제 : [프로그래머스](https://programmers.co.kr/learn/courses/30/lessons/42628)\n\n## 문제 해결 아이디어\n\n최소 힙과 최대 힙을 동시에 이용하는 것이 핵심이다. 이중 우선순위 큐에 적용하는 세가지 연산을 경우 별로 나누어 고려한다.\n\n- 원소를 추가할 경우, 최소 힙과 최대 힙에 모두 추가한다.\n- 최소값을 제거할 경우, 최소 힙이 비어있지 않으면 최소힙의 최소값을 제거한다. 만약 최대힙의 인덱스 0인 값이 지우려는 해당 원소이면, 최대힙에서 이 원소를 제거한다.\n- 최대값을 제거할 경우, 최대 힙이 비어있지 않으면 최대힙의 최대값을 제거한다. 만약 최소힙의 인덱스 0인 값이 지우려는 해당 원소이면, 최소힙에서 이 원소를 제거한다.\n- 다음 루프를 실행하기 전에, 최소 힙이나 최대 힙 중 하나라도 비어있거나 최소힙의 최소값이 최대힙의 최대값보다 크면, 두개 힙을 모두 비운다.\n\n루프가 종료된 후, 최소힙과 최대힙 둘 중 하나라도 비어있으면 이중 우선순위 큐가 비었다는 뜻이므로 [0,0]을 반환한다. 아닐경우, 최대힙에서 최대값을 최소힙에서 최소값을 반환한다.\n\n주어진 리스트 operations의 길이는 최대 1,000,000이지만, 힙으로 정렬한 수행시간은 $O(NlogN)$으로 시간내에 해결 할 수 있다.\n\n\n## 파이썬 코드\n```python\nfrom heapq import heappush, heappop\n\ndef solution(operations):\n    \n    hq1, hq2 = [], []\n    for op in operations:\n        # hq1이 최소 힙, hq2가 최대 힙\n        if op[0] == 'I':\n            heappush(hq1, int(op.split()[-1]))\n            heappush(hq2, -int(op.split()[-1]))\n        \n        # 최대 힙에서 원소를 제거한다\n        # 최소 힙이 비지 않고, 삭제할 값이 최소 힙에 있으면, 최소 힙에서 원소를 제거한다\n        elif op == 'D 1' and hq2 != []:\n            if hq1 and hq1[0] == -hq2[0]:\n                heappop(hq1)\n            heappop(hq2)\n\n        # 최소 힙에서 원소를 제거한다\n        # 최대 힙이 비지 않고, 삭제할 값이 최대 힙에 있으면, 최대 힙에서 원소를 제거한다\n        elif op == 'D -1' and hq1 != []:\n            if hq2 and hq1[0] == -hq2[0]:\n                heappop(hq2)\n            heappop(hq1)\n\n        # 최소 힙이나 최대 힙 중 하나라도 비어있거나, 두 힙의 최소값이 최대값보다 크다면 \n        # 힙을 모두 비운다\n        if (not hq1 or not hq2) or (hq1[0] > -hq2[0]): \n            hq1, hq2 = [], []\n            \n    if not hq1 or not hq2: \n        return [0, 0]\n\n    return [-hq2[0], hq1[0]]\n```","excerpt":"문제 : 프로그래머스 문제 해결 아이디어 최소 힙과 최대 힙을 동시에 이용하는 것이 핵심이다. 이중 우선순위 큐에 적용하는 세가지 연산을 경우 별로 나누어 고려한다. 원소를 추가할 경우, 최소 힙과 최대 힙에 모두 추가한다. 최소값을 제거할 경우, …","fields":{"slug":"/programmers. 이중 우선순위 큐/"},"frontmatter":{"date":"Feb 26, 2022","title":"programmers. 이중 우선순위 큐","tags":["Algorithms","programmers","Heap"],"update":"Jan 01, 0001"}}},{"node":{"rawMarkdownBody":"\n`word2vec`은 2013년 구글에서 고안한 자연어 처리 아이디어로, 이에 기반한 모델은 `Continuous Bag-of-Words(CBOW)`와 `Skip-gram` 두가지가 있다. 이 글은 그 중에서 `CBOW` 모델을 원 논문과 deeplearning.ai 수업을 참고하여 정리한 글이다. \n\n**원 논문**:\n- Mikolov et. al., 2013, Efficient Estimation of Word Representations in Vector Space ([arxiv](https://arxiv.org/pdf/1301.3781.pdf))\n- Mikolov et. al., 2013, Distributed Representations of Words and Phrases and their Compositionality ([arxiv](https://proceedings.neurips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf))\n\n\n## 0. CBOW란\nCBOW(Continuous Bag-of-Words, 연속되는 단어 주머니)는 \n- 텍스트 데이터를 벡터 공간에 표현하는 `단어 임베딩(word embedding)` 모델이자,  \n- `얕은 신경망(neural network)` 모델이며, \n- 스스로 훈련 데이터를 생성하는 `자기 지도 훈련(self-supervised learning)` 모델이다. \n\nCBOW 모델이 처음 소개될 때는 50-100 차원의 원 핫 벡터로 몇 백만개의 단어를 훈련시켰다. \n\n\n### 📖 Skip-gram과의 차이점\nCBOW 모델은 여러개의 단어 데이터를 입력하면 그에 상응하는 한개의 단어를 출력하는 `Many to One` (여러개 데이터를 입력받아 한개의 데이터를 출력하는 모델 구조) 모델이다. 반면 Skip-gram은 한개의 단어를 입력했을 때 그에 대응하는 여러개의 단어를 출력하는 `One to Many` 모델이다. 즉 두 모델의 구조는 `반전`되어있고, 입력값과 출력값이 서로 반대된다. \n\n## 1. 모델의 구조\nCBOW는 얕은 신경망 모델로, 이 글에서는 한 개의 은닉층(hidden layer)를 가지는 신경망 모델을 고려한다.\n\n![](cbow_model_architecture.png)\n*image by DeepLearning.AI*\n\n모델의 흐름은 다음과 같다.\n1. 텍스트 데이터를 원 핫 벡터로 변환한다.\n2. 첫번째 은닉층(hidden layer)을 거친다.\n    - 활성화 함수 : ReLU\n3. 두번째 결과층(output layer)을 거친다.\n    - 활성화 함수 : Softmax\n4. 결과 벡터의 값 중 가장 큰 값으로 예측한다.\n\n모델을 이해하고 실제로 구현하기 위해서는 각 층의 차원을 정확히 알아야 한다.\n### 📖 벡터의 차원\n변수를 다음과 같이 정의할 때,\n- $V$ : 단어 사전의 크기,  혹은 원-핫 벡터의 크기.\n- $N$ : 임베딩 크기. 모델의 하이퍼파라미터이다.\n- $m$ : 배치 크기. 한번에 훈련할 데이터의 개수이다.\n\n입력값 $X$의 차원\n$$\nX \\in M(V, m)\n$$\n에 대해 각 층에 대한 벡터의 차원을 다음과 같이 정리할 수 있다.\n<center>\n\n|은닉층 벡터|차원|결과층 벡터|차원|\n|---|---|---|---|\n|$W_1$|$(N, V)$|$W_2$|$(V, N)$|\n|$B_1$|$(N, m)$|$B_2$|$(V, m)$|\n|$z_1$|$(N, m)$|$z_2$|$(V, m)$|\n|$relu(z_1)$|$(N, m)$|$softmax(z_2)$|$(V, m)$|\n</center>\n\n$softmax(z_2) \\equiv \\hat{Y}$ 이므로 예측값이 입력값과 같은 차원을 가지는 것을 알 수 있다. 즉 모델이 반환하는 벡터의 열 벡터는 입력 열 벡터와 순서가 같은 원 핫 벡터이다.\n\n## 2. 모델의 전처리\n\nCBOW 모델로 문장의 빈칸을 주위 단어에 기반해 예측하는 과제를 수행해보자. 다음 문장의 빈칸에 뭐가 들어갈까?\n\n\"npm은 Node.js의 ____ 관리를 위한 패키지 매니저이다.\"\n\nCBOW 모델을 구현하기 위해서는 텍스트 데이터를 토큰화 한 후, 데이터를 모델에 입력하는 형태로 변환하는 다음 작업이 필요하다.\n\n### 📖  중심어(center word)와 맥락 단어들(context words)\n자기 지도 학습은 사람이 라벨링을 할 필요가 없다는 장점이 있다. 그러기 위해서는 가공되지 않은 텍스트 데이터에서 훈련 데이터($X$, 입력 데이터)와 훈련 타겟($Y$, 참값)을 구분해서 자료화할 필요가 있다. \n\nCBOW 모델에서 예측할 대상(target)인 문장의 빈칸을 `중심어`로, 이 단어와 문장 내에서 인접한 단어를 `맥락 단어`로 이름지을 수 있다. `맥락 단어`는 중심어로 부터 거리 $C$ 만큼 떨어져 있는 인접한 단어들로 정의하며, $C$를 `맥락의 절반 크기(context half-size)`라고 하자. $C$는 모델의 성능을 좌우하는 하이퍼파라미터 중 하나이다. 왼쪽 맥락 단어 리스트와 중심어, 오른쪽 맥락 리스트의 크기를 모두 더한 값을 `윈도우`라고 일컫는다. \n\n예를 들면, 문장 한개로 구성된 데이터에 대해 다음과 같이 이해할 수 있다. \n```python\n# given tokenized data and context half-size, \n# returns center word and list of context words \ndef center_and_context_word(data, C):\n    for i in range(C, len(data)-C):\n        center_word = data[i]\n        context_words = []\n        for j in range(i-C, i+C+1):\n            if j != i:\n                context_words.append(data[j])\n        yield center_word, context_words\n\nC = 2 # context half-size\ndata = [\"npm은\", \"Node.js의\", \"패키지\", \"관리를\", \"위한\", \"패키지\", \"매니저이다\", \".\"]\n\ncenter_word, context_word = next(center_and_context_word(data, C))\n\nprint(center_word) # \"패키지\"\nprint(context_word) # [\"npm은\", \"Node.js의\", \"관리를\", \"위한\"]\nprint(len(context_word + center_word)) # 5, window\n``` \n모델의 입력값은 `맥락 단어 벡터들의 평균값`을 취한다. 사실 CBOW 모델 이름에 Bag이 들어가는 이유는 $C$의 범위 내에 있는 맥락 단어들이 문장에서의 순서에 관계없이 여겨지기 때문이고, 이 특징은 이후에 등장하는 Sequential 모델과 구분되는 차이점이다.\n\n\n## 3. 모델 훈련하기\n\nCBOW 모델은 신경망 모델이므로 일반적인 forward propagation, backward propagation, gradient descent 과정을 거친다. 세가지 과정을 `keras` 라이브러리에서 `Layer` 객체로 비교적 간단하게 구현할 수 있다.\n\n\n### 📂 Keras로 CBOW 구현하기\n```python\nfrom tf.keras import layers\n\n# Input size: (batch_size, vocab_size)\ncbow_model = tf.keras.Sequential{\n    # 원 핫 벡터의 배치를 임베드한다\n    input_layer = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim), \n    # relu 은닉층으로 비용이 음수값을 가지지 않게 한다\n    hidden_layer = layers.Dense(units=embed_dim,activation='relu'), \n    # 원 핫 벡터의 배치를 확률로 출력한다\n    output_layer = layers.Dense(input_dim=vocab_size, activation='softmax')\n}\n\nbatch_size = 256\nepochs = 10\n\ncbow_model.compile(\n    optimizer='Adam', \n    loss=tf.keras.losses.CategoricalCrossentropy()\n)\n\ncbow_model.fit(\n    train_data, train_target, \n    batch_size=batch_size, epochs=epochs\n)\n```\n\n## 4. 단어 임베딩 추출하기\n\n단어 임베딩은 원 핫 벡터에 비해 `밀도가 높은(Dense) 벡터`로, 단어 임베딩에는 여러가지 이점이 있다. 첫째로 단어 임베딩 벡터간의 거리를 비교해서 의미론적(semantic)이고 문법론적인(syntactic) 정보를 얻을 수 있다. 둘째로 차원을 작게 만드는 것, 즉 `차원 축소(Dimensionality Reduction)`를 통해 계산 횟수를 획기적으로 줄일 수 있다. 벡터의 밀도가 높다는 것은 같은 데이터를 상대적으로 작은 차원으로 표현하는 것을 뜻한다. 반면에 벡터의 차원이 증가함에 따라 벡터를 계산하는 횟수는 기하급수적으로 늘어나게 되는데, 이 현상을 `차원의 저주(the curse of dimensionality)`라고 한다. 그 중에서 2차원이나 3차원 벡터는 시각화가 가능하므로 직관적인 이해에 도움이 된다.\n\n단어 임베딩은 CBOW 모델의 부산물이라고 할 수 있는데, 단어 임베딩은 훈련이 끝난 후 그 결과인 가중치 벡터로 부터 얻을 수 있다.\n\n단어 임베딩으로 선택할 수 있는 `옵션`은 다음과 같다.\n- 첫번째 가중치 벡터 $W_1$ 의 열(column) 벡터\n- 두번째 가중치 벡터 $W_2$ 의 행(row) 벡터\n- 두 가중치 벡터의 평균 $1/2 *(W_1 + W_2^{T})$ 의 열 벡터\n\n마지막 경우는 $1/2 * (W_1^{T} + W_2)$의 행 벡터와 같다. 위의 모든 경우에 대해 `한개의 단어 임베딩 벡터의 크기`는 임베딩 크기 $N$에 대해 $(N,1)$ 또는 $(1, N)$인 것을 알 수 있다.\n\n## 5. 모델 평가하기\n모델을 평가하는 방법에는 크게 두가지가 있다.\n### 📖  내재적 평가와 외재적 평가\n내재적 평가(Intrinsic Evaluation)는 임베딩된 단어들의 의미론적이고 문법론적인 관계를 평가하는 방법이다. 유의어(Analogies) 평가나 클러스터링 알고리즘, 또는 PCA 같은 시각화 기법들이 내재적 평가에 포함된다. 반면 외재적 평가(Extrinsic Evaluation)는 모델의 전체적인 성능을 파악하는데에 사용되는 방법이다. 전체 모델을 평가할 수 있지만, 평가 시간이 오래 걸리며 개선 방법에 대한 직관을 얻기 어렵다는 단점이 있다.\n\n### 📂  테스트 셋에 대해 (내재적으로) 평가하기\n![](Semantic-Syntactic_Word_Relationship_test_set.png)\n*table from Mikolov et al., 2013, Efficient Estimation of Word Representations in Vector Space*\n\n위의 표는 4개의 모델을 두가지 훈련 데이터에 대해 평가한 결과이다. 첫번째 훈련 데이터는 `의미론적(semantic)`이고 `문법론적(syntactic)` 관계 정확도인데, CBOW가 의미론적 정확도는 Skip-gram보다 두배 이상 떨어지지만 문법적 정확도에서는 조금 더 나은 것을 알 수 있다. 그렇지만 의미론적 정확도에 비해 문법론적 정확도에서 평가 모델들의 편차가 더 적었다. 두번째 데이터 셋에 대해서는 CBOW가 Skip-gram보다 단어 관계 평가가 조금 더 나은 것을 볼 수 있다.\n\n_참고 : 논문에서는 1억개가 넘어가지 않는 vocab에 대해 CBOW 모델을 훈련했으며, $C=4$ 설정에서 log-linear 분류로 최적의 결과를 얻었다고 한다._\n\n\n## 출처\n1. Mikolov et al., 2013, Efficient Estimation of Word Representations in Vector Space\n2. Coursera, deeplearning.ai, NLP Specialization, Course 2, Natural Language Processing with Probabilistic Models","excerpt":"은 2013년 구글에서 고안한 자연어 처리 아이디어로, 이에 기반한 모델은 와  두가지가 있다. 이 글은 그 중에서  모델을 원 논문과 deeplearning.ai 수업을 참고하여 정리한 글이다.  원 논문: Mikolov et. al., 2013,…","fields":{"slug":"/word2vec_cbow/"},"frontmatter":{"date":"Feb 25, 2022","title":"word2vec - Continuous Bag-of-Words(CBOW)","tags":["NLP","word2vec","Word Embedding"],"update":"Jan 01, 0001"}}},{"node":{"rawMarkdownBody":"\n\n> 문제 : [프로그래머스](https://programmers.co.kr/learn/courses/30/lessons/68646)\n\n\n### 문제 해결 아이디어\n- 투 포인터 알고리즘으로 문제를 해결 할 수 있다.\n\n1. 배열의 임의의 원소 a[i]에 대해 왼쪽 원소값 a[i-1]과 오른쪽 원소값 a[i+1]이 모두 a[i]보다 작은 경우 마지막까지 남을 수 없으므로 제외한다. \n2. 스택에 마지막까지 남을 원소값들을 저장한다. 오른쪽으로 한칸 나아갈 때 마다 스택에 저장한 원소와 배열 a의 남은 원소들에 대해 1번을 검사한다.\n3. 스택의 길이를 반환한다.\n\n\n### 파이썬 코드\n```python\ndef solution(a):\n\n    # 마지막까지 남는다고 판단한 풍선들의 값을 저장하는 스택\n    stack = [a[0]]\n    i = 1\n\n    while i < len(a) - 1:\n        # a[i] 값이 a[i-1]과 a[i-2]보다 크다면 마지막까지 남을 수 없다.\n        if stack[-1] < a[i] and a[i+1] < a[i]:\n            # 판단한 풍선들인 stack을 오른쪽부터 검사\n            while len(stack) > 1 and stack[-2] < stack[-1] and a[i+1] < stack[-1]:\n                stack.pop()\n        # 이외의 경우 마지막까지 남을 수 있으므로 stack에 저장\n        else:\n            stack.append(a[i])\n        # 한 칸 오른쪽 값들에 대해 검사\n        i += 1\n    \n    # 가장 오른쪽 풍선은 마지막까지 남는다.\n    if a[-1] != stack[-1]:\n        stack.append(a[-1])\n\n    return len(stack)\n```","excerpt":"문제 : 프로그래머스 문제 해결 아이디어 투 포인터 알고리즘으로 문제를 해결 할 수 있다. 배열의 임의의 원소 ai에 대해 왼쪽 원소값 ai-1과 오른쪽 원소값 ai+1이 모두 ai보다 작은 경우 마지막까지 남을 수 없으므로 제외한다.  스택에 마지…","fields":{"slug":"/programmers. 풍선 터뜨리기/"},"frontmatter":{"date":"Feb 11, 2022","title":"programmers. 풍선 터뜨리기","tags":["Algorithms","programmers","Two Pointer Algorithm"],"update":"Jan 01, 0001"}}},{"node":{"rawMarkdownBody":"\n> 문제 : [프로그래머스](https://programmers.co.kr/learn/courses/30/lessons/42861)\n\n\n### 문제 해결 아이디어\n`두 지점 사이`에 대해 건설 비용이 작은 것부터 건설하면 전체 건설 비용이 가장 작도록 선택할 수 있다. 만약 두 지점 사이의 건설 비용이 아니라 부분 그래프의 건설 비용이 주어졌다면 그리디 알고리즘으로 구현할 수 없다.\n\n#### 그리디 알고리즘 구현:\n- 건설 비용을 최소화 하는 것이 목적이므로 건설 비용이 저렴한 다리 부터 차례대로 건설한다.\n- 단, 다리의 양쪽 지점이 이미 연결된 경우에는 고려하지 않는다.\n- 모든 섬이 연결될 때까지 다리를 추가한다.\n\n\n그래프의 모든 노드에 대해 추가한 경로에 속하는 노드가 연결되어 있는지 확인하기 위해 깊이 우선 탐색을 구한현다.\n\n\n### 파이썬 코드\n```python\nfrom collections import defaultdict\n\ndef solution(n, costs):\n    cost = 0\n    nodes = list(range(n))\n    graph = defaultdict(list)\n    # 건설 비용이 저렴한 순으로 다리를 정렬한다.\n    costs.sort(key=lambda x: x[2])\n    \n    for i in range(len(costs)):\n        # 다리의 양쪽 지점이 이미 연결되어 있으면 고려하지 않는다.\n        node1, node2 = costs[i][:2]\n        if connected(node1, node2, graph):\n            continue\n            \n        # 양쪽 지점이 연결 되어 있지 않으면 다리를 건설한다.\n        cost += costs[i][2]\n        graph[node1].append(node2)\n        graph[node2].append(node1)\n        \n        # 모든 노드 node가 연결되어 있으면 멈춘다. \n        flag = True\n        for node in nodes:\n            if not connected(node1, node, graph):\n                flag = False\n        if flag:\n            break\n    return cost\n\n# graph에 있는 root와 target 노드의 연결 여부를 반환한다.\n# DFS(깊이 우선 탐색)으로 구현한다.\ndef connected(root, target, graph):\n    if root == target: \n        return True\n    visited = []\n    stack = [root]\n    while stack:\n        v = stack.pop()\n        if v == target: return True\n        if v not in visited:\n            visited.append(v)\n            for u in graph[v]:\n                stack.append(u)\n    return False\n```","excerpt":"문제 : 프로그래머스 문제 해결 아이디어 에 대해 건설 비용이 작은 것부터 건설하면 전체 건설 비용이 가장 작도록 선택할 수 있다. 만약 두 지점 사이의 건설 비용이 아니라 부분 그래프의 건설 비용이 주어졌다면 그리디 알고리즘으로 구현할 수 없다. …","fields":{"slug":"/programmers. 섬 연결하기/"},"frontmatter":{"date":"Feb 11, 2022","title":"programmers. 섬 연결하기","tags":["Algorithms","programmers","Greedy","Graph","DFS"],"update":"Jan 01, 0001"}}},{"node":{"rawMarkdownBody":"\n\n> 문제 : [프로그래머스](https://programmers.co.kr/learn/courses/30/lessons/64062)\n\n\n### 문제 해결 아이디어\n- 문제는 k 길이의 구간(또는 window) 최대값 중 최소값을 구하는 문제로 이해될 수 있다.\n\n- stones 배열의 길이가 n이라 할 때, max()함수로 최대값을 구하는데에 선형 시간이 소요되므로 max()함수로 구현시 전체 시간 복잡도는 $O(k * (n - k))$ 이다. n과 k는 최대 20,000이므로 k가 10,000일 때 연산 횟수는 100,000,000회이기 때문에 이 방법으로는 시간내에 문제를 해결할 수 없다.\n\n- 구간의 최대값을 구할 때 k개의 구간이 중복되므로 최대 힙으로 이 문제를 해결할 수 있다. \n- $[0, k)$구간의 원소를 힙으로 정렬한 뒤, 다음 구간에 추가되는 원소를 하나씩 push하며, 매번 힙의 최대값을 업데이트 한다. 이때, 힙의 최대 원소의 인덱스가 구간 외이면 pop한다.\n\n- 힙에 n개의 원소를 한번 씩 push하고, 힙의 최대값이 구간 외일 때만 pop한다. 힙에는 최소 k개의 원소가 남아있어야 하기 때문에 pop은 최대 $n-k$번 일어나는데, 이 최악의 경우에 이전 원소들은 pop없이 push되었고, 이후 원소들도 힙의 길이 k에서 다시 시작하므로 선형시간을 넘지 않는다. 따라서 전체 수행시간은 $O(nlogn)$ 이다.\n\n\n### 파이썬 코드\n```python\nfrom heapq import heappush, heappop\n\ndef solution(stones, k):\n\n    # k가 전체 구간일 때 예외 처리\n    if len(stones) == k:\n        return max(stones)\n\n    # 최대 힙 구현\n    hq = []\n    for i in range(0, k):\n        heappush(hq, (-stones[i], i))\n    k_maxes = [-hq[0][0]] # k 길이 구간에서의 최대값 저장\n\n    # 힙에 원소를 하나씩 업데이트 하면서 \n    # 만약 힙의 최대값의 인덱스가 구간 안에 있지 않으면 힙에서 제거한다.\n    for i in range(k, len(stones)):\n        heappush(hq, (-stones[i], i))\n        while hq[0][1] <= i - k:\n            heappop(hq)\n        k_maxes.append(-hq[0][0])\n        \n    return min(k_maxes) # 저장된 구간 최대값들 중 최소값 반환\n```\n","excerpt":"문제 : 프로그래머스 문제 해결 아이디어 문제는 k 길이의 구간(또는 window) 최대값 중 최소값을 구하는 문제로 이해될 수 있다. stones 배열의 길이가 n이라 할 때, max()함수로 최대값을 구하는데에 선형 시간이 소요되므로 max()함…","fields":{"slug":"/programmers. 징검다리 건너기/"},"frontmatter":{"date":"Feb 10, 2022","title":"programmers. 징검다리 건너기","tags":["Algorithms","programmers","Heap"],"update":"Jan 01, 0001"}}},{"node":{"rawMarkdownBody":"\n\n> 문제 : [프로그래머스](https://programmers.co.kr/learn/courses/30/lessons/86052)\n\n\n### 문제 해결 아이디어\n구현 문제와 그래프 사이클에 대한 문제가 합쳐져 있다.\n\n1. 구현 문제는 빛을 좌회전, 우회전하는 것으로, `delta = [(-1,0), (0,1), (1,0), (0,-1)]`라고 했을 때 좌회전은 delta 배열에서 인덱스를 왼쪽으로 한 칸 옮기는 것으로, 우회전은 인덱스를 오른쪽으로 한 칸 옮기는 것으로 구현할 수 있다.\n\n2. grid의 세로길이 n, 가로 길이 m에 대해 모든 사이클 길이의 합은 $4 x n x m$이다. 즉 모든 가능한 이동에 대해 사이클이 성립한다.\n\n3. 격자에서 같은 노드라 하더라도 빛이 지나가는 방향이 다르면 다른 경로로 여겨진다. 따라서 모든 지점에 대해 오른쪽, 왼쪽, 위쪽, 아래쪽 **4개 경로**를 통해 빛이 **들어온** 경로를 기록한다(나간 경로를 기록할 수도 있다). 빛이 같은 경로로 지나간다면 같은 사이클이 되므로 중복해서 세지 않게 주의한다.\n\n종합해보면, 임의의 노드에 4개 방향으로 빛을 쏘되 해당 노드에 같은 방향으로 쏘아진 빛이 있는 경우는 제외한다. 같은 경로로 돌아올 때까지 경로를 이동하며 기록을 남기고, 사이클이 끝날 때 마다 사이클의 길이를 기록한다.\n\n2번에 의해 이중 for문 내의 연산은 모든 i of n, j of m에 대해 합해서 $4 * n * m$번이다. n과 m은 최대 500이므로 시간 제한을 통과할 수 있다.\n\n\n### 파이썬 코드\n```python\ndef solution(grid):\n\n    ret = []\n    n, m = len(grid), len(grid[0])\n    arr = [[[] for _ in range(m)] for _ in range(n)]\n    # 인덱스를 늘리면 오른쪽으로, 인덱스를 줄이면 왼쪽으로 이동하도록 배치\n    delta = [(-1,0), (0,1), (1,0), (0,-1)]\n\n    # 모든 노드에 대해 4가지 방향으로 들어오는 사이클 검토\n    for i in range(n):\n        for j in range(m):\n            for k in range(4):\n                # 해당 노드에 같은 방향으로 들어온 기록이 있으면 스킵\n                if k in arr[i][j]:\n                    continue\n\n                cnt = 0\n                x, y, d = i, j, k\n                dx, dy = delta[d]\n                # 해당 노드에 같은 방향으로 들어온 기록이 없을 때까지 경로 이동\n                while d not in arr[x][y]:\n                    arr[x][y].append(d)\n                    cnt += 1\n\n                    if grid[x][y] == 'L':\n                        dx, dy = delta[(d - 1) % 4]\n                    elif grid[x][y] == 'R':\n                        dx, dy = delta[(d + 1) % 4]\n\n                    x, y = (x + dx) % n, (y + dy) % m\n                    d = delta.index((dx, dy))\n                # 사이클의 길이를 저장\n                ret.append(cnt)\n    return sorted(ret)\n```\n\n\n\n","excerpt":"문제 : 프로그래머스 문제 해결 아이디어 구현 문제와 그래프 사이클에 대한 문제가 합쳐져 있다. 구현 문제는 빛을 좌회전, 우회전하는 것으로, 라고 했을 때 좌회전은 delta 배열에서 인덱스를 왼쪽으로 한 칸 옮기는 것으로, 우회전은 인덱스를 오른…","fields":{"slug":"/programmers. 빛의 경로 사이클/"},"frontmatter":{"date":"Feb 09, 2022","title":"programmers. 빛의 경로 사이클","tags":["Algorithms","programmers","Implementation","Graph","Cyclic Graph"],"update":"Jan 01, 0001"}}},{"node":{"rawMarkdownBody":"\n\n> 문제 : [프로그래머스](https://programmers.co.kr/learn/courses/30/lessons/42895)\n\n\n### 문제 해결 아이디어\n- 다이내믹 프로그래밍을 활용하여 해결 할 수 있다. \n- 최대 8번까지 N의 횟수를 세어야 하므로, DP 테이블을 길이 8인 1차원 배열로 만든다.\n- DP 테이블 dp[i]에 N을 i번 이용해 만들 수 있는 수의 배열을 저장한다. dp테이블은 테이블 인덱스 i에 대해 N을 i번 반복하는 수로 초기화한다. \n- N을 사용하는 횟수의 최소값을 반환해야 하므로 dp를 작은 수 부터 채워나간다. dp[j]에 있는 원소에 대해 dp[i-j]에 있는 원소를 연산하므로, `dp[j] {사칙연산} dp[i-j]`는 N을 총 $j + (i - j) = i$번 사용한다.\n\n\n\n#### 파이썬 코드\n```python\ndef solution(N, number):\n    \n    # dp[i] : N을 i번 이용해 만들 수 있는 수의 배열\n    # 초기값은 N을 i번 사용한 수로 정의한다.\n    dp = [[]] + [[int(str(N) * i)] for i in range(1, 9)]\n\n    if [number] in dp:\n      return dp.index([number])\n\n    # dp[j] {사칙연산} dp[i-j]는 N을 총 i번 사용한다.\n    for i in range(2, 9):\n        for j in range(1, i):\n          for a in dp[j]:\n            for b in dp[i-j]:\n                dp[i].append(a + b)\n                dp[i].append(a - b)\n                dp[i].append(a * b)\n                if b != 0: # 0으로 나누지 않도록 한다.\n                    dp[i].append(a // b)\n        # 탐색 시간을 줄이기 위해 중복 제거한다.\n        dp[i] = list(set(dp[i]))\n        if number in dp[i]:\n            return i\n\n    return -1\n```\n","excerpt":"문제 : 프로그래머스 문제 해결 아이디어 다이내믹 프로그래밍을 활용하여 해결 할 수 있다.  최대 8번까지 N의 횟수를 세어야 하므로, DP 테이블을 길이 8인 1차원 배열로 만든다. DP 테이블 dpi에 N을 i번 이용해 만들 수 있는 수의 배열을…","fields":{"slug":"/programmers. N으로 표현/"},"frontmatter":{"date":"Feb 09, 2022","title":"programmers. N으로 표현","tags":["Algorithms","programmers","Dynamic Programming"],"update":"Jan 01, 0001"}}},{"node":{"rawMarkdownBody":"\n\n> 문제 : [프로그래머스](https://programmers.co.kr/learn/courses/30/lessons/12905)\n\n\n### 문제 해결 아이디어\n보드의 가로 세로 길이가 최대 1,000이므로 모든 보드의 원소에 대해 구간별로 탐색하면 문제에서 요구하는 시간을 맞출 수 없다. 대신 보드의 원소가 모두 1 또는 0이라는 것을 이용하여 (길 찾기 예제에서 자주 구현하는 것처럼) 다이내믹 프로그래밍으로 문제를 해결할 수 있다. \n- `board`를 `오른쪽`과 `아래쪽`으로 탐색해가면서,\n- `board[i][j] = 1`인 지점에서 왼쪽으로 한칸, 위쪽으로 한칸, 그리고 대각선 왼쪽 위로 한칸 이동한 지점들의 값 중 최소인 값에 1을 더한 값을 저장한다.\n이때 임의의 x, y에 대해 board[x][y]는 왼쪽과 위쪽으로 인접해 있는 정사각형의 길이를 반영하게 되며 1을 더함으로서 board[i][j]의 길이를 반영할 수 있다.\n\n예를 들어 board가 가로 세로 2인 정사각형이고 board[1][1]=1인 경우, board[0][0], board[0][1] 그리고 board[1][0] 중 한 원소라도 0인 경우 board[1][1]=1이고 board의 가장 긴 정사각형의 길이는 1이다. 반면에 세 원소가 모두 1인 경우 board[1][1]=2로 최대 정사각형의 길이는 2다.\n\n이때, 행과 열의 첫번째 인덱스에 대해서는 계산하지 않으므로 예외처리에 신경쓴다.\n\n\n### 파이썬 코드\n```python\ndef solution(board):\n    \n    # board 가로 세로 길이가 1인 경우 예외 처리\n    if len(board) == 1 or len(board[0]) == 1:\n        if max(board) == 0:\n            return 0\n        else:\n            return 1\n        \n    answer = 0\n    for i in range(1, len(board)):\n        for j in range(1, len(board[0])):\n            # board[i][j] : 왼쪽과 위쪽의 board에 대해 인접한 정사각형의 최대 길이\n            # 더해지는 1은 board[i][j]를 포함해 더해지는 길이\n            if board[i][j] != 0:\n                board[i][j] = min(board[i-1][j], board[i][j-1], board[i-1][j-1]) + 1\n            answer = max(answer, board[i][j])\n\n    return answer ** 2\n````\n\n위의 방법으로 구현할 경우 이중 for문에 의해 수행시간은 $O(N^2)$이며, 따라서 보드의 가로 세로 길이가 1,000이더라도 시간 제한을 통과할 수 있다.\n\n\n","excerpt":"문제 : 프로그래머스 문제 해결 아이디어 보드의 가로 세로 길이가 최대 1,000이므로 모든 보드의 원소에 대해 구간별로 탐색하면 문제에서 요구하는 시간을 맞출 수 없다. 대신 보드의 원소가 모두 1 또는 0이라는 것을 이용하여 (길 찾기 예제에서 …","fields":{"slug":"/programmers. 가장 큰 정사각형 찾기/"},"frontmatter":{"date":"Feb 09, 2022","title":"programmers. 가장 큰 정사각형 찾기","tags":["Algorithms","programmers","Dynamic Programming"],"update":"Jan 01, 0001"}}},{"node":{"rawMarkdownBody":"\n\n> 문제 : [프로그래머스](https://programmers.co.kr/learn/courses/30/lessons/42898)\n\n\n### 문제 해결 아이디어\n- 길 찾기 문제이며 다이나믹 프로그래밍으로 해결할 수 있다.\n- 오른쪽과 아래쪽으로만 갈 수 있으므로 왼쪽과 위쪽의 정보를 기반으로 해야한다. 최단 경로의 개수이므로 왼쪽과 위쪽의 값을 더하면 된다.\n- 배열 초기화를 0으로 했으므로, puddle에 대해서 -1로 기록해 두었다가 테이블을 채울때 마주치면 덧셈이 되지 않도록 0으로 바꿔둔다.\n- 수행시간은 이중 for문으로 $O(N^2)$이다.\n\n\n\n### 파이썬 코드\n```python\ndef solution(m, n, puddles):\n\n    # 행 n, 열 m 크기의 2차원 배열 생성\n    # road[i][j]는 (1,1)에서 (i,j)까지 갈 수 있는 최단 경로의 개수\n    road = [[0 for _ in range(m+1)] for _ in range(n+1)]\n    road[1][1] = 1\n    \n    # puddle로는 갈 수 없으므로 -1로 처리\n    for puddle in puddles:\n        y, x = puddle\n        road[x][y] = -1\n    \n    # road의 모든 지점을 탐색\n    for i in range(n):\n        for j in range(m):\n            # 시작 지점은 1로 고정\n            if i == j == 1:\n                continue\n            # 만양 puddle이면, 덧셈이 되지 않게 0으로 재처리\n            if road[i][j] == -1:\n                road[i][j] = 0\n                continue\n            # 왼쪽과 위쪽에서 올 수 있는 최단 경로의 개수의 합이 현재 지점의 최단 경로 개수\n            road[i][j] = (road[i-1][j] + road[i][j-1]) % 1000000007\n\n    return road[-1][-1]\n```","excerpt":"문제 : 프로그래머스 문제 해결 아이디어 길 찾기 문제이며 다이나믹 프로그래밍으로 해결할 수 있다. 오른쪽과 아래쪽으로만 갈 수 있으므로 왼쪽과 위쪽의 정보를 기반으로 해야한다. 최단 경로의 개수이므로 왼쪽과 위쪽의 값을 더하면 된다. 배열 초기화를…","fields":{"slug":"/programmers. 등굣길/"},"frontmatter":{"date":"Feb 09, 2022","title":"programmers. 등굣길","tags":["Algorithms","programmers","Dynamic Programming"],"update":"Jan 01, 0001"}}},{"node":{"rawMarkdownBody":"\n\n> 문제 : [프로그래머스](https://programmers.co.kr/learn/courses/30/lessons/43164)\n\n\n### 문제 해결 아이디어\n- 그래프에서 `주어진 부분 경로를 모두 사용하도록` 전체 경로를 만든다.\n- 현재 위치에서 출발하는 티켓이 있는 경우 스택에 저장하고 티켓을 사용 처리한다.\n- 현재 위치에서 출발하는 티켓이 없는 경우 해당 위치에는 도착만 가능하다. 해당 공항을 순서대로 방문 기록하면 가능한 방문 순서를 역순으로 뒤집어 저장하는 것이 된다.\n- 출발하는 티켓이 없는 경우부터 방문처리를 하므로, 기록한 순서를 역순으로 출력한다. \n\n\n### 파이썬 코드\n```python\ndef solution(tickets):\n\n    # 티켓의 경로 정보를 해시로 저장한다.\n    routes = {}\n    for t in tickets:\n        routes[t[0]] = routes.get(t[0], []) + [t[1]]\n    \n    # stack은 오른쪽에서부터 원소를 제거하므로\n    # 알파벳 순서가 앞서는 티켓을 먼저 선택하기 위해 역순으로 정렬한다.\n    for r in routes:\n        routes[r].sort(reverse=True)\n\n    # ICN에서 출발\n    stack = [\"ICN\"]\n    path = []\n    while len(stack) > 0:\n        now = stack[-1]\n        # 현재 위치 now에서 출발하는 티켓이 없으면 방문한다.\n        if now not in routes or len(routes[now]) == 0:\n            path.append(stack.pop())\n        # now에서 출발하는 티켓이 있으면 티켓의 도착지점을 stack에 저장한다.\n        else:\n            stack.append(routes[now].pop())\n\n    # path에 저장된 순서의 반대로 return한다.\n    return path[::-1]\n```","excerpt":"문제 : 프로그래머스 문제 해결 아이디어 그래프에서  전체 경로를 만든다. 현재 위치에서 출발하는 티켓이 있는 경우 스택에 저장하고 티켓을 사용 처리한다. 현재 위치에서 출발하는 티켓이 없는 경우 해당 위치에는 도착만 가능하다. 해당 공항을 순서대로…","fields":{"slug":"/programmers. 여행경로/"},"frontmatter":{"date":"Feb 09, 2022","title":"programmers. 여행경로","tags":["Algorithms","programmers","Graph"],"update":"Jan 01, 0001"}}},{"node":{"rawMarkdownBody":"\n분산 버전 관리 시스템인 깃 명령어(git commands)를 쉽게 볼 수 있도록 정리한 문서입니다.\n\n\n## 0. 깃 프로젝트의 세 단계\n* 워킹 트리 (working tree) : 파일을 수정하는 공간이다. 스테이징을 통해 스테이징 에리어로 수정사항을 보낸다.\n* 스테이징 에리어 (staging area) : 커밋을 위한 임시 스냅샷을 저장하는 공간이다. 커밋을 통해 깃 리포지토리로 수정사항을 전달해서 영구적인 스냅샷으로 만들 수 있다.\n* 깃 리포지토리, 즉 깃 르포 (git repository) : 커밋된 스냅샷을 모두 보관하고 있는 공간이다. 필요에 따라서 버전을 되돌리거나 변화해온 수정 내역을 살펴볼 수 있다.\n\n\n## 1. 배시 쉘 명령어 (Bash shell commands)\n* 디렉토리 .git 내 모든 파일 출력\n```bash\n# look inside a git directory\nls -l .git/\n```\n* 숨겨진 파일을 포함해 현재 디렉토리에 위치한 모든 파일 출력\n```bash\n# list files with a dot (hidden files)\nls -la\n```\n* 파일 file_name.py 생성하기\n```bash\n# create a file\ntouch file_name.py\n```\n* atom 에디터로 파일 file_name.py 열기\n```bash\n# open a file with atom editor\natom file_name.py\n```\n* 파일 file_name.py 내용 쉘에서 보기\n```bash\n# lookup a file\ncat file_name.py\n```\n\n## 2. 설정 명령어 (Configuration commands)\n* 깃 현재 설정 출력하기 \n```bash\n# check current configuration\ngit config -l\n```\n* 깃의 이름, 이메일 설정 변경하기\n```bash\n# change configuration\ngit config --global user.name \"user_name\"\ngit config --global user.email \"user_email\"\n```\n* 깃허브 키를 15분 동안 캐시하기\n```bash\n# cache github key for 15 minutes\ngit config --global credential.helper cache\n```\n\n\n## 3. 커밋 명령어 (Commit commands)\n* 현재 디렉토리에서 새로운 깃 르포 생성하기\n```bash\n# initialize an empty git repository in current directory\ngit init\n```\n* 현재 워킹 트리의 정보 출력하기\n```bash\n# get information of current working tree\ngit status\n```\n\n### 커밋하기\n* 파일 file_name.py를 스테이징 에리어에 올리기 (커밋 준비)\n```bash\n# command git to track follwing file\ngit add file_name.py\n```\n* 커밋 메시지 창을 열며 현재 스테이징 에리어에 있는 것을 전부 커밋하기\n```bash\n# commit everything in current staging area\n# opens a text editor to enter a commit message\ngit commit\n```\n* 커밋 메시지를 입력하지 않고 커밋하기\n```bash\n# stage changes to tracked file & commit in one step\ngit commit -a\n```\n* 간략한 커밋 메시지 commit message와 함께 커밋하기\n```bash\n# stage & commit & enter message\ngit commit -a -m \"commit message\"\n```\n\n### 커밋 진행상황 보기\n* 모든 커밋 히스토리 출력하기\n```bash\n# check history of all commits\ngit log\n```\n* 각 커밋에서 수정된 사항을 줄 별로 출력하기\n```bash\n# show actual lines that changed in each commit\ngit log -p\n```\n* 커밋의 통계 정보 출력하기\n```bash\n# show statistics about the changes in the commit\ngit log --stat\n```\n* 커밋의 브랜치 트리 출력하기\n```bash\n# show commit branch tree\ngit log --graph --oneline --all\n```\n* 커밋 아이디 commit_id에 해당하는 커밋 정보 출력하기\n```bash\n# show the information about the commit and associated petches\ngit show [commit_id]\n```\n* commit_ id1과 commit_ id2에 해당하는 두 커밋 비교하기\n```bash\n# similar to Linux diff command\ngit diff [commit_id1] [commit_id2]\n```\n* 스테이징 되었지만 커밋되지 않은 파일 출력하기\n```bash\n# alias to --cached, show all staged but not commited files\ngit diff --staged\n```\n* 파일 file1.py의 이름을 file2.py로 변경하기\n```bash\n# rename file1 with file2\n# similar to Linux mv command\ngit mv [file1.py] [file2.py]\n```\n* 파일 file_name.py 삭제하기\n```bash\n# remove file_name.py from working space\n# similar to Linux rm command\ngit rm [file_name.py]\n```\n\n### 커밋 되돌리기\n* HEAD가 가리키는 브랜치가 commit_id를 가리키게 하기\n```bash\n# resets the repo in the Index, the next snapshot to commit\ngit reset --soft [commit_id]\n```\n* HEAD 브랜치를 이동하고 스테이징 에리어를 리셋하기\n```bash\n# update Index to the snapshot that HEAD is pointing \ngit reset --mixed [commit_id]\n```\n* HEAD 브랜치를 이동하고 스테이징 에리어와 워킹 디렉토리를 리셋하기\n```bash\n# update Index to the snapshot that HEAD is pointing \ngit reset --hard [commit_id]\n```\n* 현재 스테이징 에리어에 있는 내용을 커밋 내용으로 덮어쓰기\n```bash\n# make changes to commits after-the-fact on local commits\ngit commit --amend\n```\n* 히스토리를 유지한채 새로운 커밋으로 커밋 commit_id로 되돌리기\n```bash\n# make a new commit which rolls back a previous commit\ngit revert HEAD/[commit_id]\n```\n* 커밋을 num_commit_to_reverse 개수만큼 되돌리기\n```bash\n# roll back [num_commit_to_reverse]-many commit\ngit reset --soft HEAD~[num_commit_to_reverse]\n```\n\n\n## 4. 브랜치 명령어 (Branch commands)\n* 모든 브랜치 출력하기\n```bash\n# list all branches\ngit branch\n```\n* 읽기 전용 원격 브랜치 출력하기\n```bash\n# shows read-only remote branches\ngit branch -r\n```\n* 브랜치 branch_name 생성하기\n```bash\n# creates the branch_name branch\ngit branch [branch_name]\n```\n* 브랜치 branch_name으로 이동하기\n```bash\n# switch to branch_name\ngit checkout [branch_name]\n```\n* 브랜치 branch_name을 생성하고 그 브랜치로 위치 이동하기\n```bash\n# creates a new branch and switches to it\ngit checkout -b [branch_name]\n```\n* 브랜치 branch_name 삭제하기\n```bash\n# deletes the branch branch_name\ngit branch -d [branch_name]\n```\n* 브랜치 branch_name 삭제 강제하기\n```bash\n# forcibly deletes the branch\ngit branch -D [branch_name]\n```\n* 브랜치 branch_name을 마스터 브랜치로 합치기\n```bash\n# joins branches together to the master branch\ngit merge [branch_name]\n```\n* 브랜치 충돌(merge conflicts)이 발생했을 때, 머지 취소하기\n```bash\n# when merge conflicts, abort merge action\ngit merge --abort\n```\n\n\n## 5. 깃허브 명령어 (Github commands)\n\n* 로컬 환경에 URL로 원격 르포 복제하기\n```bash\n# clone a remote repository into a local workspace\ngit clone [URL] \n```\n* 로컬 파일을 원격 르포에 푸시하기\n```bash\n# push commits from local repo to a remote repo\ngit push\n```\n* 원격 르포의 커밋을 로컬 환경에 별도의 브랜치로 복사해오기\n```bash\n# copy the commits done in the remote repository\ngit fetch\n```\n* 원격 르포의 커밋을 로컬 환경의 브랜치와 머지하기\n```bash\n# fetch from remote & merge\ngit pull\n```\n* 원격 르포 출력하기\n```bash\n# List remote repos\ngit remote\n```\n* 원격 르포 URL 출력하기\n```bash\n# show URL of remote repo\ngit remote -v\n```\n* 원격 르포 remote_name 정보 출력하기\n```bash\n# Describes a single remote repo\ngit remote show [remote_URL]\n```\n* 원격 르포의 업데이트를 로컬 환경에 불러오기\n```bash\n# Fetches the most up-to-date objects\ngit remote update\n```\n* 로컬 르포가 연결된 원격 르포 new-url로 이전하기\n```bash\n# transfer a repository from origin to [new-url]\ngit remote set-url origin [new-url]\n```\n* 브랜치 branch_name의 베이스 커밋을 바꾸기\n```bash\n# change the base commit used for the branch [branch_name]\ngit rebase [branch_name]\n```\n\n* 깃허브는 부터 personal access token을 사용해 계정의 권한을 조절하고 있다. personal access token 사용에 대해서는 [여기](https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/creating-a-personal-access-token)를 참고하자.\n\n* 깃허브에 push할 때는 파일당 **120MB의 용량 제한**이 있다. 이보다 큰 용량의 파일을 push할 때 remote rejected 에러를 마주하므로, 용량을 맞춰야 한다.\n\n\n## 참고 자료\n1. Git Docs ([link](https://git-scm.com/doc))\n2. GitHub Docs ([link](https://docs.github.com/en))\n3. Coursera, Google, Introduction to Git and Github ([link](https://www.coursera.org/learn/introduction-git-github))\n","excerpt":"분산 버전 관리 시스템인 깃 명령어(git commands)를 쉽게 볼 수 있도록 정리한 문서입니다. 0. 깃 프로젝트의 세 단계 워킹 트리 (working tree) : 파일을 수정하는 공간이다. 스테이징을 통해 스테이징 에리어로 수정사항을 보낸다…","fields":{"slug":"/git_cheat_sheet/"},"frontmatter":{"date":"Feb 04, 2022","title":"깃 치트 시트 (Git Cheat Sheet)","tags":["Git"],"update":"May 27, 2022"}}}]}},"pageContext":{}},"staticQueryHashes":["2027115977","694178885"]}